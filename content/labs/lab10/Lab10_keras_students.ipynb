{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"> CS-109A Introduction to Data Science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 10:  Neural Networks using `keras` \n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Fall 2018**<br/>\n",
    "**Instructors:** Pavlos Protopapas and Kevin Rader<br/>\n",
    "**Lab Instructor:** Eleni Kaxiras<br/>\n",
    "**Authors:** David Sondak, Eleni Kaxiras, and Pavlos Protopapas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "blockquote { background: #AEDE94; }\n",
       "h1 { \n",
       "    padding-top: 25px;\n",
       "    padding-bottom: 25px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "h2 { \n",
       "    padding-top: 10px;\n",
       "    padding-bottom: 10px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "\n",
       "div.exercise {\n",
       "\tbackground-color: #ffcccc;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "}\n",
       "div.theme {\n",
       "\tbackground-color: #DDDDDD;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 18pt;\n",
       "}\n",
       "div.gc { \n",
       "\tbackground-color: #AEDE94;\n",
       "\tborder-color: #E9967A; \t \n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 12pt;\n",
       "}\n",
       "p.q1 { \n",
       "    padding-top: 5px;\n",
       "    padding-bottom: 5px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "header {\n",
       "   padding-top: 35px;\n",
       "    padding-bottom: 35px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "</style>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## RUN THIS CELL TO GET THE RIGHT FORMATTING \n",
    "import requests\n",
    "from IPython.core.display import HTML\n",
    "styles = requests.get\\\n",
    "    (\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\").text\n",
    "HTML(styles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anatomy of an Artificial Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a previous lab we created our own neural network by writing some simple python functions.  We focused on a regression problem where we tried to learn a function. We practiced using the logistic activation function in a network with multiple nodes, but a single or two hidden layers.  Some of the key observations were:\n",
    "* Increasing the number of nodes allows us to represent more complicated functions  \n",
    "* The weights and biases have a very big impact on the solution\n",
    "* Finding the \"correct\" weights and biases is really hard to do manually\n",
    "* There must be a better method for determining the weights and biases automatically\n",
    "\n",
    "We also didn't assess the effects of different activation functions or different network depths. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The 3 parts of an ANN\n",
    "\n",
    "- **Part 1: the input layer** (dimentions are determined from our dataset)\n",
    "- **Part 2: the internal architecture or hidden layers** (the number of layers, the activation functions, the learnable parameters and other hyperparameters)\n",
    "- **Part 3: the output layer** (what we want from the network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A word about .npy files\n",
    "\n",
    "Numpy arrays are faster than plain python lists, as we know. Numpy also offers a file format called .npy, which, when it comes to reading the same data multiple times from disk storage, is a lot faster than reading from a csv file. You can save any list or array into this format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('/tmp/123', np.array([[1, 2, 3], [4, 5, 6]]))\n",
    "hello = np.load('/tmp/123.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [4, 5, 6]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hello"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Keras` Basics ![](figs/keras.png)\n",
    "https://keras.io/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning computations can be quite demanding. TensorFlow is a framework for representing complicated ML algorithms and executing them in any platform, from a phone to a distributed system using GPUs. Developed by Google Brain, TensorFlow is used very broadly today. \n",
    "\n",
    "**[`keras`](https://keras.io/)**, is a high-level API used for fast prototyping, advanced research, and production. We will use `tf.keras` which is TensorFlow's implementation of the `keras` API.\n",
    "\n",
    "### Models are assemblies of layers\n",
    "\n",
    "The core data structure of Keras is a **model**, a way to organize layers. A model is understood as a sequence or a graph of standalone, fully-configurable modules that can be plugged together with as few restrictions as possible. In particular, neural layers, cost functions, optimizers, initialization schemes, activation functions, regularization schemes are all standalone modules that you can combine to create new models.\n",
    "\n",
    "The simplest type of model is the **Sequential** model, a linear stack of layers. For more complex architectures, one can use the Keras **Functional** API, which allows to build arbitrary graphs of layers.\n",
    "\n",
    "https://keras.io/models/model/\n",
    "\n",
    "Everything you need to know about the Sequential model is here: https://keras.io/models/sequential/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Installation\n",
    "\n",
    "If you haven't already, install `Keras` using the instructions found at [https://keras.io/#installation](https://keras.io/#installation)\n",
    "\n",
    "Choose the TensorFlow installation instructions (found at [https://www.tensorflow.org/install/](https://www.tensorflow.org/install/) )."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Approximating a Gaussian using keras\n",
    "Let's try to redo the problem from last week.  Recall that we had a function\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "f\\left(x\\right) = e^{-x^{2}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "and we wanted to use a neural network to approximate that function.  This week, we will use `keras` to do the true optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import the necessary `keras` modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/winniewang/anaconda/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0\n",
      "2.1.6-tf\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "print(tf.VERSION)\n",
    "print(tf.keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[_DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 268435456, 13810696059528186261)]\n"
     ]
    }
   ],
   "source": [
    "# Checking if our machine has GPUs. Mine does not..\n",
    "with tf.Session() as sess:\n",
    "    devices = sess.list_devices()\n",
    "    print(devices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we get started, we need to create some **data**.  We will generate data points from an underlying function (here the Guassian).  Then we will use the `sklearn` `train_test_split` method to split the dataset into training and testing portions.  Remember that we train a machine learning algorithm on the training set and then assess the algorithm's performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "n_samples = 1050 # set the number of samples to take for each dataset\n",
    "test_size = 0.3 # set the proportion of data to hold out for testing\n",
    "\n",
    "# define the function and add noise\n",
    "\n",
    "def f_gauss(x):\n",
    "    return np.exp(-x * x) + np.random.normal(loc=0, scale=.1, size = x.shape[0])\n",
    "\n",
    "X = np.random.permutation(np.linspace(-10, 10, n_samples)) # choose some points from the function\n",
    "Y = f_gauss(X)\n",
    "\n",
    "# create training and testing data from this set of points\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1a2683af60>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEBCAYAAAB4wNK4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n5LCFT09/AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a keras network\n",
    "\n",
    "Now we will create a neural network model with keras. We're going to use a single layer and just 2 neurons in that layer. We will start with the sigmoid activation function. We also choose a linear output layer since we are doing regression. The loss function is selected to be the **mean squared error (MSE)**. In addition to these choices we must also specify our initial weights as well as the optimization method that will be used to minimize the loss function. The keras interface has many choises as to those hyperparameters.\n",
    "\n",
    "**Part 1:** First we start by defining the number of nodes in a layer and the input dimensions. If we have more than one layer we might need to define a value for the number of nodes (H) for each layer.\n",
    "\n",
    "`H = \n",
    "input_dim =`\n",
    "\n",
    "Then we instantiate the model\n",
    "\n",
    "`model = models.Sequential() `\n",
    "\n",
    "**Part 2:** Then we add the hidden layers. Adding layers and stacking them is done using `.add()`\n",
    "\n",
    "`model.add(layers.Dense(H, input_dim=input_dim,  \n",
    "                activation='sigmoid')) `\n",
    "\n",
    "**An alternative way** \n",
    "\n",
    "`model = Sequential([\n",
    "    Dense(200, input_shape=(X_train.shape[1],), activation='relu'),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dense(50, activation='relu'),\n",
    "    Dense(4, activation='linear')\n",
    "])`\n",
    "\n",
    "**Part 3:** We end with the final layer (output)\n",
    "\n",
    "`model.add(layers.Dense(1, \n",
    "                activation='linear')) `\n",
    "                \n",
    "Our model is not ready yet. We need to configure its learning process with .compile():\n",
    "\n",
    "`model.compile(loss='mean_squared_error', optimizer='sgd')`\n",
    "\n",
    "If you need to, you can further configure your optimizer. A core principle of Keras is to make things reasonably simple, while allowing the user to be fully in control when they need to (the ultimate control being the easy extensibility of the source code)\n",
    "\n",
    "`model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.SGD(lr=0.01))`\n",
    "              \n",
    "Our model is now ready to use. We haven't trained it yet, but we'll do that now using the fit method. Notice that we also need to specify the batch size for the stochastic gradient decent algorithm as well as the number of epochs to run.\n",
    "\n",
    "`model.fit(X_train, Y_train, batch_size=100, epochs=100)#, verbose=1)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"><b>Exercise 1:</b> </div>\n",
    "\n",
    "Build a NN with one hidden layer with **2 neurons**. Use the `tanh` activation function. Train the model using the X_train dataset from above (train the model in this case means run `.compile` and `.fit`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "735/735 [==============================] - 0s 644us/step - loss: 0.0736\n",
      "Epoch 2/100\n",
      "735/735 [==============================] - 0s 16us/step - loss: 0.0712\n",
      "Epoch 3/100\n",
      "735/735 [==============================] - 0s 15us/step - loss: 0.0694\n",
      "Epoch 4/100\n",
      "735/735 [==============================] - 0s 18us/step - loss: 0.0679\n",
      "Epoch 5/100\n",
      "735/735 [==============================] - 0s 22us/step - loss: 0.0668\n",
      "Epoch 6/100\n",
      "735/735 [==============================] - 0s 16us/step - loss: 0.0661\n",
      "Epoch 7/100\n",
      "735/735 [==============================] - 0s 24us/step - loss: 0.0655\n",
      "Epoch 8/100\n",
      "735/735 [==============================] - 0s 28us/step - loss: 0.0650\n",
      "Epoch 9/100\n",
      "735/735 [==============================] - 0s 25us/step - loss: 0.0647\n",
      "Epoch 10/100\n",
      "735/735 [==============================] - 0s 14us/step - loss: 0.0644\n",
      "Epoch 11/100\n",
      "735/735 [==============================] - 0s 16us/step - loss: 0.0641\n",
      "Epoch 12/100\n",
      "735/735 [==============================] - 0s 16us/step - loss: 0.0639\n",
      "Epoch 13/100\n",
      "735/735 [==============================] - 0s 21us/step - loss: 0.0638\n",
      "Epoch 14/100\n",
      "735/735 [==============================] - 0s 21us/step - loss: 0.0636\n",
      "Epoch 15/100\n",
      "735/735 [==============================] - 0s 21us/step - loss: 0.0635\n",
      "Epoch 16/100\n",
      "735/735 [==============================] - 0s 15us/step - loss: 0.0634\n",
      "Epoch 17/100\n",
      "735/735 [==============================] - 0s 15us/step - loss: 0.0633\n",
      "Epoch 18/100\n",
      "735/735 [==============================] - 0s 15us/step - loss: 0.0632\n",
      "Epoch 19/100\n",
      "735/735 [==============================] - 0s 29us/step - loss: 0.0631\n",
      "Epoch 20/100\n",
      "735/735 [==============================] - 0s 27us/step - loss: 0.0630\n",
      "Epoch 21/100\n",
      "735/735 [==============================] - 0s 20us/step - loss: 0.0629\n",
      "Epoch 22/100\n",
      "735/735 [==============================] - 0s 17us/step - loss: 0.0627\n",
      "Epoch 23/100\n",
      "735/735 [==============================] - 0s 17us/step - loss: 0.0626\n",
      "Epoch 24/100\n",
      "735/735 [==============================] - 0s 26us/step - loss: 0.0625\n",
      "Epoch 25/100\n",
      "735/735 [==============================] - 0s 22us/step - loss: 0.0623\n",
      "Epoch 26/100\n",
      "735/735 [==============================] - 0s 21us/step - loss: 0.0622\n",
      "Epoch 27/100\n",
      "735/735 [==============================] - 0s 14us/step - loss: 0.0620\n",
      "Epoch 28/100\n",
      "735/735 [==============================] - 0s 15us/step - loss: 0.0618\n",
      "Epoch 29/100\n",
      "735/735 [==============================] - 0s 22us/step - loss: 0.0615\n",
      "Epoch 30/100\n",
      "735/735 [==============================] - 0s 18us/step - loss: 0.0613\n",
      "Epoch 31/100\n",
      "735/735 [==============================] - 0s 22us/step - loss: 0.0610\n",
      "Epoch 32/100\n",
      "735/735 [==============================] - 0s 15us/step - loss: 0.0607\n",
      "Epoch 33/100\n",
      "735/735 [==============================] - 0s 15us/step - loss: 0.0604\n",
      "Epoch 34/100\n",
      "735/735 [==============================] - 0s 15us/step - loss: 0.0600\n",
      "Epoch 35/100\n",
      "735/735 [==============================] - 0s 29us/step - loss: 0.0595\n",
      "Epoch 36/100\n",
      "735/735 [==============================] - 0s 21us/step - loss: 0.0591\n",
      "Epoch 37/100\n",
      "735/735 [==============================] - 0s 16us/step - loss: 0.0586\n",
      "Epoch 38/100\n",
      "735/735 [==============================] - 0s 18us/step - loss: 0.0580\n",
      "Epoch 39/100\n",
      "735/735 [==============================] - 0s 17us/step - loss: 0.0574\n",
      "Epoch 40/100\n",
      "735/735 [==============================] - 0s 26us/step - loss: 0.0568\n",
      "Epoch 41/100\n",
      "735/735 [==============================] - 0s 21us/step - loss: 0.0562\n",
      "Epoch 42/100\n",
      "735/735 [==============================] - 0s 18us/step - loss: 0.0554\n",
      "Epoch 43/100\n",
      "735/735 [==============================] - 0s 16us/step - loss: 0.0547\n",
      "Epoch 44/100\n",
      "735/735 [==============================] - 0s 29us/step - loss: 0.0540\n",
      "Epoch 45/100\n",
      "735/735 [==============================] - 0s 21us/step - loss: 0.0531\n",
      "Epoch 46/100\n",
      "735/735 [==============================] - 0s 15us/step - loss: 0.0522\n",
      "Epoch 47/100\n",
      "735/735 [==============================] - 0s 19us/step - loss: 0.0513\n",
      "Epoch 48/100\n",
      "735/735 [==============================] - 0s 18us/step - loss: 0.0503\n",
      "Epoch 49/100\n",
      "735/735 [==============================] - 0s 20us/step - loss: 0.0494\n",
      "Epoch 50/100\n",
      "735/735 [==============================] - 0s 28us/step - loss: 0.0483\n",
      "Epoch 51/100\n",
      "735/735 [==============================] - 0s 26us/step - loss: 0.0474\n",
      "Epoch 52/100\n",
      "735/735 [==============================] - 0s 27us/step - loss: 0.0465\n",
      "Epoch 53/100\n",
      "735/735 [==============================] - 0s 25us/step - loss: 0.0454\n",
      "Epoch 54/100\n",
      "735/735 [==============================] - 0s 22us/step - loss: 0.0444\n",
      "Epoch 55/100\n",
      "735/735 [==============================] - 0s 27us/step - loss: 0.0434\n",
      "Epoch 56/100\n",
      "735/735 [==============================] - 0s 25us/step - loss: 0.0424\n",
      "Epoch 57/100\n",
      "735/735 [==============================] - 0s 24us/step - loss: 0.0414\n",
      "Epoch 58/100\n",
      "735/735 [==============================] - 0s 17us/step - loss: 0.0404\n",
      "Epoch 59/100\n",
      "735/735 [==============================] - 0s 21us/step - loss: 0.0393\n",
      "Epoch 60/100\n",
      "735/735 [==============================] - 0s 22us/step - loss: 0.0384\n",
      "Epoch 61/100\n",
      "735/735 [==============================] - 0s 25us/step - loss: 0.0375\n",
      "Epoch 62/100\n",
      "735/735 [==============================] - 0s 24us/step - loss: 0.0365\n",
      "Epoch 63/100\n",
      "735/735 [==============================] - 0s 21us/step - loss: 0.0356\n",
      "Epoch 64/100\n",
      "735/735 [==============================] - 0s 24us/step - loss: 0.0347\n",
      "Epoch 65/100\n",
      "735/735 [==============================] - 0s 20us/step - loss: 0.0339\n",
      "Epoch 66/100\n",
      "735/735 [==============================] - 0s 15us/step - loss: 0.0330\n",
      "Epoch 67/100\n",
      "735/735 [==============================] - 0s 16us/step - loss: 0.0322\n",
      "Epoch 68/100\n",
      "735/735 [==============================] - 0s 16us/step - loss: 0.0314\n",
      "Epoch 69/100\n",
      "735/735 [==============================] - 0s 18us/step - loss: 0.0306\n",
      "Epoch 70/100\n",
      "735/735 [==============================] - 0s 15us/step - loss: 0.0299\n",
      "Epoch 71/100\n",
      "735/735 [==============================] - 0s 27us/step - loss: 0.0293\n",
      "Epoch 72/100\n",
      "735/735 [==============================] - 0s 20us/step - loss: 0.0286\n",
      "Epoch 73/100\n",
      "735/735 [==============================] - 0s 22us/step - loss: 0.0279\n",
      "Epoch 74/100\n",
      "735/735 [==============================] - 0s 20us/step - loss: 0.0273\n",
      "Epoch 75/100\n",
      "735/735 [==============================] - 0s 16us/step - loss: 0.0267\n",
      "Epoch 76/100\n",
      "735/735 [==============================] - 0s 17us/step - loss: 0.0261\n",
      "Epoch 77/100\n",
      "735/735 [==============================] - 0s 28us/step - loss: 0.0256\n",
      "Epoch 78/100\n",
      "735/735 [==============================] - 0s 32us/step - loss: 0.0250\n",
      "Epoch 79/100\n",
      "735/735 [==============================] - 0s 23us/step - loss: 0.0245\n",
      "Epoch 80/100\n",
      "735/735 [==============================] - 0s 21us/step - loss: 0.0239\n",
      "Epoch 81/100\n",
      "735/735 [==============================] - 0s 25us/step - loss: 0.0234\n",
      "Epoch 82/100\n",
      "735/735 [==============================] - 0s 14us/step - loss: 0.0229\n",
      "Epoch 83/100\n",
      "735/735 [==============================] - 0s 24us/step - loss: 0.0225\n",
      "Epoch 84/100\n",
      "735/735 [==============================] - 0s 16us/step - loss: 0.0220\n",
      "Epoch 85/100\n",
      "735/735 [==============================] - 0s 19us/step - loss: 0.0216\n",
      "Epoch 86/100\n",
      "735/735 [==============================] - 0s 19us/step - loss: 0.0212\n",
      "Epoch 87/100\n",
      "735/735 [==============================] - 0s 22us/step - loss: 0.0208\n",
      "Epoch 88/100\n",
      "735/735 [==============================] - 0s 22us/step - loss: 0.0204\n",
      "Epoch 89/100\n",
      "735/735 [==============================] - 0s 19us/step - loss: 0.0200\n",
      "Epoch 90/100\n",
      "735/735 [==============================] - 0s 19us/step - loss: 0.0197\n",
      "Epoch 91/100\n",
      "735/735 [==============================] - 0s 27us/step - loss: 0.0193\n",
      "Epoch 92/100\n",
      "735/735 [==============================] - 0s 22us/step - loss: 0.0190\n",
      "Epoch 93/100\n",
      "735/735 [==============================] - 0s 17us/step - loss: 0.0187\n",
      "Epoch 94/100\n",
      "735/735 [==============================] - 0s 15us/step - loss: 0.0184\n",
      "Epoch 95/100\n",
      "735/735 [==============================] - 0s 24us/step - loss: 0.0181\n",
      "Epoch 96/100\n",
      "735/735 [==============================] - 0s 23us/step - loss: 0.0178\n",
      "Epoch 97/100\n",
      "735/735 [==============================] - 0s 17us/step - loss: 0.0176\n",
      "Epoch 98/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "735/735 [==============================] - 0s 15us/step - loss: 0.0173\n",
      "Epoch 99/100\n",
      "735/735 [==============================] - 0s 18us/step - loss: 0.0171\n",
      "Epoch 100/100\n",
      "735/735 [==============================] - 0s 19us/step - loss: 0.0168\n"
     ]
    }
   ],
   "source": [
    "# %load solutions/NN_1_layer_2_nodes.py\n",
    "H = 2 # number of nodes in the layer\n",
    "input_dim = 1 # input dimension: just x\n",
    "\n",
    "model = models.Sequential() # create sequential multi-layer perceptron\n",
    "\n",
    "# our first hidden layer\n",
    "model.add(layers.Dense(H, input_dim=input_dim, \n",
    "                activation='tanh')) \n",
    "# layer 1\n",
    "model.add(layers.Dense(1, kernel_initializer='normal', \n",
    "                activation='linear')) \n",
    "\n",
    "# compile the model\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "# fit the model\n",
    "model_history = model.fit(X_train, Y_train, batch_size=100, epochs=100, verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great!  We've trained a model.  Now it's time to explore the results.  Notice the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Some constants for our plots\n",
    "FIG_SIZE = (10,5)\n",
    "FONT_SIZE = 10\n",
    "LABEL_SIZE = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use our model to predict in the range we want\n",
    "X_range = np.linspace(-10, 10, 1000)\n",
    "y_pred = model.predict(X_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnsAAAFWCAYAAAAR7lviAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMAwzgPn/NYfNAjRc9lgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the results\n",
    "fig, ax = plt.subplots(1, 1, figsize=FIG_SIZE)\n",
    "ax.scatter(X_train, Y_train, label='Training data')\n",
    "ax.plot(X_range, y_pred, lw=4, color='r', label=f'MLP with one hidden layer and {H} nodes')\n",
    "ax.set_xlabel(r'$X$', fontsize=FONT_SIZE)\n",
    "ax.set_ylabel(r'$Y$', fontsize=FONT_SIZE)\n",
    "ax.set_title(f'NN with {len(model_history.model.layers)-1} layers, {H} nodes in each layer', fontsize=LABEL_SIZE)\n",
    "ax.tick_params(labelsize=LABEL_SIZE)\n",
    "\n",
    "ax.legend(loc=0, fontsize=FONT_SIZE)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"><b>Exercise 2:</b></div>\n",
    "\n",
    "Change the number of neurons in the layer. Try changing the activation function to `reLU`.  Can you get better results?  What worked the best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 367 samples, validate on 368 samples\n",
      "Epoch 1/1200\n",
      "367/367 [==============================] - 0s 1ms/step - loss: 0.0834 - val_loss: 0.0671\n",
      "Epoch 2/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0806 - val_loss: 0.0649\n",
      "Epoch 3/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0784 - val_loss: 0.0633\n",
      "Epoch 4/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0767 - val_loss: 0.0623\n",
      "Epoch 5/1200\n",
      "367/367 [==============================] - 0s 23us/step - loss: 0.0758 - val_loss: 0.0617\n",
      "Epoch 6/1200\n",
      "367/367 [==============================] - 0s 23us/step - loss: 0.0752 - val_loss: 0.0613\n",
      "Epoch 7/1200\n",
      "367/367 [==============================] - 0s 30us/step - loss: 0.0749 - val_loss: 0.0610\n",
      "Epoch 8/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0745 - val_loss: 0.0606\n",
      "Epoch 9/1200\n",
      "367/367 [==============================] - 0s 19us/step - loss: 0.0741 - val_loss: 0.0601\n",
      "Epoch 10/1200\n",
      "367/367 [==============================] - 0s 22us/step - loss: 0.0736 - val_loss: 0.0596\n",
      "Epoch 11/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0729 - val_loss: 0.0591\n",
      "Epoch 12/1200\n",
      "367/367 [==============================] - 0s 25us/step - loss: 0.0722 - val_loss: 0.0586\n",
      "Epoch 13/1200\n",
      "367/367 [==============================] - 0s 28us/step - loss: 0.0714 - val_loss: 0.0581\n",
      "Epoch 14/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0709 - val_loss: 0.0576\n",
      "Epoch 15/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0702 - val_loss: 0.0572\n",
      "Epoch 16/1200\n",
      "367/367 [==============================] - 0s 20us/step - loss: 0.0696 - val_loss: 0.0567\n",
      "Epoch 17/1200\n",
      "367/367 [==============================] - 0s 22us/step - loss: 0.0690 - val_loss: 0.0561\n",
      "Epoch 18/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0683 - val_loss: 0.0556\n",
      "Epoch 19/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0677 - val_loss: 0.0551\n",
      "Epoch 20/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0670 - val_loss: 0.0545\n",
      "Epoch 21/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0663 - val_loss: 0.0539\n",
      "Epoch 22/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0655 - val_loss: 0.0533\n",
      "Epoch 23/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0649 - val_loss: 0.0527\n",
      "Epoch 24/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0641 - val_loss: 0.0522\n",
      "Epoch 25/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0634 - val_loss: 0.0516\n",
      "Epoch 26/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0627 - val_loss: 0.0511\n",
      "Epoch 27/1200\n",
      "367/367 [==============================] - 0s 40us/step - loss: 0.0619 - val_loss: 0.0505\n",
      "Epoch 28/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0612 - val_loss: 0.0500\n",
      "Epoch 29/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0605 - val_loss: 0.0494\n",
      "Epoch 30/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0597 - val_loss: 0.0488\n",
      "Epoch 31/1200\n",
      "367/367 [==============================] - 0s 24us/step - loss: 0.0590 - val_loss: 0.0482\n",
      "Epoch 32/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0582 - val_loss: 0.0476\n",
      "Epoch 33/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0574 - val_loss: 0.0471\n",
      "Epoch 34/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0567 - val_loss: 0.0466\n",
      "Epoch 35/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0560 - val_loss: 0.0461\n",
      "Epoch 36/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0553 - val_loss: 0.0456\n",
      "Epoch 37/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0547 - val_loss: 0.0452\n",
      "Epoch 38/1200\n",
      "367/367 [==============================] - 0s 19us/step - loss: 0.0539 - val_loss: 0.0447\n",
      "Epoch 39/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0533 - val_loss: 0.0443\n",
      "Epoch 40/1200\n",
      "367/367 [==============================] - 0s 19us/step - loss: 0.0527 - val_loss: 0.0438\n",
      "Epoch 41/1200\n",
      "367/367 [==============================] - 0s 30us/step - loss: 0.0522 - val_loss: 0.0435\n",
      "Epoch 42/1200\n",
      "367/367 [==============================] - 0s 23us/step - loss: 0.0516 - val_loss: 0.0431\n",
      "Epoch 43/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0510 - val_loss: 0.0428\n",
      "Epoch 44/1200\n",
      "367/367 [==============================] - 0s 27us/step - loss: 0.0505 - val_loss: 0.0425\n",
      "Epoch 45/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0500 - val_loss: 0.0421\n",
      "Epoch 46/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0495 - val_loss: 0.0418\n",
      "Epoch 47/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0490 - val_loss: 0.0415\n",
      "Epoch 48/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0485 - val_loss: 0.0413\n",
      "Epoch 49/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0481 - val_loss: 0.0410\n",
      "Epoch 50/1200\n",
      "367/367 [==============================] - 0s 19us/step - loss: 0.0477 - val_loss: 0.0408\n",
      "Epoch 51/1200\n",
      "367/367 [==============================] - 0s 20us/step - loss: 0.0472 - val_loss: 0.0405\n",
      "Epoch 52/1200\n",
      "367/367 [==============================] - 0s 47us/step - loss: 0.0469 - val_loss: 0.0403\n",
      "Epoch 53/1200\n",
      "367/367 [==============================] - 0s 20us/step - loss: 0.0465 - val_loss: 0.0401\n",
      "Epoch 54/1200\n",
      "367/367 [==============================] - 0s 19us/step - loss: 0.0461 - val_loss: 0.0399\n",
      "Epoch 55/1200\n",
      "367/367 [==============================] - 0s 23us/step - loss: 0.0458 - val_loss: 0.0397\n",
      "Epoch 56/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0455 - val_loss: 0.0394\n",
      "Epoch 57/1200\n",
      "367/367 [==============================] - 0s 19us/step - loss: 0.0452 - val_loss: 0.0393\n",
      "Epoch 58/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0448 - val_loss: 0.0391\n",
      "Epoch 59/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0445 - val_loss: 0.0389\n",
      "Epoch 60/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0442 - val_loss: 0.0387\n",
      "Epoch 61/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0440 - val_loss: 0.0385\n",
      "Epoch 62/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0437 - val_loss: 0.0384\n",
      "Epoch 63/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0434 - val_loss: 0.0382\n",
      "Epoch 64/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0431 - val_loss: 0.0381\n",
      "Epoch 65/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0429 - val_loss: 0.0379\n",
      "Epoch 66/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0426 - val_loss: 0.0378\n",
      "Epoch 67/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0423 - val_loss: 0.0376\n",
      "Epoch 68/1200\n",
      "367/367 [==============================] - 0s 14us/step - loss: 0.0422 - val_loss: 0.0374\n",
      "Epoch 69/1200\n",
      "367/367 [==============================] - 0s 40us/step - loss: 0.0418 - val_loss: 0.0373\n",
      "Epoch 70/1200\n",
      "367/367 [==============================] - 0s 30us/step - loss: 0.0416 - val_loss: 0.0372\n",
      "Epoch 71/1200\n",
      "367/367 [==============================] - 0s 25us/step - loss: 0.0415 - val_loss: 0.0370\n",
      "Epoch 72/1200\n",
      "367/367 [==============================] - 0s 24us/step - loss: 0.0412 - val_loss: 0.0368\n",
      "Epoch 73/1200\n",
      "367/367 [==============================] - 0s 19us/step - loss: 0.0410 - val_loss: 0.0367\n",
      "Epoch 74/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0408 - val_loss: 0.0367\n",
      "Epoch 75/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0407 - val_loss: 0.0365\n",
      "Epoch 76/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0405 - val_loss: 0.0364\n",
      "Epoch 77/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0403 - val_loss: 0.0363\n",
      "Epoch 78/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0402 - val_loss: 0.0363\n",
      "Epoch 79/1200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "367/367 [==============================] - 0s 19us/step - loss: 0.0400 - val_loss: 0.0361\n",
      "Epoch 80/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0399 - val_loss: 0.0360\n",
      "Epoch 81/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0398 - val_loss: 0.0359\n",
      "Epoch 82/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0395 - val_loss: 0.0358\n",
      "Epoch 83/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0395 - val_loss: 0.0358\n",
      "Epoch 84/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0394 - val_loss: 0.0357\n",
      "Epoch 85/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0392 - val_loss: 0.0356\n",
      "Epoch 86/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0392 - val_loss: 0.0356\n",
      "Epoch 87/1200\n",
      "367/367 [==============================] - 0s 22us/step - loss: 0.0391 - val_loss: 0.0354\n",
      "Epoch 88/1200\n",
      "367/367 [==============================] - 0s 21us/step - loss: 0.0389 - val_loss: 0.0353\n",
      "Epoch 89/1200\n",
      "367/367 [==============================] - 0s 21us/step - loss: 0.0387 - val_loss: 0.0353\n",
      "Epoch 90/1200\n",
      "367/367 [==============================] - 0s 19us/step - loss: 0.0387 - val_loss: 0.0352\n",
      "Epoch 91/1200\n",
      "367/367 [==============================] - 0s 23us/step - loss: 0.0385 - val_loss: 0.0351\n",
      "Epoch 92/1200\n",
      "367/367 [==============================] - 0s 28us/step - loss: 0.0384 - val_loss: 0.0350\n",
      "Epoch 93/1200\n",
      "367/367 [==============================] - 0s 33us/step - loss: 0.0383 - val_loss: 0.0349\n",
      "Epoch 94/1200\n",
      "367/367 [==============================] - 0s 30us/step - loss: 0.0383 - val_loss: 0.0349\n",
      "Epoch 95/1200\n",
      "367/367 [==============================] - 0s 24us/step - loss: 0.0383 - val_loss: 0.0348\n",
      "Epoch 96/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0380 - val_loss: 0.0349\n",
      "Epoch 97/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0381 - val_loss: 0.0349\n",
      "Epoch 98/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0381 - val_loss: 0.0346\n",
      "Epoch 99/1200\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.039 - 0s 15us/step - loss: 0.0378 - val_loss: 0.0344\n",
      "Epoch 100/1200\n",
      "367/367 [==============================] - 0s 23us/step - loss: 0.0377 - val_loss: 0.0344\n",
      "Epoch 101/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0377 - val_loss: 0.0344\n",
      "Epoch 102/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0376 - val_loss: 0.0342\n",
      "Epoch 103/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0374 - val_loss: 0.0343\n",
      "Epoch 104/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0374 - val_loss: 0.0344\n",
      "Epoch 105/1200\n",
      "367/367 [==============================] - 0s 20us/step - loss: 0.0375 - val_loss: 0.0343\n",
      "Epoch 106/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0372 - val_loss: 0.0341\n",
      "Epoch 107/1200\n",
      "367/367 [==============================] - 0s 20us/step - loss: 0.0371 - val_loss: 0.0341\n",
      "Epoch 108/1200\n",
      "367/367 [==============================] - 0s 25us/step - loss: 0.0371 - val_loss: 0.0341\n",
      "Epoch 109/1200\n",
      "367/367 [==============================] - 0s 19us/step - loss: 0.0370 - val_loss: 0.0340\n",
      "Epoch 110/1200\n",
      "367/367 [==============================] - 0s 21us/step - loss: 0.0369 - val_loss: 0.0340\n",
      "Epoch 111/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0368 - val_loss: 0.0339\n",
      "Epoch 112/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0368 - val_loss: 0.0339\n",
      "Epoch 113/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0367 - val_loss: 0.0338\n",
      "Epoch 114/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0366 - val_loss: 0.0337\n",
      "Epoch 115/1200\n",
      "367/367 [==============================] - 0s 14us/step - loss: 0.0365 - val_loss: 0.0339\n",
      "Epoch 116/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0365 - val_loss: 0.0339\n",
      "Epoch 117/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0365 - val_loss: 0.0336\n",
      "Epoch 118/1200\n",
      "367/367 [==============================] - 0s 20us/step - loss: 0.0364 - val_loss: 0.0334\n",
      "Epoch 119/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0362 - val_loss: 0.0334\n",
      "Epoch 120/1200\n",
      "367/367 [==============================] - 0s 14us/step - loss: 0.0363 - val_loss: 0.0333\n",
      "Epoch 121/1200\n",
      "367/367 [==============================] - 0s 23us/step - loss: 0.0361 - val_loss: 0.0333\n",
      "Epoch 122/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0361 - val_loss: 0.0333\n",
      "Epoch 123/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0360 - val_loss: 0.0331\n",
      "Epoch 124/1200\n",
      "367/367 [==============================] - 0s 19us/step - loss: 0.0358 - val_loss: 0.0330\n",
      "Epoch 125/1200\n",
      "367/367 [==============================] - 0s 20us/step - loss: 0.0359 - val_loss: 0.0329\n",
      "Epoch 126/1200\n",
      "367/367 [==============================] - 0s 24us/step - loss: 0.0357 - val_loss: 0.0329\n",
      "Epoch 127/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0356 - val_loss: 0.0329\n",
      "Epoch 128/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0356 - val_loss: 0.0329\n",
      "Epoch 129/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0356 - val_loss: 0.0329\n",
      "Epoch 130/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0355 - val_loss: 0.0327\n",
      "Epoch 131/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0353 - val_loss: 0.0326\n",
      "Epoch 132/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0354 - val_loss: 0.0326\n",
      "Epoch 133/1200\n",
      "367/367 [==============================] - 0s 21us/step - loss: 0.0352 - val_loss: 0.0325\n",
      "Epoch 134/1200\n",
      "367/367 [==============================] - 0s 21us/step - loss: 0.0351 - val_loss: 0.0327\n",
      "Epoch 135/1200\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.038 - 0s 18us/step - loss: 0.0352 - val_loss: 0.0326\n",
      "Epoch 136/1200\n",
      "367/367 [==============================] - 0s 19us/step - loss: 0.0352 - val_loss: 0.0324\n",
      "Epoch 137/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0350 - val_loss: 0.0324\n",
      "Epoch 138/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0351 - val_loss: 0.0324\n",
      "Epoch 139/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0350 - val_loss: 0.0322\n",
      "Epoch 140/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0349 - val_loss: 0.0323\n",
      "Epoch 141/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0348 - val_loss: 0.0322\n",
      "Epoch 142/1200\n",
      "367/367 [==============================] - 0s 22us/step - loss: 0.0346 - val_loss: 0.0322\n",
      "Epoch 143/1200\n",
      "367/367 [==============================] - 0s 21us/step - loss: 0.0345 - val_loss: 0.0322\n",
      "Epoch 144/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0346 - val_loss: 0.0322\n",
      "Epoch 145/1200\n",
      "367/367 [==============================] - 0s 20us/step - loss: 0.0346 - val_loss: 0.0321\n",
      "Epoch 146/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0344 - val_loss: 0.0322\n",
      "Epoch 147/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0344 - val_loss: 0.0322\n",
      "Epoch 148/1200\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.034 - 0s 19us/step - loss: 0.0344 - val_loss: 0.0321\n",
      "Epoch 149/1200\n",
      "367/367 [==============================] - 0s 20us/step - loss: 0.0342 - val_loss: 0.0320\n",
      "Epoch 150/1200\n",
      "367/367 [==============================] - 0s 29us/step - loss: 0.0343 - val_loss: 0.0320\n",
      "Epoch 151/1200\n",
      "367/367 [==============================] - 0s 30us/step - loss: 0.0341 - val_loss: 0.0319\n",
      "Epoch 152/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0341 - val_loss: 0.0320\n",
      "Epoch 153/1200\n",
      "367/367 [==============================] - 0s 31us/step - loss: 0.0342 - val_loss: 0.0319\n",
      "Epoch 154/1200\n",
      "367/367 [==============================] - 0s 31us/step - loss: 0.0340 - val_loss: 0.0319\n",
      "Epoch 155/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0341 - val_loss: 0.0320\n",
      "Epoch 156/1200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "367/367 [==============================] - 0s 23us/step - loss: 0.0340 - val_loss: 0.0317\n",
      "Epoch 157/1200\n",
      "367/367 [==============================] - 0s 30us/step - loss: 0.0338 - val_loss: 0.0317\n",
      "Epoch 158/1200\n",
      "367/367 [==============================] - 0s 32us/step - loss: 0.0338 - val_loss: 0.0317\n",
      "Epoch 159/1200\n",
      "367/367 [==============================] - 0s 25us/step - loss: 0.0338 - val_loss: 0.0316\n",
      "Epoch 160/1200\n",
      "367/367 [==============================] - 0s 23us/step - loss: 0.0337 - val_loss: 0.0316\n",
      "Epoch 161/1200\n",
      "367/367 [==============================] - 0s 30us/step - loss: 0.0337 - val_loss: 0.0316\n",
      "Epoch 162/1200\n",
      "367/367 [==============================] - 0s 21us/step - loss: 0.0337 - val_loss: 0.0314\n",
      "Epoch 163/1200\n",
      "367/367 [==============================] - 0s 20us/step - loss: 0.0335 - val_loss: 0.0313\n",
      "Epoch 164/1200\n",
      "367/367 [==============================] - 0s 26us/step - loss: 0.0335 - val_loss: 0.0313\n",
      "Epoch 165/1200\n",
      "367/367 [==============================] - 0s 24us/step - loss: 0.0335 - val_loss: 0.0312\n",
      "Epoch 166/1200\n",
      "367/367 [==============================] - 0s 28us/step - loss: 0.0334 - val_loss: 0.0311\n",
      "Epoch 167/1200\n",
      "367/367 [==============================] - 0s 30us/step - loss: 0.0334 - val_loss: 0.0311\n",
      "Epoch 168/1200\n",
      "367/367 [==============================] - 0s 23us/step - loss: 0.0333 - val_loss: 0.0311\n",
      "Epoch 169/1200\n",
      "367/367 [==============================] - 0s 22us/step - loss: 0.0333 - val_loss: 0.0311\n",
      "Epoch 170/1200\n",
      "367/367 [==============================] - 0s 29us/step - loss: 0.0332 - val_loss: 0.0310\n",
      "Epoch 171/1200\n",
      "367/367 [==============================] - 0s 22us/step - loss: 0.0332 - val_loss: 0.0312\n",
      "Epoch 172/1200\n",
      "367/367 [==============================] - 0s 25us/step - loss: 0.0332 - val_loss: 0.0311\n",
      "Epoch 173/1200\n",
      "367/367 [==============================] - 0s 23us/step - loss: 0.0331 - val_loss: 0.0309\n",
      "Epoch 174/1200\n",
      "367/367 [==============================] - 0s 19us/step - loss: 0.0331 - val_loss: 0.0309\n",
      "Epoch 175/1200\n",
      "367/367 [==============================] - 0s 28us/step - loss: 0.0329 - val_loss: 0.0309\n",
      "Epoch 176/1200\n",
      "367/367 [==============================] - 0s 31us/step - loss: 0.0330 - val_loss: 0.0309\n",
      "Epoch 177/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0329 - val_loss: 0.0308\n",
      "Epoch 178/1200\n",
      "367/367 [==============================] - 0s 30us/step - loss: 0.0328 - val_loss: 0.0308\n",
      "Epoch 179/1200\n",
      "367/367 [==============================] - 0s 24us/step - loss: 0.0328 - val_loss: 0.0308\n",
      "Epoch 180/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0327 - val_loss: 0.0307\n",
      "Epoch 181/1200\n",
      "367/367 [==============================] - 0s 22us/step - loss: 0.0327 - val_loss: 0.0306\n",
      "Epoch 182/1200\n",
      "367/367 [==============================] - 0s 20us/step - loss: 0.0327 - val_loss: 0.0305\n",
      "Epoch 183/1200\n",
      "367/367 [==============================] - 0s 26us/step - loss: 0.0326 - val_loss: 0.0305\n",
      "Epoch 184/1200\n",
      "367/367 [==============================] - 0s 19us/step - loss: 0.0326 - val_loss: 0.0305\n",
      "Epoch 185/1200\n",
      "367/367 [==============================] - 0s 20us/step - loss: 0.0326 - val_loss: 0.0304\n",
      "Epoch 186/1200\n",
      "367/367 [==============================] - 0s 22us/step - loss: 0.0324 - val_loss: 0.0304\n",
      "Epoch 187/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0324 - val_loss: 0.0305\n",
      "Epoch 188/1200\n",
      "367/367 [==============================] - 0s 28us/step - loss: 0.0324 - val_loss: 0.0304\n",
      "Epoch 189/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0323 - val_loss: 0.0303\n",
      "Epoch 190/1200\n",
      "367/367 [==============================] - 0s 29us/step - loss: 0.0322 - val_loss: 0.0303\n",
      "Epoch 191/1200\n",
      "367/367 [==============================] - 0s 19us/step - loss: 0.0322 - val_loss: 0.0304\n",
      "Epoch 192/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0323 - val_loss: 0.0302\n",
      "Epoch 193/1200\n",
      "367/367 [==============================] - 0s 25us/step - loss: 0.0322 - val_loss: 0.0301\n",
      "Epoch 194/1200\n",
      "367/367 [==============================] - 0s 24us/step - loss: 0.0321 - val_loss: 0.0301\n",
      "Epoch 195/1200\n",
      "367/367 [==============================] - 0s 25us/step - loss: 0.0321 - val_loss: 0.0301\n",
      "Epoch 196/1200\n",
      "367/367 [==============================] - 0s 25us/step - loss: 0.0321 - val_loss: 0.0300\n",
      "Epoch 197/1200\n",
      "367/367 [==============================] - 0s 26us/step - loss: 0.0319 - val_loss: 0.0300\n",
      "Epoch 198/1200\n",
      "367/367 [==============================] - 0s 31us/step - loss: 0.0319 - val_loss: 0.0303\n",
      "Epoch 199/1200\n",
      "367/367 [==============================] - 0s 30us/step - loss: 0.0321 - val_loss: 0.0302\n",
      "Epoch 200/1200\n",
      "367/367 [==============================] - 0s 21us/step - loss: 0.0320 - val_loss: 0.0298\n",
      "Epoch 201/1200\n",
      "367/367 [==============================] - 0s 25us/step - loss: 0.0317 - val_loss: 0.0298\n",
      "Epoch 202/1200\n",
      "367/367 [==============================] - 0s 28us/step - loss: 0.0318 - val_loss: 0.0298\n",
      "Epoch 203/1200\n",
      "367/367 [==============================] - 0s 24us/step - loss: 0.0318 - val_loss: 0.0296\n",
      "Epoch 204/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0316 - val_loss: 0.0296\n",
      "Epoch 205/1200\n",
      "367/367 [==============================] - 0s 24us/step - loss: 0.0316 - val_loss: 0.0295\n",
      "Epoch 206/1200\n",
      "367/367 [==============================] - 0s 19us/step - loss: 0.0315 - val_loss: 0.0295\n",
      "Epoch 207/1200\n",
      "367/367 [==============================] - 0s 14us/step - loss: 0.0316 - val_loss: 0.0295\n",
      "Epoch 208/1200\n",
      "367/367 [==============================] - 0s 14us/step - loss: 0.0315 - val_loss: 0.0295\n",
      "Epoch 209/1200\n",
      "367/367 [==============================] - 0s 25us/step - loss: 0.0314 - val_loss: 0.0295\n",
      "Epoch 210/1200\n",
      "367/367 [==============================] - 0s 23us/step - loss: 0.0314 - val_loss: 0.0294\n",
      "Epoch 211/1200\n",
      "367/367 [==============================] - 0s 21us/step - loss: 0.0313 - val_loss: 0.0293\n",
      "Epoch 212/1200\n",
      "367/367 [==============================] - 0s 19us/step - loss: 0.0313 - val_loss: 0.0293\n",
      "Epoch 213/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0313 - val_loss: 0.0292\n",
      "Epoch 214/1200\n",
      "367/367 [==============================] - 0s 33us/step - loss: 0.0313 - val_loss: 0.0292\n",
      "Epoch 215/1200\n",
      "367/367 [==============================] - 0s 27us/step - loss: 0.0312 - val_loss: 0.0292\n",
      "Epoch 216/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0312 - val_loss: 0.0293\n",
      "Epoch 217/1200\n",
      "367/367 [==============================] - 0s 28us/step - loss: 0.0312 - val_loss: 0.0292\n",
      "Epoch 218/1200\n",
      "367/367 [==============================] - 0s 30us/step - loss: 0.0310 - val_loss: 0.0292\n",
      "Epoch 219/1200\n",
      "367/367 [==============================] - 0s 19us/step - loss: 0.0311 - val_loss: 0.0293\n",
      "Epoch 220/1200\n",
      "367/367 [==============================] - 0s 25us/step - loss: 0.0312 - val_loss: 0.0292\n",
      "Epoch 221/1200\n",
      "367/367 [==============================] - 0s 28us/step - loss: 0.0310 - val_loss: 0.0291\n",
      "Epoch 222/1200\n",
      "367/367 [==============================] - 0s 19us/step - loss: 0.0309 - val_loss: 0.0291\n",
      "Epoch 223/1200\n",
      "367/367 [==============================] - 0s 27us/step - loss: 0.0309 - val_loss: 0.0291\n",
      "Epoch 224/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0308 - val_loss: 0.0291\n",
      "Epoch 225/1200\n",
      "367/367 [==============================] - 0s 25us/step - loss: 0.0308 - val_loss: 0.0291\n",
      "Epoch 226/1200\n",
      "367/367 [==============================] - 0s 24us/step - loss: 0.0307 - val_loss: 0.0290\n",
      "Epoch 227/1200\n",
      "367/367 [==============================] - 0s 19us/step - loss: 0.0307 - val_loss: 0.0290\n",
      "Epoch 228/1200\n",
      "367/367 [==============================] - 0s 28us/step - loss: 0.0306 - val_loss: 0.0289\n",
      "Epoch 229/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0305 - val_loss: 0.0288\n",
      "Epoch 230/1200\n",
      "367/367 [==============================] - 0s 28us/step - loss: 0.0306 - val_loss: 0.0288\n",
      "Epoch 231/1200\n",
      "367/367 [==============================] - 0s 25us/step - loss: 0.0305 - val_loss: 0.0287\n",
      "Epoch 232/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0304 - val_loss: 0.0287\n",
      "Epoch 233/1200\n",
      "367/367 [==============================] - 0s 20us/step - loss: 0.0304 - val_loss: 0.0286\n",
      "Epoch 234/1200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "367/367 [==============================] - 0s 16us/step - loss: 0.0303 - val_loss: 0.0285\n",
      "Epoch 235/1200\n",
      "367/367 [==============================] - 0s 23us/step - loss: 0.0303 - val_loss: 0.0286\n",
      "Epoch 236/1200\n",
      "367/367 [==============================] - 0s 19us/step - loss: 0.0303 - val_loss: 0.0285\n",
      "Epoch 237/1200\n",
      "367/367 [==============================] - 0s 28us/step - loss: 0.0302 - val_loss: 0.0284\n",
      "Epoch 238/1200\n",
      "367/367 [==============================] - 0s 20us/step - loss: 0.0302 - val_loss: 0.0283\n",
      "Epoch 239/1200\n",
      "367/367 [==============================] - 0s 22us/step - loss: 0.0302 - val_loss: 0.0283\n",
      "Epoch 240/1200\n",
      "367/367 [==============================] - 0s 24us/step - loss: 0.0301 - val_loss: 0.0284\n",
      "Epoch 241/1200\n",
      "367/367 [==============================] - 0s 22us/step - loss: 0.0301 - val_loss: 0.0284\n",
      "Epoch 242/1200\n",
      "367/367 [==============================] - 0s 24us/step - loss: 0.0301 - val_loss: 0.0283\n",
      "Epoch 243/1200\n",
      "367/367 [==============================] - 0s 20us/step - loss: 0.0300 - val_loss: 0.0282\n",
      "Epoch 244/1200\n",
      "367/367 [==============================] - 0s 26us/step - loss: 0.0300 - val_loss: 0.0281\n",
      "Epoch 245/1200\n",
      "367/367 [==============================] - 0s 29us/step - loss: 0.0298 - val_loss: 0.0282\n",
      "Epoch 246/1200\n",
      "367/367 [==============================] - 0s 25us/step - loss: 0.0299 - val_loss: 0.0283\n",
      "Epoch 247/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0300 - val_loss: 0.0281\n",
      "Epoch 248/1200\n",
      "367/367 [==============================] - 0s 26us/step - loss: 0.0299 - val_loss: 0.0280\n",
      "Epoch 249/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0298 - val_loss: 0.0278\n",
      "Epoch 250/1200\n",
      "367/367 [==============================] - 0s 20us/step - loss: 0.0297 - val_loss: 0.0278\n",
      "Epoch 251/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0297 - val_loss: 0.0277\n",
      "Epoch 252/1200\n",
      "367/367 [==============================] - 0s 19us/step - loss: 0.0295 - val_loss: 0.0279\n",
      "Epoch 253/1200\n",
      "367/367 [==============================] - 0s 23us/step - loss: 0.0297 - val_loss: 0.0279\n",
      "Epoch 254/1200\n",
      "367/367 [==============================] - 0s 26us/step - loss: 0.0296 - val_loss: 0.0277\n",
      "Epoch 255/1200\n",
      "367/367 [==============================] - 0s 22us/step - loss: 0.0295 - val_loss: 0.0278\n",
      "Epoch 256/1200\n",
      "367/367 [==============================] - 0s 20us/step - loss: 0.0295 - val_loss: 0.0277\n",
      "Epoch 257/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0294 - val_loss: 0.0277\n",
      "Epoch 258/1200\n",
      "367/367 [==============================] - 0s 24us/step - loss: 0.0294 - val_loss: 0.0279\n",
      "Epoch 259/1200\n",
      "367/367 [==============================] - 0s 32us/step - loss: 0.0295 - val_loss: 0.0280\n",
      "Epoch 260/1200\n",
      "367/367 [==============================] - 0s 35us/step - loss: 0.0293 - val_loss: 0.0279\n",
      "Epoch 261/1200\n",
      "367/367 [==============================] - 0s 34us/step - loss: 0.0294 - val_loss: 0.0279\n",
      "Epoch 262/1200\n",
      "367/367 [==============================] - 0s 35us/step - loss: 0.0295 - val_loss: 0.0278\n",
      "Epoch 263/1200\n",
      "367/367 [==============================] - 0s 28us/step - loss: 0.0294 - val_loss: 0.0278\n",
      "Epoch 264/1200\n",
      "367/367 [==============================] - 0s 21us/step - loss: 0.0293 - val_loss: 0.0277\n",
      "Epoch 265/1200\n",
      "367/367 [==============================] - 0s 26us/step - loss: 0.0290 - val_loss: 0.0278\n",
      "Epoch 266/1200\n",
      "367/367 [==============================] - 0s 26us/step - loss: 0.0293 - val_loss: 0.0277\n",
      "Epoch 267/1200\n",
      "367/367 [==============================] - 0s 26us/step - loss: 0.0291 - val_loss: 0.0274\n",
      "Epoch 268/1200\n",
      "367/367 [==============================] - 0s 21us/step - loss: 0.0290 - val_loss: 0.0278\n",
      "Epoch 269/1200\n",
      "367/367 [==============================] - 0s 29us/step - loss: 0.0293 - val_loss: 0.0275\n",
      "Epoch 270/1200\n",
      "367/367 [==============================] - 0s 20us/step - loss: 0.0290 - val_loss: 0.0273\n",
      "Epoch 271/1200\n",
      "367/367 [==============================] - 0s 23us/step - loss: 0.0288 - val_loss: 0.0274\n",
      "Epoch 272/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0288 - val_loss: 0.0271\n",
      "Epoch 273/1200\n",
      "367/367 [==============================] - 0s 14us/step - loss: 0.0287 - val_loss: 0.0271\n",
      "Epoch 274/1200\n",
      "367/367 [==============================] - 0s 22us/step - loss: 0.0287 - val_loss: 0.0271\n",
      "Epoch 275/1200\n",
      "367/367 [==============================] - 0s 24us/step - loss: 0.0287 - val_loss: 0.0270\n",
      "Epoch 276/1200\n",
      "367/367 [==============================] - 0s 21us/step - loss: 0.0287 - val_loss: 0.0271\n",
      "Epoch 277/1200\n",
      "367/367 [==============================] - 0s 20us/step - loss: 0.0286 - val_loss: 0.0270\n",
      "Epoch 278/1200\n",
      "367/367 [==============================] - 0s 19us/step - loss: 0.0286 - val_loss: 0.0269\n",
      "Epoch 279/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0286 - val_loss: 0.0269\n",
      "Epoch 280/1200\n",
      "367/367 [==============================] - 0s 19us/step - loss: 0.0285 - val_loss: 0.0268\n",
      "Epoch 281/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0284 - val_loss: 0.0268\n",
      "Epoch 282/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0284 - val_loss: 0.0270\n",
      "Epoch 283/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0286 - val_loss: 0.0268\n",
      "Epoch 284/1200\n",
      "367/367 [==============================] - 0s 24us/step - loss: 0.0284 - val_loss: 0.0266\n",
      "Epoch 285/1200\n",
      "367/367 [==============================] - 0s 21us/step - loss: 0.0282 - val_loss: 0.0266\n",
      "Epoch 286/1200\n",
      "367/367 [==============================] - 0s 20us/step - loss: 0.0282 - val_loss: 0.0265\n",
      "Epoch 287/1200\n",
      "367/367 [==============================] - 0s 19us/step - loss: 0.0282 - val_loss: 0.0265\n",
      "Epoch 288/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0281 - val_loss: 0.0265\n",
      "Epoch 289/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0281 - val_loss: 0.0265\n",
      "Epoch 290/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0280 - val_loss: 0.0265\n",
      "Epoch 291/1200\n",
      "367/367 [==============================] - 0s 20us/step - loss: 0.0280 - val_loss: 0.0266\n",
      "Epoch 292/1200\n",
      "367/367 [==============================] - 0s 19us/step - loss: 0.0280 - val_loss: 0.0266\n",
      "Epoch 293/1200\n",
      "367/367 [==============================] - 0s 19us/step - loss: 0.0280 - val_loss: 0.0266\n",
      "Epoch 294/1200\n",
      "367/367 [==============================] - 0s 21us/step - loss: 0.0279 - val_loss: 0.0267\n",
      "Epoch 295/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0279 - val_loss: 0.0266\n",
      "Epoch 296/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0278 - val_loss: 0.0267\n",
      "Epoch 297/1200\n",
      "367/367 [==============================] - 0s 14us/step - loss: 0.0279 - val_loss: 0.0266\n",
      "Epoch 298/1200\n",
      "367/367 [==============================] - 0s 20us/step - loss: 0.0278 - val_loss: 0.0265\n",
      "Epoch 299/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0277 - val_loss: 0.0264\n",
      "Epoch 300/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0277 - val_loss: 0.0263\n",
      "Epoch 301/1200\n",
      "367/367 [==============================] - 0s 23us/step - loss: 0.0277 - val_loss: 0.0263\n",
      "Epoch 302/1200\n",
      "367/367 [==============================] - 0s 19us/step - loss: 0.0276 - val_loss: 0.0262\n",
      "Epoch 303/1200\n",
      "367/367 [==============================] - 0s 19us/step - loss: 0.0276 - val_loss: 0.0260\n",
      "Epoch 304/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0275 - val_loss: 0.0259\n",
      "Epoch 305/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0274 - val_loss: 0.0259\n",
      "Epoch 306/1200\n",
      "367/367 [==============================] - 0s 20us/step - loss: 0.0274 - val_loss: 0.0259\n",
      "Epoch 307/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0274 - val_loss: 0.0257\n",
      "Epoch 308/1200\n",
      "367/367 [==============================] - 0s 20us/step - loss: 0.0274 - val_loss: 0.0257\n",
      "Epoch 309/1200\n",
      "367/367 [==============================] - 0s 28us/step - loss: 0.0273 - val_loss: 0.0257\n",
      "Epoch 310/1200\n",
      "367/367 [==============================] - 0s 20us/step - loss: 0.0273 - val_loss: 0.0257\n",
      "Epoch 311/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0273 - val_loss: 0.0257\n",
      "Epoch 312/1200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "367/367 [==============================] - 0s 17us/step - loss: 0.0272 - val_loss: 0.0257\n",
      "Epoch 313/1200\n",
      "367/367 [==============================] - 0s 22us/step - loss: 0.0271 - val_loss: 0.0257\n",
      "Epoch 314/1200\n",
      "367/367 [==============================] - 0s 19us/step - loss: 0.0272 - val_loss: 0.0257\n",
      "Epoch 315/1200\n",
      "367/367 [==============================] - 0s 23us/step - loss: 0.0271 - val_loss: 0.0257\n",
      "Epoch 316/1200\n",
      "367/367 [==============================] - 0s 25us/step - loss: 0.0270 - val_loss: 0.0257\n",
      "Epoch 317/1200\n",
      "367/367 [==============================] - 0s 21us/step - loss: 0.0271 - val_loss: 0.0257\n",
      "Epoch 318/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0270 - val_loss: 0.0256\n",
      "Epoch 319/1200\n",
      "367/367 [==============================] - 0s 20us/step - loss: 0.0270 - val_loss: 0.0256\n",
      "Epoch 320/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0269 - val_loss: 0.0255\n",
      "Epoch 321/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0269 - val_loss: 0.0256\n",
      "Epoch 322/1200\n",
      "367/367 [==============================] - 0s 20us/step - loss: 0.0269 - val_loss: 0.0254\n",
      "Epoch 323/1200\n",
      "367/367 [==============================] - 0s 27us/step - loss: 0.0268 - val_loss: 0.0253\n",
      "Epoch 324/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0268 - val_loss: 0.0255\n",
      "Epoch 325/1200\n",
      "367/367 [==============================] - 0s 22us/step - loss: 0.0270 - val_loss: 0.0252\n",
      "Epoch 326/1200\n",
      "367/367 [==============================] - 0s 14us/step - loss: 0.0267 - val_loss: 0.0252\n",
      "Epoch 327/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0269 - val_loss: 0.0254\n",
      "Epoch 328/1200\n",
      "367/367 [==============================] - 0s 19us/step - loss: 0.0268 - val_loss: 0.0251\n",
      "Epoch 329/1200\n",
      "367/367 [==============================] - 0s 19us/step - loss: 0.0265 - val_loss: 0.0252\n",
      "Epoch 330/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0267 - val_loss: 0.0252\n",
      "Epoch 331/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0265 - val_loss: 0.0252\n",
      "Epoch 332/1200\n",
      "367/367 [==============================] - 0s 19us/step - loss: 0.0264 - val_loss: 0.0253\n",
      "Epoch 333/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0265 - val_loss: 0.0252\n",
      "Epoch 334/1200\n",
      "367/367 [==============================] - 0s 22us/step - loss: 0.0264 - val_loss: 0.0252\n",
      "Epoch 335/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0264 - val_loss: 0.0252\n",
      "Epoch 336/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0264 - val_loss: 0.0251\n",
      "Epoch 337/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0262 - val_loss: 0.0251\n",
      "Epoch 338/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0264 - val_loss: 0.0252\n",
      "Epoch 339/1200\n",
      "367/367 [==============================] - 0s 24us/step - loss: 0.0262 - val_loss: 0.0251\n",
      "Epoch 340/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0263 - val_loss: 0.0253\n",
      "Epoch 341/1200\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.026 - 0s 18us/step - loss: 0.0264 - val_loss: 0.0249\n",
      "Epoch 342/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0260 - val_loss: 0.0250\n",
      "Epoch 343/1200\n",
      "367/367 [==============================] - 0s 19us/step - loss: 0.0262 - val_loss: 0.0249\n",
      "Epoch 344/1200\n",
      "367/367 [==============================] - 0s 27us/step - loss: 0.0262 - val_loss: 0.0248\n",
      "Epoch 345/1200\n",
      "367/367 [==============================] - 0s 22us/step - loss: 0.0260 - val_loss: 0.0247\n",
      "Epoch 346/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0259 - val_loss: 0.0248\n",
      "Epoch 347/1200\n",
      "367/367 [==============================] - 0s 19us/step - loss: 0.0259 - val_loss: 0.0249\n",
      "Epoch 348/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0260 - val_loss: 0.0246\n",
      "Epoch 349/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0260 - val_loss: 0.0245\n",
      "Epoch 350/1200\n",
      "367/367 [==============================] - 0s 20us/step - loss: 0.0258 - val_loss: 0.0244\n",
      "Epoch 351/1200\n",
      "367/367 [==============================] - 0s 23us/step - loss: 0.0258 - val_loss: 0.0244\n",
      "Epoch 352/1200\n",
      "367/367 [==============================] - 0s 19us/step - loss: 0.0258 - val_loss: 0.0243\n",
      "Epoch 353/1200\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.025 - 0s 16us/step - loss: 0.0257 - val_loss: 0.0243\n",
      "Epoch 354/1200\n",
      "367/367 [==============================] - 0s 28us/step - loss: 0.0257 - val_loss: 0.0243\n",
      "Epoch 355/1200\n",
      "367/367 [==============================] - 0s 56us/step - loss: 0.0256 - val_loss: 0.0242\n",
      "Epoch 356/1200\n",
      "367/367 [==============================] - 0s 46us/step - loss: 0.0256 - val_loss: 0.0243\n",
      "Epoch 357/1200\n",
      "367/367 [==============================] - 0s 29us/step - loss: 0.0256 - val_loss: 0.0242\n",
      "Epoch 358/1200\n",
      "367/367 [==============================] - 0s 26us/step - loss: 0.0256 - val_loss: 0.0242\n",
      "Epoch 359/1200\n",
      "367/367 [==============================] - 0s 34us/step - loss: 0.0255 - val_loss: 0.0242\n",
      "Epoch 360/1200\n",
      "367/367 [==============================] - 0s 32us/step - loss: 0.0254 - val_loss: 0.0243\n",
      "Epoch 361/1200\n",
      "367/367 [==============================] - 0s 37us/step - loss: 0.0256 - val_loss: 0.0242\n",
      "Epoch 362/1200\n",
      "367/367 [==============================] - 0s 35us/step - loss: 0.0255 - val_loss: 0.0241\n",
      "Epoch 363/1200\n",
      "367/367 [==============================] - 0s 33us/step - loss: 0.0253 - val_loss: 0.0242\n",
      "Epoch 364/1200\n",
      "367/367 [==============================] - 0s 58us/step - loss: 0.0254 - val_loss: 0.0241\n",
      "Epoch 365/1200\n",
      "367/367 [==============================] - 0s 40us/step - loss: 0.0252 - val_loss: 0.0242\n",
      "Epoch 366/1200\n",
      "367/367 [==============================] - 0s 28us/step - loss: 0.0253 - val_loss: 0.0242\n",
      "Epoch 367/1200\n",
      "367/367 [==============================] - 0s 28us/step - loss: 0.0255 - val_loss: 0.0240\n",
      "Epoch 368/1200\n",
      "367/367 [==============================] - 0s 43us/step - loss: 0.0252 - val_loss: 0.0239\n",
      "Epoch 369/1200\n",
      "367/367 [==============================] - 0s 36us/step - loss: 0.0251 - val_loss: 0.0239\n",
      "Epoch 370/1200\n",
      "367/367 [==============================] - 0s 26us/step - loss: 0.0251 - val_loss: 0.0239\n",
      "Epoch 371/1200\n",
      "367/367 [==============================] - 0s 32us/step - loss: 0.0251 - val_loss: 0.0238\n",
      "Epoch 372/1200\n",
      "367/367 [==============================] - 0s 19us/step - loss: 0.0250 - val_loss: 0.0238\n",
      "Epoch 373/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0250 - val_loss: 0.0239\n",
      "Epoch 374/1200\n",
      "367/367 [==============================] - 0s 21us/step - loss: 0.0251 - val_loss: 0.0238\n",
      "Epoch 375/1200\n",
      "367/367 [==============================] - 0s 24us/step - loss: 0.0248 - val_loss: 0.0239\n",
      "Epoch 376/1200\n",
      "367/367 [==============================] - 0s 24us/step - loss: 0.0251 - val_loss: 0.0239\n",
      "Epoch 377/1200\n",
      "367/367 [==============================] - 0s 21us/step - loss: 0.0249 - val_loss: 0.0238\n",
      "Epoch 378/1200\n",
      "367/367 [==============================] - 0s 24us/step - loss: 0.0248 - val_loss: 0.0240\n",
      "Epoch 379/1200\n",
      "367/367 [==============================] - 0s 39us/step - loss: 0.0251 - val_loss: 0.0238\n",
      "Epoch 380/1200\n",
      "367/367 [==============================] - 0s 24us/step - loss: 0.0249 - val_loss: 0.0237\n",
      "Epoch 381/1200\n",
      "367/367 [==============================] - 0s 20us/step - loss: 0.0247 - val_loss: 0.0236\n",
      "Epoch 382/1200\n",
      "367/367 [==============================] - 0s 20us/step - loss: 0.0246 - val_loss: 0.0236\n",
      "Epoch 383/1200\n",
      "367/367 [==============================] - 0s 21us/step - loss: 0.0246 - val_loss: 0.0235\n",
      "Epoch 384/1200\n",
      "367/367 [==============================] - 0s 24us/step - loss: 0.0247 - val_loss: 0.0234\n",
      "Epoch 385/1200\n",
      "367/367 [==============================] - 0s 21us/step - loss: 0.0247 - val_loss: 0.0233\n",
      "Epoch 386/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0245 - val_loss: 0.0233\n",
      "Epoch 387/1200\n",
      "367/367 [==============================] - 0s 25us/step - loss: 0.0245 - val_loss: 0.0233\n",
      "Epoch 388/1200\n",
      "367/367 [==============================] - 0s 25us/step - loss: 0.0245 - val_loss: 0.0232\n",
      "Epoch 389/1200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "367/367 [==============================] - 0s 21us/step - loss: 0.0244 - val_loss: 0.0232\n",
      "Epoch 390/1200\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.024 - 0s 33us/step - loss: 0.0245 - val_loss: 0.0231\n",
      "Epoch 391/1200\n",
      "367/367 [==============================] - 0s 29us/step - loss: 0.0244 - val_loss: 0.0231\n",
      "Epoch 392/1200\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.027 - 0s 29us/step - loss: 0.0244 - val_loss: 0.0231\n",
      "Epoch 393/1200\n",
      "367/367 [==============================] - 0s 32us/step - loss: 0.0244 - val_loss: 0.0230\n",
      "Epoch 394/1200\n",
      "367/367 [==============================] - 0s 29us/step - loss: 0.0243 - val_loss: 0.0230\n",
      "Epoch 395/1200\n",
      "367/367 [==============================] - 0s 28us/step - loss: 0.0244 - val_loss: 0.0230\n",
      "Epoch 396/1200\n",
      "367/367 [==============================] - 0s 30us/step - loss: 0.0242 - val_loss: 0.0230\n",
      "Epoch 397/1200\n",
      "367/367 [==============================] - 0s 26us/step - loss: 0.0242 - val_loss: 0.0231\n",
      "Epoch 398/1200\n",
      "367/367 [==============================] - 0s 44us/step - loss: 0.0242 - val_loss: 0.0230\n",
      "Epoch 399/1200\n",
      "367/367 [==============================] - 0s 38us/step - loss: 0.0241 - val_loss: 0.0230\n",
      "Epoch 400/1200\n",
      "367/367 [==============================] - 0s 41us/step - loss: 0.0240 - val_loss: 0.0232\n",
      "Epoch 401/1200\n",
      "367/367 [==============================] - 0s 40us/step - loss: 0.0242 - val_loss: 0.0232\n",
      "Epoch 402/1200\n",
      "367/367 [==============================] - 0s 21us/step - loss: 0.0241 - val_loss: 0.0230\n",
      "Epoch 403/1200\n",
      "367/367 [==============================] - 0s 21us/step - loss: 0.0241 - val_loss: 0.0233\n",
      "Epoch 404/1200\n",
      "367/367 [==============================] - 0s 32us/step - loss: 0.0242 - val_loss: 0.0230\n",
      "Epoch 405/1200\n",
      "367/367 [==============================] - 0s 23us/step - loss: 0.0240 - val_loss: 0.0229\n",
      "Epoch 406/1200\n",
      "367/367 [==============================] - 0s 23us/step - loss: 0.0239 - val_loss: 0.0229\n",
      "Epoch 407/1200\n",
      "367/367 [==============================] - 0s 24us/step - loss: 0.0238 - val_loss: 0.0228\n",
      "Epoch 408/1200\n",
      "367/367 [==============================] - 0s 39us/step - loss: 0.0238 - val_loss: 0.0229\n",
      "Epoch 409/1200\n",
      "367/367 [==============================] - 0s 24us/step - loss: 0.0239 - val_loss: 0.0228\n",
      "Epoch 410/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0237 - val_loss: 0.0227\n",
      "Epoch 411/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0238 - val_loss: 0.0228\n",
      "Epoch 412/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0240 - val_loss: 0.0226\n",
      "Epoch 413/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0236 - val_loss: 0.0226\n",
      "Epoch 414/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0237 - val_loss: 0.0226\n",
      "Epoch 415/1200\n",
      "367/367 [==============================] - 0s 19us/step - loss: 0.0236 - val_loss: 0.0228\n",
      "Epoch 416/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0237 - val_loss: 0.0226\n",
      "Epoch 417/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0234 - val_loss: 0.0227\n",
      "Epoch 418/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0237 - val_loss: 0.0228\n",
      "Epoch 419/1200\n",
      "367/367 [==============================] - 0s 23us/step - loss: 0.0237 - val_loss: 0.0224\n",
      "Epoch 420/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0233 - val_loss: 0.0228\n",
      "Epoch 421/1200\n",
      "367/367 [==============================] - 0s 21us/step - loss: 0.0236 - val_loss: 0.0230\n",
      "Epoch 422/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0237 - val_loss: 0.0225\n",
      "Epoch 423/1200\n",
      "367/367 [==============================] - 0s 23us/step - loss: 0.0234 - val_loss: 0.0224\n",
      "Epoch 424/1200\n",
      "367/367 [==============================] - 0s 20us/step - loss: 0.0233 - val_loss: 0.0224\n",
      "Epoch 425/1200\n",
      "367/367 [==============================] - 0s 31us/step - loss: 0.0233 - val_loss: 0.0223\n",
      "Epoch 426/1200\n",
      "367/367 [==============================] - 0s 20us/step - loss: 0.0232 - val_loss: 0.0224\n",
      "Epoch 427/1200\n",
      "367/367 [==============================] - 0s 22us/step - loss: 0.0232 - val_loss: 0.0226\n",
      "Epoch 428/1200\n",
      "367/367 [==============================] - 0s 27us/step - loss: 0.0233 - val_loss: 0.0223\n",
      "Epoch 429/1200\n",
      "367/367 [==============================] - 0s 25us/step - loss: 0.0231 - val_loss: 0.0223\n",
      "Epoch 430/1200\n",
      "367/367 [==============================] - 0s 19us/step - loss: 0.0231 - val_loss: 0.0223\n",
      "Epoch 431/1200\n",
      "367/367 [==============================] - 0s 26us/step - loss: 0.0231 - val_loss: 0.0222\n",
      "Epoch 432/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0230 - val_loss: 0.0221\n",
      "Epoch 433/1200\n",
      "367/367 [==============================] - 0s 21us/step - loss: 0.0230 - val_loss: 0.0221\n",
      "Epoch 434/1200\n",
      "367/367 [==============================] - 0s 23us/step - loss: 0.0229 - val_loss: 0.0221\n",
      "Epoch 435/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0229 - val_loss: 0.0221\n",
      "Epoch 436/1200\n",
      "367/367 [==============================] - 0s 19us/step - loss: 0.0230 - val_loss: 0.0220\n",
      "Epoch 437/1200\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.025 - 0s 17us/step - loss: 0.0229 - val_loss: 0.0219\n",
      "Epoch 438/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0228 - val_loss: 0.0219\n",
      "Epoch 439/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0228 - val_loss: 0.0218\n",
      "Epoch 440/1200\n",
      "367/367 [==============================] - 0s 20us/step - loss: 0.0228 - val_loss: 0.0218\n",
      "Epoch 441/1200\n",
      "367/367 [==============================] - 0s 14us/step - loss: 0.0228 - val_loss: 0.0217\n",
      "Epoch 442/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0227 - val_loss: 0.0217\n",
      "Epoch 443/1200\n",
      "367/367 [==============================] - 0s 14us/step - loss: 0.0227 - val_loss: 0.0217\n",
      "Epoch 444/1200\n",
      "367/367 [==============================] - 0s 21us/step - loss: 0.0227 - val_loss: 0.0217\n",
      "Epoch 445/1200\n",
      "367/367 [==============================] - 0s 14us/step - loss: 0.0227 - val_loss: 0.0217\n",
      "Epoch 446/1200\n",
      "367/367 [==============================] - 0s 19us/step - loss: 0.0226 - val_loss: 0.0217\n",
      "Epoch 447/1200\n",
      "367/367 [==============================] - 0s 22us/step - loss: 0.0225 - val_loss: 0.0217\n",
      "Epoch 448/1200\n",
      "367/367 [==============================] - 0s 21us/step - loss: 0.0226 - val_loss: 0.0217\n",
      "Epoch 449/1200\n",
      "367/367 [==============================] - 0s 19us/step - loss: 0.0224 - val_loss: 0.0218\n",
      "Epoch 450/1200\n",
      "367/367 [==============================] - 0s 21us/step - loss: 0.0227 - val_loss: 0.0217\n",
      "Epoch 451/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0225 - val_loss: 0.0218\n",
      "Epoch 452/1200\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.023 - 0s 19us/step - loss: 0.0226 - val_loss: 0.0221\n",
      "Epoch 453/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0229 - val_loss: 0.0216\n",
      "Epoch 454/1200\n",
      "367/367 [==============================] - 0s 21us/step - loss: 0.0224 - val_loss: 0.0216\n",
      "Epoch 455/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0225 - val_loss: 0.0217\n",
      "Epoch 456/1200\n",
      "367/367 [==============================] - 0s 20us/step - loss: 0.0225 - val_loss: 0.0214\n",
      "Epoch 457/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0223 - val_loss: 0.0215\n",
      "Epoch 458/1200\n",
      "367/367 [==============================] - 0s 19us/step - loss: 0.0224 - val_loss: 0.0214\n",
      "Epoch 459/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0223 - val_loss: 0.0214\n",
      "Epoch 460/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0222 - val_loss: 0.0214\n",
      "Epoch 461/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0222 - val_loss: 0.0214\n",
      "Epoch 462/1200\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.021 - 0s 18us/step - loss: 0.0224 - val_loss: 0.0214\n",
      "Epoch 463/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0221 - val_loss: 0.0214\n",
      "Epoch 464/1200\n",
      "367/367 [==============================] - 0s 19us/step - loss: 0.0221 - val_loss: 0.0214\n",
      "Epoch 465/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0222 - val_loss: 0.0213\n",
      "Epoch 466/1200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "367/367 [==============================] - 0s 17us/step - loss: 0.0219 - val_loss: 0.0214\n",
      "Epoch 467/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0221 - val_loss: 0.0215\n",
      "Epoch 468/1200\n",
      "367/367 [==============================] - 0s 20us/step - loss: 0.0222 - val_loss: 0.0211\n",
      "Epoch 469/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0220 - val_loss: 0.0212\n",
      "Epoch 470/1200\n",
      "367/367 [==============================] - 0s 21us/step - loss: 0.0219 - val_loss: 0.0210\n",
      "Epoch 471/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0218 - val_loss: 0.0212\n",
      "Epoch 472/1200\n",
      "367/367 [==============================] - 0s 20us/step - loss: 0.0220 - val_loss: 0.0211\n",
      "Epoch 473/1200\n",
      "367/367 [==============================] - 0s 20us/step - loss: 0.0218 - val_loss: 0.0210\n",
      "Epoch 474/1200\n",
      "367/367 [==============================] - 0s 20us/step - loss: 0.0220 - val_loss: 0.0210\n",
      "Epoch 475/1200\n",
      "367/367 [==============================] - 0s 19us/step - loss: 0.0219 - val_loss: 0.0209\n",
      "Epoch 476/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0217 - val_loss: 0.0211\n",
      "Epoch 477/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0219 - val_loss: 0.0209\n",
      "Epoch 478/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0216 - val_loss: 0.0209\n",
      "Epoch 479/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0218 - val_loss: 0.0212\n",
      "Epoch 480/1200\n",
      "367/367 [==============================] - 0s 25us/step - loss: 0.0220 - val_loss: 0.0208\n",
      "Epoch 481/1200\n",
      "367/367 [==============================] - 0s 21us/step - loss: 0.0216 - val_loss: 0.0209\n",
      "Epoch 482/1200\n",
      "367/367 [==============================] - 0s 21us/step - loss: 0.0216 - val_loss: 0.0209\n",
      "Epoch 483/1200\n",
      "367/367 [==============================] - 0s 23us/step - loss: 0.0215 - val_loss: 0.0207\n",
      "Epoch 484/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0215 - val_loss: 0.0208\n",
      "Epoch 485/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0215 - val_loss: 0.0207\n",
      "Epoch 486/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0214 - val_loss: 0.0207\n",
      "Epoch 487/1200\n",
      "367/367 [==============================] - 0s 20us/step - loss: 0.0214 - val_loss: 0.0207\n",
      "Epoch 488/1200\n",
      "367/367 [==============================] - 0s 22us/step - loss: 0.0213 - val_loss: 0.0208\n",
      "Epoch 489/1200\n",
      "367/367 [==============================] - 0s 14us/step - loss: 0.0214 - val_loss: 0.0208\n",
      "Epoch 490/1200\n",
      "367/367 [==============================] - 0s 20us/step - loss: 0.0213 - val_loss: 0.0208\n",
      "Epoch 491/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0213 - val_loss: 0.0209\n",
      "Epoch 492/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0214 - val_loss: 0.0207\n",
      "Epoch 493/1200\n",
      "367/367 [==============================] - 0s 19us/step - loss: 0.0212 - val_loss: 0.0209\n",
      "Epoch 494/1200\n",
      "367/367 [==============================] - 0s 19us/step - loss: 0.0214 - val_loss: 0.0208\n",
      "Epoch 495/1200\n",
      "367/367 [==============================] - 0s 21us/step - loss: 0.0214 - val_loss: 0.0207\n",
      "Epoch 496/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0211 - val_loss: 0.0208\n",
      "Epoch 497/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0213 - val_loss: 0.0207\n",
      "Epoch 498/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0212 - val_loss: 0.0206\n",
      "Epoch 499/1200\n",
      "367/367 [==============================] - 0s 20us/step - loss: 0.0212 - val_loss: 0.0205\n",
      "Epoch 500/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0210 - val_loss: 0.0204\n",
      "Epoch 501/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0210 - val_loss: 0.0205\n",
      "Epoch 502/1200\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.021 - 0s 14us/step - loss: 0.0210 - val_loss: 0.0203\n",
      "Epoch 503/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0209 - val_loss: 0.0203\n",
      "Epoch 504/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0210 - val_loss: 0.0203\n",
      "Epoch 505/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0210 - val_loss: 0.0202\n",
      "Epoch 506/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0208 - val_loss: 0.0202\n",
      "Epoch 507/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0209 - val_loss: 0.0201\n",
      "Epoch 508/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0208 - val_loss: 0.0201\n",
      "Epoch 509/1200\n",
      "367/367 [==============================] - 0s 14us/step - loss: 0.0209 - val_loss: 0.0201\n",
      "Epoch 510/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0208 - val_loss: 0.0200\n",
      "Epoch 511/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0209 - val_loss: 0.0200\n",
      "Epoch 512/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0207 - val_loss: 0.0199\n",
      "Epoch 513/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0207 - val_loss: 0.0200\n",
      "Epoch 514/1200\n",
      "367/367 [==============================] - 0s 14us/step - loss: 0.0208 - val_loss: 0.0199\n",
      "Epoch 515/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0206 - val_loss: 0.0200\n",
      "Epoch 516/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0208 - val_loss: 0.0199\n",
      "Epoch 517/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0206 - val_loss: 0.0199\n",
      "Epoch 518/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0206 - val_loss: 0.0200\n",
      "Epoch 519/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0206 - val_loss: 0.0199\n",
      "Epoch 520/1200\n",
      "367/367 [==============================] - 0s 14us/step - loss: 0.0207 - val_loss: 0.0199\n",
      "Epoch 521/1200\n",
      "367/367 [==============================] - 0s 19us/step - loss: 0.0205 - val_loss: 0.0199\n",
      "Epoch 522/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0206 - val_loss: 0.0200\n",
      "Epoch 523/1200\n",
      "367/367 [==============================] - 0s 21us/step - loss: 0.0205 - val_loss: 0.0197\n",
      "Epoch 524/1200\n",
      "367/367 [==============================] - 0s 14us/step - loss: 0.0204 - val_loss: 0.0199\n",
      "Epoch 525/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0206 - val_loss: 0.0197\n",
      "Epoch 526/1200\n",
      "367/367 [==============================] - 0s 14us/step - loss: 0.0203 - val_loss: 0.0198\n",
      "Epoch 527/1200\n",
      "367/367 [==============================] - 0s 23us/step - loss: 0.0204 - val_loss: 0.0197\n",
      "Epoch 528/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0204 - val_loss: 0.0196\n",
      "Epoch 529/1200\n",
      "367/367 [==============================] - 0s 20us/step - loss: 0.0203 - val_loss: 0.0197\n",
      "Epoch 530/1200\n",
      "367/367 [==============================] - 0s 14us/step - loss: 0.0203 - val_loss: 0.0196\n",
      "Epoch 531/1200\n",
      "367/367 [==============================] - 0s 20us/step - loss: 0.0202 - val_loss: 0.0196\n",
      "Epoch 532/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0202 - val_loss: 0.0196\n",
      "Epoch 533/1200\n",
      "367/367 [==============================] - 0s 19us/step - loss: 0.0201 - val_loss: 0.0195\n",
      "Epoch 534/1200\n",
      "367/367 [==============================] - 0s 21us/step - loss: 0.0201 - val_loss: 0.0195\n",
      "Epoch 535/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0202 - val_loss: 0.0194\n",
      "Epoch 536/1200\n",
      "367/367 [==============================] - 0s 14us/step - loss: 0.0200 - val_loss: 0.0194\n",
      "Epoch 537/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0201 - val_loss: 0.0194\n",
      "Epoch 538/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0201 - val_loss: 0.0194\n",
      "Epoch 539/1200\n",
      "367/367 [==============================] - 0s 22us/step - loss: 0.0201 - val_loss: 0.0194\n",
      "Epoch 540/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0201 - val_loss: 0.0192\n",
      "Epoch 541/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0200 - val_loss: 0.0193\n",
      "Epoch 542/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0199 - val_loss: 0.0192\n",
      "Epoch 543/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0198 - val_loss: 0.0192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 544/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0199 - val_loss: 0.0192\n",
      "Epoch 545/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0199 - val_loss: 0.0191\n",
      "Epoch 546/1200\n",
      "367/367 [==============================] - 0s 20us/step - loss: 0.0198 - val_loss: 0.0192\n",
      "Epoch 547/1200\n",
      "367/367 [==============================] - 0s 19us/step - loss: 0.0199 - val_loss: 0.0191\n",
      "Epoch 548/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0197 - val_loss: 0.0191\n",
      "Epoch 549/1200\n",
      "367/367 [==============================] - 0s 20us/step - loss: 0.0198 - val_loss: 0.0191\n",
      "Epoch 550/1200\n",
      "367/367 [==============================] - 0s 23us/step - loss: 0.0197 - val_loss: 0.0191\n",
      "Epoch 551/1200\n",
      "367/367 [==============================] - 0s 20us/step - loss: 0.0197 - val_loss: 0.0191\n",
      "Epoch 552/1200\n",
      "367/367 [==============================] - 0s 21us/step - loss: 0.0197 - val_loss: 0.0191\n",
      "Epoch 553/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0196 - val_loss: 0.0190\n",
      "Epoch 554/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0196 - val_loss: 0.0191\n",
      "Epoch 555/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0196 - val_loss: 0.0191\n",
      "Epoch 556/1200\n",
      "367/367 [==============================] - 0s 24us/step - loss: 0.0196 - val_loss: 0.0190\n",
      "Epoch 557/1200\n",
      "367/367 [==============================] - 0s 19us/step - loss: 0.0196 - val_loss: 0.0191\n",
      "Epoch 558/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0196 - val_loss: 0.0190\n",
      "Epoch 559/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0197 - val_loss: 0.0192\n",
      "Epoch 560/1200\n",
      "367/367 [==============================] - 0s 20us/step - loss: 0.0195 - val_loss: 0.0190\n",
      "Epoch 561/1200\n",
      "367/367 [==============================] - 0s 14us/step - loss: 0.0195 - val_loss: 0.0192\n",
      "Epoch 562/1200\n",
      "367/367 [==============================] - 0s 19us/step - loss: 0.0196 - val_loss: 0.0189\n",
      "Epoch 563/1200\n",
      "367/367 [==============================] - 0s 20us/step - loss: 0.0194 - val_loss: 0.0190\n",
      "Epoch 564/1200\n",
      "367/367 [==============================] - 0s 20us/step - loss: 0.0194 - val_loss: 0.0191\n",
      "Epoch 565/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0194 - val_loss: 0.0190\n",
      "Epoch 566/1200\n",
      "367/367 [==============================] - 0s 19us/step - loss: 0.0193 - val_loss: 0.0191\n",
      "Epoch 567/1200\n",
      "367/367 [==============================] - 0s 20us/step - loss: 0.0195 - val_loss: 0.0189\n",
      "Epoch 568/1200\n",
      "367/367 [==============================] - 0s 28us/step - loss: 0.0194 - val_loss: 0.0188\n",
      "Epoch 569/1200\n",
      "367/367 [==============================] - 0s 20us/step - loss: 0.0194 - val_loss: 0.0188\n",
      "Epoch 570/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0193 - val_loss: 0.0188\n",
      "Epoch 571/1200\n",
      "367/367 [==============================] - 0s 22us/step - loss: 0.0193 - val_loss: 0.0187\n",
      "Epoch 572/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0193 - val_loss: 0.0186\n",
      "Epoch 573/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0191 - val_loss: 0.0186\n",
      "Epoch 574/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0191 - val_loss: 0.0186\n",
      "Epoch 575/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0191 - val_loss: 0.0185\n",
      "Epoch 576/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0191 - val_loss: 0.0185\n",
      "Epoch 577/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0191 - val_loss: 0.0185\n",
      "Epoch 578/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0190 - val_loss: 0.0185\n",
      "Epoch 579/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0191 - val_loss: 0.0185\n",
      "Epoch 580/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0190 - val_loss: 0.0185\n",
      "Epoch 581/1200\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.019 - 0s 19us/step - loss: 0.0190 - val_loss: 0.0185\n",
      "Epoch 582/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0190 - val_loss: 0.0184\n",
      "Epoch 583/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0191 - val_loss: 0.0185\n",
      "Epoch 584/1200\n",
      "367/367 [==============================] - 0s 31us/step - loss: 0.0190 - val_loss: 0.0184\n",
      "Epoch 585/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0189 - val_loss: 0.0185\n",
      "Epoch 586/1200\n",
      "367/367 [==============================] - 0s 42us/step - loss: 0.0189 - val_loss: 0.0184\n",
      "Epoch 587/1200\n",
      "367/367 [==============================] - 0s 44us/step - loss: 0.0188 - val_loss: 0.0185\n",
      "Epoch 588/1200\n",
      "367/367 [==============================] - 0s 88us/step - loss: 0.0189 - val_loss: 0.0184\n",
      "Epoch 589/1200\n",
      "367/367 [==============================] - 0s 250us/step - loss: 0.0188 - val_loss: 0.0184\n",
      "Epoch 590/1200\n",
      "367/367 [==============================] - 0s 52us/step - loss: 0.0187 - val_loss: 0.0184\n",
      "Epoch 591/1200\n",
      "367/367 [==============================] - 0s 34us/step - loss: 0.0187 - val_loss: 0.0183\n",
      "Epoch 592/1200\n",
      "367/367 [==============================] - 0s 21us/step - loss: 0.0187 - val_loss: 0.0183\n",
      "Epoch 593/1200\n",
      "367/367 [==============================] - 0s 24us/step - loss: 0.0187 - val_loss: 0.0182\n",
      "Epoch 594/1200\n",
      "367/367 [==============================] - 0s 40us/step - loss: 0.0187 - val_loss: 0.0182\n",
      "Epoch 595/1200\n",
      "367/367 [==============================] - 0s 27us/step - loss: 0.0186 - val_loss: 0.0182\n",
      "Epoch 596/1200\n",
      "367/367 [==============================] - 0s 30us/step - loss: 0.0187 - val_loss: 0.0182\n",
      "Epoch 597/1200\n",
      "367/367 [==============================] - 0s 34us/step - loss: 0.0186 - val_loss: 0.0181\n",
      "Epoch 598/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0185 - val_loss: 0.0181\n",
      "Epoch 599/1200\n",
      "367/367 [==============================] - 0s 28us/step - loss: 0.0185 - val_loss: 0.0182\n",
      "Epoch 600/1200\n",
      "367/367 [==============================] - 0s 21us/step - loss: 0.0185 - val_loss: 0.0181\n",
      "Epoch 601/1200\n",
      "367/367 [==============================] - 0s 25us/step - loss: 0.0185 - val_loss: 0.0183\n",
      "Epoch 602/1200\n",
      "367/367 [==============================] - 0s 27us/step - loss: 0.0186 - val_loss: 0.0182\n",
      "Epoch 603/1200\n",
      "367/367 [==============================] - 0s 25us/step - loss: 0.0184 - val_loss: 0.0183\n",
      "Epoch 604/1200\n",
      "367/367 [==============================] - 0s 21us/step - loss: 0.0185 - val_loss: 0.0187\n",
      "Epoch 605/1200\n",
      "367/367 [==============================] - 0s 29us/step - loss: 0.0188 - val_loss: 0.0182\n",
      "Epoch 606/1200\n",
      "367/367 [==============================] - 0s 28us/step - loss: 0.0183 - val_loss: 0.0185\n",
      "Epoch 607/1200\n",
      "367/367 [==============================] - 0s 35us/step - loss: 0.0190 - val_loss: 0.0183\n",
      "Epoch 608/1200\n",
      "367/367 [==============================] - 0s 33us/step - loss: 0.0185 - val_loss: 0.0183\n",
      "Epoch 609/1200\n",
      "367/367 [==============================] - 0s 33us/step - loss: 0.0186 - val_loss: 0.0186\n",
      "Epoch 610/1200\n",
      "367/367 [==============================] - 0s 23us/step - loss: 0.0187 - val_loss: 0.0181\n",
      "Epoch 611/1200\n",
      "367/367 [==============================] - 0s 29us/step - loss: 0.0183 - val_loss: 0.0187\n",
      "Epoch 612/1200\n",
      "367/367 [==============================] - 0s 49us/step - loss: 0.0189 - val_loss: 0.0182\n",
      "Epoch 613/1200\n",
      "367/367 [==============================] - 0s 29us/step - loss: 0.0184 - val_loss: 0.0186\n",
      "Epoch 614/1200\n",
      "367/367 [==============================] - 0s 36us/step - loss: 0.0188 - val_loss: 0.0185\n",
      "Epoch 615/1200\n",
      "367/367 [==============================] - 0s 22us/step - loss: 0.0187 - val_loss: 0.0180\n",
      "Epoch 616/1200\n",
      "367/367 [==============================] - 0s 26us/step - loss: 0.0183 - val_loss: 0.0181\n",
      "Epoch 617/1200\n",
      "367/367 [==============================] - 0s 26us/step - loss: 0.0184 - val_loss: 0.0178\n",
      "Epoch 618/1200\n",
      "367/367 [==============================] - 0s 24us/step - loss: 0.0183 - val_loss: 0.0178\n",
      "Epoch 619/1200\n",
      "367/367 [==============================] - 0s 33us/step - loss: 0.0181 - val_loss: 0.0177\n",
      "Epoch 620/1200\n",
      "367/367 [==============================] - 0s 24us/step - loss: 0.0181 - val_loss: 0.0178\n",
      "Epoch 621/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0181 - val_loss: 0.0177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 622/1200\n",
      "367/367 [==============================] - 0s 23us/step - loss: 0.0180 - val_loss: 0.0178\n",
      "Epoch 623/1200\n",
      "367/367 [==============================] - 0s 26us/step - loss: 0.0182 - val_loss: 0.0177\n",
      "Epoch 624/1200\n",
      "367/367 [==============================] - 0s 26us/step - loss: 0.0181 - val_loss: 0.0176\n",
      "Epoch 625/1200\n",
      "367/367 [==============================] - 0s 20us/step - loss: 0.0180 - val_loss: 0.0176\n",
      "Epoch 626/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0180 - val_loss: 0.0177\n",
      "Epoch 627/1200\n",
      "367/367 [==============================] - 0s 26us/step - loss: 0.0181 - val_loss: 0.0176\n",
      "Epoch 628/1200\n",
      "367/367 [==============================] - 0s 27us/step - loss: 0.0180 - val_loss: 0.0177\n",
      "Epoch 629/1200\n",
      "367/367 [==============================] - 0s 19us/step - loss: 0.0181 - val_loss: 0.0176\n",
      "Epoch 630/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0179 - val_loss: 0.0176\n",
      "Epoch 631/1200\n",
      "367/367 [==============================] - 0s 19us/step - loss: 0.0180 - val_loss: 0.0177\n",
      "Epoch 632/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0179 - val_loss: 0.0176\n",
      "Epoch 633/1200\n",
      "367/367 [==============================] - 0s 24us/step - loss: 0.0180 - val_loss: 0.0176\n",
      "Epoch 634/1200\n",
      "367/367 [==============================] - 0s 21us/step - loss: 0.0179 - val_loss: 0.0175\n",
      "Epoch 635/1200\n",
      "367/367 [==============================] - 0s 23us/step - loss: 0.0178 - val_loss: 0.0176\n",
      "Epoch 636/1200\n",
      "367/367 [==============================] - 0s 21us/step - loss: 0.0178 - val_loss: 0.0176\n",
      "Epoch 637/1200\n",
      "367/367 [==============================] - 0s 23us/step - loss: 0.0178 - val_loss: 0.0175\n",
      "Epoch 638/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0177 - val_loss: 0.0176\n",
      "Epoch 639/1200\n",
      "367/367 [==============================] - 0s 19us/step - loss: 0.0178 - val_loss: 0.0176\n",
      "Epoch 640/1200\n",
      "367/367 [==============================] - 0s 26us/step - loss: 0.0178 - val_loss: 0.0174\n",
      "Epoch 641/1200\n",
      "367/367 [==============================] - 0s 22us/step - loss: 0.0178 - val_loss: 0.0173\n",
      "Epoch 642/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0176 - val_loss: 0.0174\n",
      "Epoch 643/1200\n",
      "367/367 [==============================] - 0s 27us/step - loss: 0.0176 - val_loss: 0.0173\n",
      "Epoch 644/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0176 - val_loss: 0.0173\n",
      "Epoch 645/1200\n",
      "367/367 [==============================] - 0s 20us/step - loss: 0.0176 - val_loss: 0.0172\n",
      "Epoch 646/1200\n",
      "367/367 [==============================] - 0s 22us/step - loss: 0.0175 - val_loss: 0.0172\n",
      "Epoch 647/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0175 - val_loss: 0.0172\n",
      "Epoch 648/1200\n",
      "367/367 [==============================] - 0s 19us/step - loss: 0.0176 - val_loss: 0.0172\n",
      "Epoch 649/1200\n",
      "367/367 [==============================] - 0s 14us/step - loss: 0.0175 - val_loss: 0.0172\n",
      "Epoch 650/1200\n",
      "367/367 [==============================] - 0s 19us/step - loss: 0.0175 - val_loss: 0.0172\n",
      "Epoch 651/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0175 - val_loss: 0.0171\n",
      "Epoch 652/1200\n",
      "367/367 [==============================] - 0s 22us/step - loss: 0.0174 - val_loss: 0.0173\n",
      "Epoch 653/1200\n",
      "367/367 [==============================] - 0s 14us/step - loss: 0.0175 - val_loss: 0.0171\n",
      "Epoch 654/1200\n",
      "367/367 [==============================] - 0s 21us/step - loss: 0.0174 - val_loss: 0.0172\n",
      "Epoch 655/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0174 - val_loss: 0.0172\n",
      "Epoch 656/1200\n",
      "367/367 [==============================] - 0s 26us/step - loss: 0.0174 - val_loss: 0.0171\n",
      "Epoch 657/1200\n",
      "367/367 [==============================] - 0s 33us/step - loss: 0.0174 - val_loss: 0.0172\n",
      "Epoch 658/1200\n",
      "367/367 [==============================] - 0s 49us/step - loss: 0.0173 - val_loss: 0.0171\n",
      "Epoch 659/1200\n",
      "367/367 [==============================] - 0s 146us/step - loss: 0.0173 - val_loss: 0.0172\n",
      "Epoch 660/1200\n",
      "367/367 [==============================] - 0s 85us/step - loss: 0.0173 - val_loss: 0.0171\n",
      "Epoch 661/1200\n",
      "367/367 [==============================] - 0s 54us/step - loss: 0.0172 - val_loss: 0.0171\n",
      "Epoch 662/1200\n",
      "367/367 [==============================] - 0s 41us/step - loss: 0.0172 - val_loss: 0.0171\n",
      "Epoch 663/1200\n",
      "367/367 [==============================] - 0s 41us/step - loss: 0.0172 - val_loss: 0.0170\n",
      "Epoch 664/1200\n",
      "367/367 [==============================] - 0s 26us/step - loss: 0.0172 - val_loss: 0.0170\n",
      "Epoch 665/1200\n",
      "367/367 [==============================] - 0s 36us/step - loss: 0.0172 - val_loss: 0.0170\n",
      "Epoch 666/1200\n",
      "367/367 [==============================] - 0s 35us/step - loss: 0.0172 - val_loss: 0.0170\n",
      "Epoch 667/1200\n",
      "367/367 [==============================] - 0s 29us/step - loss: 0.0171 - val_loss: 0.0169\n",
      "Epoch 668/1200\n",
      "367/367 [==============================] - 0s 21us/step - loss: 0.0171 - val_loss: 0.0170\n",
      "Epoch 669/1200\n",
      "367/367 [==============================] - 0s 22us/step - loss: 0.0172 - val_loss: 0.0169\n",
      "Epoch 670/1200\n",
      "367/367 [==============================] - 0s 31us/step - loss: 0.0171 - val_loss: 0.0170\n",
      "Epoch 671/1200\n",
      "367/367 [==============================] - 0s 21us/step - loss: 0.0172 - val_loss: 0.0170\n",
      "Epoch 672/1200\n",
      "367/367 [==============================] - 0s 22us/step - loss: 0.0172 - val_loss: 0.0169\n",
      "Epoch 673/1200\n",
      "367/367 [==============================] - 0s 27us/step - loss: 0.0171 - val_loss: 0.0171\n",
      "Epoch 674/1200\n",
      "367/367 [==============================] - 0s 22us/step - loss: 0.0172 - val_loss: 0.0169\n",
      "Epoch 675/1200\n",
      "367/367 [==============================] - 0s 26us/step - loss: 0.0170 - val_loss: 0.0169\n",
      "Epoch 676/1200\n",
      "367/367 [==============================] - 0s 19us/step - loss: 0.0170 - val_loss: 0.0169\n",
      "Epoch 677/1200\n",
      "367/367 [==============================] - 0s 19us/step - loss: 0.0171 - val_loss: 0.0167\n",
      "Epoch 678/1200\n",
      "367/367 [==============================] - 0s 23us/step - loss: 0.0169 - val_loss: 0.0167\n",
      "Epoch 679/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0169 - val_loss: 0.0167\n",
      "Epoch 680/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0169 - val_loss: 0.0167\n",
      "Epoch 681/1200\n",
      "367/367 [==============================] - 0s 20us/step - loss: 0.0169 - val_loss: 0.0168\n",
      "Epoch 682/1200\n",
      "367/367 [==============================] - 0s 20us/step - loss: 0.0170 - val_loss: 0.0167\n",
      "Epoch 683/1200\n",
      "367/367 [==============================] - 0s 19us/step - loss: 0.0169 - val_loss: 0.0166\n",
      "Epoch 684/1200\n",
      "367/367 [==============================] - 0s 20us/step - loss: 0.0168 - val_loss: 0.0167\n",
      "Epoch 685/1200\n",
      "367/367 [==============================] - 0s 22us/step - loss: 0.0169 - val_loss: 0.0166\n",
      "Epoch 686/1200\n",
      "367/367 [==============================] - 0s 22us/step - loss: 0.0167 - val_loss: 0.0166\n",
      "Epoch 687/1200\n",
      "367/367 [==============================] - 0s 23us/step - loss: 0.0169 - val_loss: 0.0167\n",
      "Epoch 688/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0169 - val_loss: 0.0165\n",
      "Epoch 689/1200\n",
      "367/367 [==============================] - 0s 28us/step - loss: 0.0167 - val_loss: 0.0165\n",
      "Epoch 690/1200\n",
      "367/367 [==============================] - 0s 25us/step - loss: 0.0167 - val_loss: 0.0165\n",
      "Epoch 691/1200\n",
      "367/367 [==============================] - 0s 26us/step - loss: 0.0168 - val_loss: 0.0165\n",
      "Epoch 692/1200\n",
      "367/367 [==============================] - 0s 31us/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 693/1200\n",
      "367/367 [==============================] - 0s 25us/step - loss: 0.0168 - val_loss: 0.0165\n",
      "Epoch 694/1200\n",
      "367/367 [==============================] - 0s 23us/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 695/1200\n",
      "367/367 [==============================] - 0s 26us/step - loss: 0.0168 - val_loss: 0.0165\n",
      "Epoch 696/1200\n",
      "367/367 [==============================] - 0s 25us/step - loss: 0.0166 - val_loss: 0.0164\n",
      "Epoch 697/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0166 - val_loss: 0.0165\n",
      "Epoch 698/1200\n",
      "367/367 [==============================] - 0s 19us/step - loss: 0.0167 - val_loss: 0.0163\n",
      "Epoch 699/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0165 - val_loss: 0.0164\n",
      "Epoch 700/1200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "367/367 [==============================] - 0s 21us/step - loss: 0.0166 - val_loss: 0.0163\n",
      "Epoch 701/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0165 - val_loss: 0.0163\n",
      "Epoch 702/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0166 - val_loss: 0.0163\n",
      "Epoch 703/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0165 - val_loss: 0.0163\n",
      "Epoch 704/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0165 - val_loss: 0.0165\n",
      "Epoch 705/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0166 - val_loss: 0.0163\n",
      "Epoch 706/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0165 - val_loss: 0.0164\n",
      "Epoch 707/1200\n",
      "367/367 [==============================] - 0s 19us/step - loss: 0.0165 - val_loss: 0.0164\n",
      "Epoch 708/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0165 - val_loss: 0.0164\n",
      "Epoch 709/1200\n",
      "367/367 [==============================] - 0s 19us/step - loss: 0.0164 - val_loss: 0.0163\n",
      "Epoch 710/1200\n",
      "367/367 [==============================] - 0s 19us/step - loss: 0.0164 - val_loss: 0.0165\n",
      "Epoch 711/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0165 - val_loss: 0.0163\n",
      "Epoch 712/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0164 - val_loss: 0.0163\n",
      "Epoch 713/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0164 - val_loss: 0.0164\n",
      "Epoch 714/1200\n",
      "367/367 [==============================] - 0s 47us/step - loss: 0.0163 - val_loss: 0.0163\n",
      "Epoch 715/1200\n",
      "367/367 [==============================] - 0s 52us/step - loss: 0.0163 - val_loss: 0.0164\n",
      "Epoch 716/1200\n",
      "367/367 [==============================] - 0s 54us/step - loss: 0.0163 - val_loss: 0.0163\n",
      "Epoch 717/1200\n",
      "367/367 [==============================] - 0s 84us/step - loss: 0.0162 - val_loss: 0.0164\n",
      "Epoch 718/1200\n",
      "367/367 [==============================] - 0s 38us/step - loss: 0.0164 - val_loss: 0.0163\n",
      "Epoch 719/1200\n",
      "367/367 [==============================] - 0s 48us/step - loss: 0.0162 - val_loss: 0.0162\n",
      "Epoch 720/1200\n",
      "367/367 [==============================] - 0s 25us/step - loss: 0.0163 - val_loss: 0.0162\n",
      "Epoch 721/1200\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.016 - 0s 30us/step - loss: 0.0162 - val_loss: 0.0160\n",
      "Epoch 722/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0161 - val_loss: 0.0161\n",
      "Epoch 723/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0162 - val_loss: 0.0160\n",
      "Epoch 724/1200\n",
      "367/367 [==============================] - 0s 22us/step - loss: 0.0162 - val_loss: 0.0160\n",
      "Epoch 725/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0161 - val_loss: 0.0160\n",
      "Epoch 726/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0161 - val_loss: 0.0159\n",
      "Epoch 727/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0160 - val_loss: 0.0160\n",
      "Epoch 728/1200\n",
      "367/367 [==============================] - 0s 29us/step - loss: 0.0161 - val_loss: 0.0160\n",
      "Epoch 729/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0160 - val_loss: 0.0161\n",
      "Epoch 730/1200\n",
      "367/367 [==============================] - 0s 28us/step - loss: 0.0162 - val_loss: 0.0160\n",
      "Epoch 731/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0160 - val_loss: 0.0160\n",
      "Epoch 732/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0162 - val_loss: 0.0161\n",
      "Epoch 733/1200\n",
      "367/367 [==============================] - 0s 24us/step - loss: 0.0161 - val_loss: 0.0160\n",
      "Epoch 734/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0160 - val_loss: 0.0161\n",
      "Epoch 735/1200\n",
      "367/367 [==============================] - 0s 24us/step - loss: 0.0160 - val_loss: 0.0160\n",
      "Epoch 736/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0159 - val_loss: 0.0160\n",
      "Epoch 737/1200\n",
      "367/367 [==============================] - 0s 26us/step - loss: 0.0160 - val_loss: 0.0160\n",
      "Epoch 738/1200\n",
      "367/367 [==============================] - 0s 14us/step - loss: 0.0159 - val_loss: 0.0160\n",
      "Epoch 739/1200\n",
      "367/367 [==============================] - 0s 19us/step - loss: 0.0160 - val_loss: 0.0159\n",
      "Epoch 740/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0159 - val_loss: 0.0160\n",
      "Epoch 741/1200\n",
      "367/367 [==============================] - 0s 19us/step - loss: 0.0159 - val_loss: 0.0161\n",
      "Epoch 742/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0160 - val_loss: 0.0158\n",
      "Epoch 743/1200\n",
      "367/367 [==============================] - 0s 23us/step - loss: 0.0160 - val_loss: 0.0158\n",
      "Epoch 744/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0158 - val_loss: 0.0158\n",
      "Epoch 745/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0159 - val_loss: 0.0159\n",
      "Epoch 746/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0158 - val_loss: 0.0157\n",
      "Epoch 747/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0157 - val_loss: 0.0158\n",
      "Epoch 748/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0158 - val_loss: 0.0157\n",
      "Epoch 749/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0157 - val_loss: 0.0157\n",
      "Epoch 750/1200\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.015 - 0s 17us/step - loss: 0.0158 - val_loss: 0.0157\n",
      "Epoch 751/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0156 - val_loss: 0.0157\n",
      "Epoch 752/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0157 - val_loss: 0.0158\n",
      "Epoch 753/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0157 - val_loss: 0.0156\n",
      "Epoch 754/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0156 - val_loss: 0.0157\n",
      "Epoch 755/1200\n",
      "367/367 [==============================] - 0s 14us/step - loss: 0.0157 - val_loss: 0.0156\n",
      "Epoch 756/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0156 - val_loss: 0.0156\n",
      "Epoch 757/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0156 - val_loss: 0.0155\n",
      "Epoch 758/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0156 - val_loss: 0.0156\n",
      "Epoch 759/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0157 - val_loss: 0.0155\n",
      "Epoch 760/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0155 - val_loss: 0.0156\n",
      "Epoch 761/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0156 - val_loss: 0.0156\n",
      "Epoch 762/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0157 - val_loss: 0.0155\n",
      "Epoch 763/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0156 - val_loss: 0.0155\n",
      "Epoch 764/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0155 - val_loss: 0.0155\n",
      "Epoch 765/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0155 - val_loss: 0.0154\n",
      "Epoch 766/1200\n",
      "367/367 [==============================] - 0s 22us/step - loss: 0.0155 - val_loss: 0.0154\n",
      "Epoch 767/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0154 - val_loss: 0.0154\n",
      "Epoch 768/1200\n",
      "367/367 [==============================] - 0s 20us/step - loss: 0.0154 - val_loss: 0.0154\n",
      "Epoch 769/1200\n",
      "367/367 [==============================] - 0s 14us/step - loss: 0.0155 - val_loss: 0.0154\n",
      "Epoch 770/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0154 - val_loss: 0.0154\n",
      "Epoch 771/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0153 - val_loss: 0.0154\n",
      "Epoch 772/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0154 - val_loss: 0.0154\n",
      "Epoch 773/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0154 - val_loss: 0.0153\n",
      "Epoch 774/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0154 - val_loss: 0.0155\n",
      "Epoch 775/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0155 - val_loss: 0.0153\n",
      "Epoch 776/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0153 - val_loss: 0.0153\n",
      "Epoch 777/1200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "367/367 [==============================] - 0s 16us/step - loss: 0.0153 - val_loss: 0.0153\n",
      "Epoch 778/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0153 - val_loss: 0.0152\n",
      "Epoch 779/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0153 - val_loss: 0.0153\n",
      "Epoch 780/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0153 - val_loss: 0.0152\n",
      "Epoch 781/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0152 - val_loss: 0.0152\n",
      "Epoch 782/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0152 - val_loss: 0.0153\n",
      "Epoch 783/1200\n",
      "367/367 [==============================] - 0s 21us/step - loss: 0.0153 - val_loss: 0.0152\n",
      "Epoch 784/1200\n",
      "367/367 [==============================] - 0s 19us/step - loss: 0.0152 - val_loss: 0.0153\n",
      "Epoch 785/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0151 - val_loss: 0.0153\n",
      "Epoch 786/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0152 - val_loss: 0.0153\n",
      "Epoch 787/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0152 - val_loss: 0.0153\n",
      "Epoch 788/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0151 - val_loss: 0.0152\n",
      "Epoch 789/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0151 - val_loss: 0.0151\n",
      "Epoch 790/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0152 - val_loss: 0.0152\n",
      "Epoch 791/1200\n",
      "367/367 [==============================] - 0s 14us/step - loss: 0.0151 - val_loss: 0.0153\n",
      "Epoch 792/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0152 - val_loss: 0.0152\n",
      "Epoch 793/1200\n",
      "367/367 [==============================] - 0s 14us/step - loss: 0.0151 - val_loss: 0.0152\n",
      "Epoch 794/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0152 - val_loss: 0.0152\n",
      "Epoch 795/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0152 - val_loss: 0.0153\n",
      "Epoch 796/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0152 - val_loss: 0.0152\n",
      "Epoch 797/1200\n",
      "367/367 [==============================] - 0s 21us/step - loss: 0.0152 - val_loss: 0.0151\n",
      "Epoch 798/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0151 - val_loss: 0.0152\n",
      "Epoch 799/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0152 - val_loss: 0.0152\n",
      "Epoch 800/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0151 - val_loss: 0.0150\n",
      "Epoch 801/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0151 - val_loss: 0.0151\n",
      "Epoch 802/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0151 - val_loss: 0.0150\n",
      "Epoch 803/1200\n",
      "367/367 [==============================] - 0s 21us/step - loss: 0.0150 - val_loss: 0.0150\n",
      "Epoch 804/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0149 - val_loss: 0.0150\n",
      "Epoch 805/1200\n",
      "367/367 [==============================] - 0s 20us/step - loss: 0.0149 - val_loss: 0.0149\n",
      "Epoch 806/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0148 - val_loss: 0.0149\n",
      "Epoch 807/1200\n",
      "367/367 [==============================] - 0s 22us/step - loss: 0.0148 - val_loss: 0.0150\n",
      "Epoch 808/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0149 - val_loss: 0.0149\n",
      "Epoch 809/1200\n",
      "367/367 [==============================] - 0s 23us/step - loss: 0.0148 - val_loss: 0.0149\n",
      "Epoch 810/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0149 - val_loss: 0.0150\n",
      "Epoch 811/1200\n",
      "367/367 [==============================] - 0s 29us/step - loss: 0.0149 - val_loss: 0.0149\n",
      "Epoch 812/1200\n",
      "367/367 [==============================] - 0s 25us/step - loss: 0.0149 - val_loss: 0.0150\n",
      "Epoch 813/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0148 - val_loss: 0.0149\n",
      "Epoch 814/1200\n",
      "367/367 [==============================] - 0s 25us/step - loss: 0.0148 - val_loss: 0.0151\n",
      "Epoch 815/1200\n",
      "367/367 [==============================] - 0s 19us/step - loss: 0.0148 - val_loss: 0.0149\n",
      "Epoch 816/1200\n",
      "367/367 [==============================] - 0s 26us/step - loss: 0.0147 - val_loss: 0.0150\n",
      "Epoch 817/1200\n",
      "367/367 [==============================] - 0s 28us/step - loss: 0.0148 - val_loss: 0.0148\n",
      "Epoch 818/1200\n",
      "367/367 [==============================] - 0s 21us/step - loss: 0.0147 - val_loss: 0.0151\n",
      "Epoch 819/1200\n",
      "367/367 [==============================] - 0s 30us/step - loss: 0.0149 - val_loss: 0.0149\n",
      "Epoch 820/1200\n",
      "367/367 [==============================] - 0s 23us/step - loss: 0.0148 - val_loss: 0.0148\n",
      "Epoch 821/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0147 - val_loss: 0.0149\n",
      "Epoch 822/1200\n",
      "367/367 [==============================] - 0s 26us/step - loss: 0.0147 - val_loss: 0.0148\n",
      "Epoch 823/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0146 - val_loss: 0.0147\n",
      "Epoch 824/1200\n",
      "367/367 [==============================] - 0s 22us/step - loss: 0.0146 - val_loss: 0.0147\n",
      "Epoch 825/1200\n",
      "367/367 [==============================] - 0s 12us/step - loss: 0.0146 - val_loss: 0.0147\n",
      "Epoch 826/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0146 - val_loss: 0.0147\n",
      "Epoch 827/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0146 - val_loss: 0.0147\n",
      "Epoch 828/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0146 - val_loss: 0.0147\n",
      "Epoch 829/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0145 - val_loss: 0.0147\n",
      "Epoch 830/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0146 - val_loss: 0.0147\n",
      "Epoch 831/1200\n",
      "367/367 [==============================] - 0s 14us/step - loss: 0.0146 - val_loss: 0.0147\n",
      "Epoch 832/1200\n",
      "367/367 [==============================] - 0s 13us/step - loss: 0.0146 - val_loss: 0.0146\n",
      "Epoch 833/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0145 - val_loss: 0.0147\n",
      "Epoch 834/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0145 - val_loss: 0.0146\n",
      "Epoch 835/1200\n",
      "367/367 [==============================] - 0s 20us/step - loss: 0.0145 - val_loss: 0.0146\n",
      "Epoch 836/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0144 - val_loss: 0.0146\n",
      "Epoch 837/1200\n",
      "367/367 [==============================] - 0s 20us/step - loss: 0.0145 - val_loss: 0.0146\n",
      "Epoch 838/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0145 - val_loss: 0.0146\n",
      "Epoch 839/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0144 - val_loss: 0.0146\n",
      "Epoch 840/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0144 - val_loss: 0.0147\n",
      "Epoch 841/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0145 - val_loss: 0.0146\n",
      "Epoch 842/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0144 - val_loss: 0.0146\n",
      "Epoch 843/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0145 - val_loss: 0.0146\n",
      "Epoch 844/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0144 - val_loss: 0.0146\n",
      "Epoch 845/1200\n",
      "367/367 [==============================] - 0s 14us/step - loss: 0.0144 - val_loss: 0.0145\n",
      "Epoch 846/1200\n",
      "367/367 [==============================] - 0s 20us/step - loss: 0.0144 - val_loss: 0.0145\n",
      "Epoch 847/1200\n",
      "367/367 [==============================] - 0s 14us/step - loss: 0.0143 - val_loss: 0.0145\n",
      "Epoch 848/1200\n",
      "367/367 [==============================] - 0s 21us/step - loss: 0.0143 - val_loss: 0.0145\n",
      "Epoch 849/1200\n",
      "367/367 [==============================] - 0s 14us/step - loss: 0.0143 - val_loss: 0.0145\n",
      "Epoch 850/1200\n",
      "367/367 [==============================] - 0s 23us/step - loss: 0.0143 - val_loss: 0.0144\n",
      "Epoch 851/1200\n",
      "367/367 [==============================] - 0s 20us/step - loss: 0.0143 - val_loss: 0.0144\n",
      "Epoch 852/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0142 - val_loss: 0.0144\n",
      "Epoch 853/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0143 - val_loss: 0.0144\n",
      "Epoch 854/1200\n",
      "367/367 [==============================] - 0s 41us/step - loss: 0.0142 - val_loss: 0.0144\n",
      "Epoch 855/1200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "367/367 [==============================] - 0s 23us/step - loss: 0.0143 - val_loss: 0.0144\n",
      "Epoch 856/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0143 - val_loss: 0.0144\n",
      "Epoch 857/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0142 - val_loss: 0.0143\n",
      "Epoch 858/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0142 - val_loss: 0.0144\n",
      "Epoch 859/1200\n",
      "367/367 [==============================] - 0s 19us/step - loss: 0.0142 - val_loss: 0.0143\n",
      "Epoch 860/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0142 - val_loss: 0.0143\n",
      "Epoch 861/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0142 - val_loss: 0.0143\n",
      "Epoch 862/1200\n",
      "367/367 [==============================] - 0s 14us/step - loss: 0.0141 - val_loss: 0.0143\n",
      "Epoch 863/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0142 - val_loss: 0.0142\n",
      "Epoch 864/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0141 - val_loss: 0.0142\n",
      "Epoch 865/1200\n",
      "367/367 [==============================] - 0s 14us/step - loss: 0.0141 - val_loss: 0.0142\n",
      "Epoch 866/1200\n",
      "367/367 [==============================] - 0s 21us/step - loss: 0.0142 - val_loss: 0.0143\n",
      "Epoch 867/1200\n",
      "367/367 [==============================] - 0s 20us/step - loss: 0.0142 - val_loss: 0.0146\n",
      "Epoch 868/1200\n",
      "367/367 [==============================] - 0s 22us/step - loss: 0.0144 - val_loss: 0.0142\n",
      "Epoch 869/1200\n",
      "367/367 [==============================] - 0s 30us/step - loss: 0.0141 - val_loss: 0.0145\n",
      "Epoch 870/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0144 - val_loss: 0.0142\n",
      "Epoch 871/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0140 - val_loss: 0.0143\n",
      "Epoch 872/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0142 - val_loss: 0.0142\n",
      "Epoch 873/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0143 - val_loss: 0.0142\n",
      "Epoch 874/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0141 - val_loss: 0.0141\n",
      "Epoch 875/1200\n",
      "367/367 [==============================] - 0s 20us/step - loss: 0.0140 - val_loss: 0.0142\n",
      "Epoch 876/1200\n",
      "367/367 [==============================] - 0s 14us/step - loss: 0.0142 - val_loss: 0.0142\n",
      "Epoch 877/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0140 - val_loss: 0.0142\n",
      "Epoch 878/1200\n",
      "367/367 [==============================] - 0s 14us/step - loss: 0.0140 - val_loss: 0.0143\n",
      "Epoch 879/1200\n",
      "367/367 [==============================] - 0s 14us/step - loss: 0.0141 - val_loss: 0.0142\n",
      "Epoch 880/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0140 - val_loss: 0.0146\n",
      "Epoch 881/1200\n",
      "367/367 [==============================] - 0s 14us/step - loss: 0.0144 - val_loss: 0.0142\n",
      "Epoch 882/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0140 - val_loss: 0.0145\n",
      "Epoch 883/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0143 - val_loss: 0.0142\n",
      "Epoch 884/1200\n",
      "367/367 [==============================] - 0s 14us/step - loss: 0.0140 - val_loss: 0.0143\n",
      "Epoch 885/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0140 - val_loss: 0.0143\n",
      "Epoch 886/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0141 - val_loss: 0.0143\n",
      "Epoch 887/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0139 - val_loss: 0.0141\n",
      "Epoch 888/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0140 - val_loss: 0.0143\n",
      "Epoch 889/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0141 - val_loss: 0.0140\n",
      "Epoch 890/1200\n",
      "367/367 [==============================] - 0s 14us/step - loss: 0.0138 - val_loss: 0.0142\n",
      "Epoch 891/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0140 - val_loss: 0.0142\n",
      "Epoch 892/1200\n",
      "367/367 [==============================] - 0s 14us/step - loss: 0.0139 - val_loss: 0.0140\n",
      "Epoch 893/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0140 - val_loss: 0.0141\n",
      "Epoch 894/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0139 - val_loss: 0.0142\n",
      "Epoch 895/1200\n",
      "367/367 [==============================] - 0s 13us/step - loss: 0.0142 - val_loss: 0.0144\n",
      "Epoch 896/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0139 - val_loss: 0.0144\n",
      "Epoch 897/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0142 - val_loss: 0.0148\n",
      "Epoch 898/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0144 - val_loss: 0.0141\n",
      "Epoch 899/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0139 - val_loss: 0.0146\n",
      "Epoch 900/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0141 - val_loss: 0.0143\n",
      "Epoch 901/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0140 - val_loss: 0.0147\n",
      "Epoch 902/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0142 - val_loss: 0.0144\n",
      "Epoch 903/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0140 - val_loss: 0.0141\n",
      "Epoch 904/1200\n",
      "367/367 [==============================] - 0s 14us/step - loss: 0.0139 - val_loss: 0.0147\n",
      "Epoch 905/1200\n",
      "367/367 [==============================] - 0s 14us/step - loss: 0.0142 - val_loss: 0.0142\n",
      "Epoch 906/1200\n",
      "367/367 [==============================] - 0s 20us/step - loss: 0.0140 - val_loss: 0.0141\n",
      "Epoch 907/1200\n",
      "367/367 [==============================] - 0s 20us/step - loss: 0.0140 - val_loss: 0.0140\n",
      "Epoch 908/1200\n",
      "367/367 [==============================] - 0s 25us/step - loss: 0.0137 - val_loss: 0.0142\n",
      "Epoch 909/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0139 - val_loss: 0.0141\n",
      "Epoch 910/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0138 - val_loss: 0.0138\n",
      "Epoch 911/1200\n",
      "367/367 [==============================] - 0s 23us/step - loss: 0.0137 - val_loss: 0.0139\n",
      "Epoch 912/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0137 - val_loss: 0.0138\n",
      "Epoch 913/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0136 - val_loss: 0.0142\n",
      "Epoch 914/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0139 - val_loss: 0.0138\n",
      "Epoch 915/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0135 - val_loss: 0.0138\n",
      "Epoch 916/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0137 - val_loss: 0.0139\n",
      "Epoch 917/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0137 - val_loss: 0.0138\n",
      "Epoch 918/1200\n",
      "367/367 [==============================] - 0s 14us/step - loss: 0.0135 - val_loss: 0.0142\n",
      "Epoch 919/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0138 - val_loss: 0.0140\n",
      "Epoch 920/1200\n",
      "367/367 [==============================] - 0s 20us/step - loss: 0.0136 - val_loss: 0.0138\n",
      "Epoch 921/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0135 - val_loss: 0.0141\n",
      "Epoch 922/1200\n",
      "367/367 [==============================] - 0s 19us/step - loss: 0.0138 - val_loss: 0.0140\n",
      "Epoch 923/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0136 - val_loss: 0.0139\n",
      "Epoch 924/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0136 - val_loss: 0.0140\n",
      "Epoch 925/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0137 - val_loss: 0.0137\n",
      "Epoch 926/1200\n",
      "367/367 [==============================] - 0s 19us/step - loss: 0.0135 - val_loss: 0.0137\n",
      "Epoch 927/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0135 - val_loss: 0.0137\n",
      "Epoch 928/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0134 - val_loss: 0.0137\n",
      "Epoch 929/1200\n",
      "367/367 [==============================] - 0s 14us/step - loss: 0.0134 - val_loss: 0.0137\n",
      "Epoch 930/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0134 - val_loss: 0.0137\n",
      "Epoch 931/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0134 - val_loss: 0.0136\n",
      "Epoch 932/1200\n",
      "367/367 [==============================] - 0s 19us/step - loss: 0.0134 - val_loss: 0.0136\n",
      "Epoch 933/1200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "367/367 [==============================] - 0s 16us/step - loss: 0.0134 - val_loss: 0.0136\n",
      "Epoch 934/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0135 - val_loss: 0.0137\n",
      "Epoch 935/1200\n",
      "367/367 [==============================] - 0s 20us/step - loss: 0.0135 - val_loss: 0.0136\n",
      "Epoch 936/1200\n",
      "367/367 [==============================] - 0s 19us/step - loss: 0.0134 - val_loss: 0.0136\n",
      "Epoch 937/1200\n",
      "367/367 [==============================] - 0s 20us/step - loss: 0.0133 - val_loss: 0.0137\n",
      "Epoch 938/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0134 - val_loss: 0.0137\n",
      "Epoch 939/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0133 - val_loss: 0.0137\n",
      "Epoch 940/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0134 - val_loss: 0.0136\n",
      "Epoch 941/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0133 - val_loss: 0.0137\n",
      "Epoch 942/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0134 - val_loss: 0.0137\n",
      "Epoch 943/1200\n",
      "367/367 [==============================] - 0s 19us/step - loss: 0.0134 - val_loss: 0.0137\n",
      "Epoch 944/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0133 - val_loss: 0.0137\n",
      "Epoch 945/1200\n",
      "367/367 [==============================] - 0s 14us/step - loss: 0.0135 - val_loss: 0.0136\n",
      "Epoch 946/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0133 - val_loss: 0.0137\n",
      "Epoch 947/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0134 - val_loss: 0.0136\n",
      "Epoch 948/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0132 - val_loss: 0.0137\n",
      "Epoch 949/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0135 - val_loss: 0.0137\n",
      "Epoch 950/1200\n",
      "367/367 [==============================] - 0s 14us/step - loss: 0.0134 - val_loss: 0.0136\n",
      "Epoch 951/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0133 - val_loss: 0.0135\n",
      "Epoch 952/1200\n",
      "367/367 [==============================] - 0s 14us/step - loss: 0.0133 - val_loss: 0.0135\n",
      "Epoch 953/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0132 - val_loss: 0.0135\n",
      "Epoch 954/1200\n",
      "367/367 [==============================] - 0s 14us/step - loss: 0.0132 - val_loss: 0.0135\n",
      "Epoch 955/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0132 - val_loss: 0.0135\n",
      "Epoch 956/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0132 - val_loss: 0.0135\n",
      "Epoch 957/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0132 - val_loss: 0.0135\n",
      "Epoch 958/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0132 - val_loss: 0.0135\n",
      "Epoch 959/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0132 - val_loss: 0.0135\n",
      "Epoch 960/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0132 - val_loss: 0.0135\n",
      "Epoch 961/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0131 - val_loss: 0.0135\n",
      "Epoch 962/1200\n",
      "367/367 [==============================] - 0s 22us/step - loss: 0.0131 - val_loss: 0.0136\n",
      "Epoch 963/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0131 - val_loss: 0.0135\n",
      "Epoch 964/1200\n",
      "367/367 [==============================] - 0s 21us/step - loss: 0.0131 - val_loss: 0.0136\n",
      "Epoch 965/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0133 - val_loss: 0.0135\n",
      "Epoch 966/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0131 - val_loss: 0.0134\n",
      "Epoch 967/1200\n",
      "367/367 [==============================] - 0s 14us/step - loss: 0.0132 - val_loss: 0.0135\n",
      "Epoch 968/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0132 - val_loss: 0.0133\n",
      "Epoch 969/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0132 - val_loss: 0.0133\n",
      "Epoch 970/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0131 - val_loss: 0.0134\n",
      "Epoch 971/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0131 - val_loss: 0.0135\n",
      "Epoch 972/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0131 - val_loss: 0.0134\n",
      "Epoch 973/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0131 - val_loss: 0.0134\n",
      "Epoch 974/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0131 - val_loss: 0.0135\n",
      "Epoch 975/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0131 - val_loss: 0.0134\n",
      "Epoch 976/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0131 - val_loss: 0.0135\n",
      "Epoch 977/1200\n",
      "367/367 [==============================] - 0s 14us/step - loss: 0.0130 - val_loss: 0.0135\n",
      "Epoch 978/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0133 - val_loss: 0.0134\n",
      "Epoch 979/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0133 - val_loss: 0.0134\n",
      "Epoch 980/1200\n",
      "367/367 [==============================] - 0s 20us/step - loss: 0.0131 - val_loss: 0.0134\n",
      "Epoch 981/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0130 - val_loss: 0.0133\n",
      "Epoch 982/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0129 - val_loss: 0.0133\n",
      "Epoch 983/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0130 - val_loss: 0.0133\n",
      "Epoch 984/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0130 - val_loss: 0.0133\n",
      "Epoch 985/1200\n",
      "367/367 [==============================] - 0s 14us/step - loss: 0.0130 - val_loss: 0.0133\n",
      "Epoch 986/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0131 - val_loss: 0.0133\n",
      "Epoch 987/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0130 - val_loss: 0.0133\n",
      "Epoch 988/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0129 - val_loss: 0.0134\n",
      "Epoch 989/1200\n",
      "367/367 [==============================] - 0s 19us/step - loss: 0.0131 - val_loss: 0.0133\n",
      "Epoch 990/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0130 - val_loss: 0.0133\n",
      "Epoch 991/1200\n",
      "367/367 [==============================] - 0s 14us/step - loss: 0.0129 - val_loss: 0.0135\n",
      "Epoch 992/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0131 - val_loss: 0.0134\n",
      "Epoch 993/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0130 - val_loss: 0.0132\n",
      "Epoch 994/1200\n",
      "367/367 [==============================] - 0s 21us/step - loss: 0.0129 - val_loss: 0.0133\n",
      "Epoch 995/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0129 - val_loss: 0.0132\n",
      "Epoch 996/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0129 - val_loss: 0.0133\n",
      "Epoch 997/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0130 - val_loss: 0.0131\n",
      "Epoch 998/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0128 - val_loss: 0.0132\n",
      "Epoch 999/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0129 - val_loss: 0.0132\n",
      "Epoch 1000/1200\n",
      "367/367 [==============================] - 0s 14us/step - loss: 0.0128 - val_loss: 0.0132\n",
      "Epoch 1001/1200\n",
      "367/367 [==============================] - 0s 35us/step - loss: 0.0129 - val_loss: 0.0132\n",
      "Epoch 1002/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0129 - val_loss: 0.0131\n",
      "Epoch 1003/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0127 - val_loss: 0.0132\n",
      "Epoch 1004/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0129 - val_loss: 0.0132\n",
      "Epoch 1005/1200\n",
      "367/367 [==============================] - 0s 19us/step - loss: 0.0129 - val_loss: 0.0131\n",
      "Epoch 1006/1200\n",
      "367/367 [==============================] - 0s 14us/step - loss: 0.0128 - val_loss: 0.0131\n",
      "Epoch 1007/1200\n",
      "367/367 [==============================] - 0s 19us/step - loss: 0.0129 - val_loss: 0.0131\n",
      "Epoch 1008/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0128 - val_loss: 0.0132\n",
      "Epoch 1009/1200\n",
      "367/367 [==============================] - 0s 19us/step - loss: 0.0129 - val_loss: 0.0132\n",
      "Epoch 1010/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0128 - val_loss: 0.0132\n",
      "Epoch 1011/1200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "367/367 [==============================] - 0s 16us/step - loss: 0.0128 - val_loss: 0.0132\n",
      "Epoch 1012/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0127 - val_loss: 0.0131\n",
      "Epoch 1013/1200\n",
      "367/367 [==============================] - 0s 14us/step - loss: 0.0127 - val_loss: 0.0132\n",
      "Epoch 1014/1200\n",
      "367/367 [==============================] - 0s 19us/step - loss: 0.0129 - val_loss: 0.0130\n",
      "Epoch 1015/1200\n",
      "367/367 [==============================] - 0s 14us/step - loss: 0.0128 - val_loss: 0.0131\n",
      "Epoch 1016/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0127 - val_loss: 0.0130\n",
      "Epoch 1017/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0127 - val_loss: 0.0132\n",
      "Epoch 1018/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0128 - val_loss: 0.0130\n",
      "Epoch 1019/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0127 - val_loss: 0.0131\n",
      "Epoch 1020/1200\n",
      "367/367 [==============================] - 0s 14us/step - loss: 0.0128 - val_loss: 0.0131\n",
      "Epoch 1021/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0127 - val_loss: 0.0130\n",
      "Epoch 1022/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0127 - val_loss: 0.0131\n",
      "Epoch 1023/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0127 - val_loss: 0.0130\n",
      "Epoch 1024/1200\n",
      "367/367 [==============================] - 0s 20us/step - loss: 0.0128 - val_loss: 0.0132\n",
      "Epoch 1025/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0127 - val_loss: 0.0131\n",
      "Epoch 1026/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0128 - val_loss: 0.0133\n",
      "Epoch 1027/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0128 - val_loss: 0.0131\n",
      "Epoch 1028/1200\n",
      "367/367 [==============================] - 0s 14us/step - loss: 0.0129 - val_loss: 0.0133\n",
      "Epoch 1029/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0127 - val_loss: 0.0131\n",
      "Epoch 1030/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0129 - val_loss: 0.0134\n",
      "Epoch 1031/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0131 - val_loss: 0.0133\n",
      "Epoch 1032/1200\n",
      "367/367 [==============================] - 0s 22us/step - loss: 0.0129 - val_loss: 0.0131\n",
      "Epoch 1033/1200\n",
      "367/367 [==============================] - 0s 24us/step - loss: 0.0126 - val_loss: 0.0133\n",
      "Epoch 1034/1200\n",
      "367/367 [==============================] - 0s 22us/step - loss: 0.0128 - val_loss: 0.0131\n",
      "Epoch 1035/1200\n",
      "367/367 [==============================] - 0s 21us/step - loss: 0.0126 - val_loss: 0.0131\n",
      "Epoch 1036/1200\n",
      "367/367 [==============================] - 0s 22us/step - loss: 0.0127 - val_loss: 0.0130\n",
      "Epoch 1037/1200\n",
      "367/367 [==============================] - 0s 24us/step - loss: 0.0126 - val_loss: 0.0130\n",
      "Epoch 1038/1200\n",
      "367/367 [==============================] - 0s 24us/step - loss: 0.0126 - val_loss: 0.0130\n",
      "Epoch 1039/1200\n",
      "367/367 [==============================] - 0s 21us/step - loss: 0.0126 - val_loss: 0.0130\n",
      "Epoch 1040/1200\n",
      "367/367 [==============================] - 0s 21us/step - loss: 0.0125 - val_loss: 0.0129\n",
      "Epoch 1041/1200\n",
      "367/367 [==============================] - 0s 30us/step - loss: 0.0125 - val_loss: 0.0129\n",
      "Epoch 1042/1200\n",
      "367/367 [==============================] - 0s 36us/step - loss: 0.0125 - val_loss: 0.0129\n",
      "Epoch 1043/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0125 - val_loss: 0.0129\n",
      "Epoch 1044/1200\n",
      "367/367 [==============================] - 0s 20us/step - loss: 0.0126 - val_loss: 0.0130\n",
      "Epoch 1045/1200\n",
      "367/367 [==============================] - 0s 19us/step - loss: 0.0126 - val_loss: 0.0129\n",
      "Epoch 1046/1200\n",
      "367/367 [==============================] - 0s 20us/step - loss: 0.0124 - val_loss: 0.0131\n",
      "Epoch 1047/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0127 - val_loss: 0.0130\n",
      "Epoch 1048/1200\n",
      "367/367 [==============================] - 0s 24us/step - loss: 0.0126 - val_loss: 0.0128\n",
      "Epoch 1049/1200\n",
      "367/367 [==============================] - 0s 25us/step - loss: 0.0125 - val_loss: 0.0130\n",
      "Epoch 1050/1200\n",
      "367/367 [==============================] - 0s 26us/step - loss: 0.0126 - val_loss: 0.0128\n",
      "Epoch 1051/1200\n",
      "367/367 [==============================] - 0s 30us/step - loss: 0.0124 - val_loss: 0.0130\n",
      "Epoch 1052/1200\n",
      "367/367 [==============================] - 0s 26us/step - loss: 0.0126 - val_loss: 0.0129\n",
      "Epoch 1053/1200\n",
      "367/367 [==============================] - 0s 20us/step - loss: 0.0125 - val_loss: 0.0129\n",
      "Epoch 1054/1200\n",
      "367/367 [==============================] - 0s 24us/step - loss: 0.0125 - val_loss: 0.0131\n",
      "Epoch 1055/1200\n",
      "367/367 [==============================] - 0s 22us/step - loss: 0.0126 - val_loss: 0.0128\n",
      "Epoch 1056/1200\n",
      "367/367 [==============================] - 0s 33us/step - loss: 0.0125 - val_loss: 0.0132\n",
      "Epoch 1057/1200\n",
      "367/367 [==============================] - 0s 28us/step - loss: 0.0128 - val_loss: 0.0129\n",
      "Epoch 1058/1200\n",
      "367/367 [==============================] - 0s 20us/step - loss: 0.0126 - val_loss: 0.0130\n",
      "Epoch 1059/1200\n",
      "367/367 [==============================] - 0s 39us/step - loss: 0.0126 - val_loss: 0.0132\n",
      "Epoch 1060/1200\n",
      "367/367 [==============================] - 0s 21us/step - loss: 0.0128 - val_loss: 0.0130\n",
      "Epoch 1061/1200\n",
      "367/367 [==============================] - 0s 26us/step - loss: 0.0126 - val_loss: 0.0130\n",
      "Epoch 1062/1200\n",
      "367/367 [==============================] - 0s 19us/step - loss: 0.0125 - val_loss: 0.0130\n",
      "Epoch 1063/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0125 - val_loss: 0.0128\n",
      "Epoch 1064/1200\n",
      "367/367 [==============================] - 0s 20us/step - loss: 0.0125 - val_loss: 0.0129\n",
      "Epoch 1065/1200\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.012 - 0s 16us/step - loss: 0.0125 - val_loss: 0.0128\n",
      "Epoch 1066/1200\n",
      "367/367 [==============================] - 0s 20us/step - loss: 0.0123 - val_loss: 0.0129\n",
      "Epoch 1067/1200\n",
      "367/367 [==============================] - 0s 19us/step - loss: 0.0124 - val_loss: 0.0128\n",
      "Epoch 1068/1200\n",
      "367/367 [==============================] - 0s 20us/step - loss: 0.0123 - val_loss: 0.0128\n",
      "Epoch 1069/1200\n",
      "367/367 [==============================] - 0s 22us/step - loss: 0.0126 - val_loss: 0.0127\n",
      "Epoch 1070/1200\n",
      "367/367 [==============================] - 0s 19us/step - loss: 0.0124 - val_loss: 0.0130\n",
      "Epoch 1071/1200\n",
      "367/367 [==============================] - 0s 21us/step - loss: 0.0126 - val_loss: 0.0128\n",
      "Epoch 1072/1200\n",
      "367/367 [==============================] - 0s 19us/step - loss: 0.0124 - val_loss: 0.0127\n",
      "Epoch 1073/1200\n",
      "367/367 [==============================] - 0s 30us/step - loss: 0.0124 - val_loss: 0.0128\n",
      "Epoch 1074/1200\n",
      "367/367 [==============================] - 0s 21us/step - loss: 0.0125 - val_loss: 0.0126\n",
      "Epoch 1075/1200\n",
      "367/367 [==============================] - 0s 24us/step - loss: 0.0123 - val_loss: 0.0130\n",
      "Epoch 1076/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0126 - val_loss: 0.0128\n",
      "Epoch 1077/1200\n",
      "367/367 [==============================] - 0s 14us/step - loss: 0.0124 - val_loss: 0.0128\n",
      "Epoch 1078/1200\n",
      "367/367 [==============================] - 0s 14us/step - loss: 0.0126 - val_loss: 0.0128\n",
      "Epoch 1079/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0124 - val_loss: 0.0126\n",
      "Epoch 1080/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0123 - val_loss: 0.0131\n",
      "Epoch 1081/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0126 - val_loss: 0.0126\n",
      "Epoch 1082/1200\n",
      "367/367 [==============================] - 0s 25us/step - loss: 0.0121 - val_loss: 0.0131\n",
      "Epoch 1083/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0128 - val_loss: 0.0128\n",
      "Epoch 1084/1200\n",
      "367/367 [==============================] - 0s 22us/step - loss: 0.0123 - val_loss: 0.0131\n",
      "Epoch 1085/1200\n",
      "367/367 [==============================] - 0s 22us/step - loss: 0.0126 - val_loss: 0.0132\n",
      "Epoch 1086/1200\n",
      "367/367 [==============================] - 0s 27us/step - loss: 0.0125 - val_loss: 0.0126\n",
      "Epoch 1087/1200\n",
      "367/367 [==============================] - 0s 21us/step - loss: 0.0123 - val_loss: 0.0130\n",
      "Epoch 1088/1200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "367/367 [==============================] - 0s 21us/step - loss: 0.0125 - val_loss: 0.0126\n",
      "Epoch 1089/1200\n",
      "367/367 [==============================] - 0s 24us/step - loss: 0.0123 - val_loss: 0.0129\n",
      "Epoch 1090/1200\n",
      "367/367 [==============================] - 0s 21us/step - loss: 0.0124 - val_loss: 0.0126\n",
      "Epoch 1091/1200\n",
      "367/367 [==============================] - 0s 25us/step - loss: 0.0122 - val_loss: 0.0127\n",
      "Epoch 1092/1200\n",
      "367/367 [==============================] - 0s 26us/step - loss: 0.0123 - val_loss: 0.0127\n",
      "Epoch 1093/1200\n",
      "367/367 [==============================] - 0s 26us/step - loss: 0.0123 - val_loss: 0.0126\n",
      "Epoch 1094/1200\n",
      "367/367 [==============================] - 0s 19us/step - loss: 0.0122 - val_loss: 0.0127\n",
      "Epoch 1095/1200\n",
      "367/367 [==============================] - 0s 23us/step - loss: 0.0122 - val_loss: 0.0127\n",
      "Epoch 1096/1200\n",
      "367/367 [==============================] - 0s 22us/step - loss: 0.0122 - val_loss: 0.0126\n",
      "Epoch 1097/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0123 - val_loss: 0.0127\n",
      "Epoch 1098/1200\n",
      "367/367 [==============================] - 0s 21us/step - loss: 0.0123 - val_loss: 0.0126\n",
      "Epoch 1099/1200\n",
      "367/367 [==============================] - 0s 24us/step - loss: 0.0122 - val_loss: 0.0126\n",
      "Epoch 1100/1200\n",
      "367/367 [==============================] - 0s 29us/step - loss: 0.0122 - val_loss: 0.0126\n",
      "Epoch 1101/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0122 - val_loss: 0.0125\n",
      "Epoch 1102/1200\n",
      "367/367 [==============================] - 0s 19us/step - loss: 0.0121 - val_loss: 0.0125\n",
      "Epoch 1103/1200\n",
      "367/367 [==============================] - 0s 21us/step - loss: 0.0121 - val_loss: 0.0125\n",
      "Epoch 1104/1200\n",
      "367/367 [==============================] - 0s 32us/step - loss: 0.0121 - val_loss: 0.0125\n",
      "Epoch 1105/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0122 - val_loss: 0.0124\n",
      "Epoch 1106/1200\n",
      "367/367 [==============================] - 0s 23us/step - loss: 0.0122 - val_loss: 0.0125\n",
      "Epoch 1107/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0122 - val_loss: 0.0125\n",
      "Epoch 1108/1200\n",
      "367/367 [==============================] - 0s 21us/step - loss: 0.0122 - val_loss: 0.0125\n",
      "Epoch 1109/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0123 - val_loss: 0.0124\n",
      "Epoch 1110/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0121 - val_loss: 0.0125\n",
      "Epoch 1111/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0122 - val_loss: 0.0124\n",
      "Epoch 1112/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0121 - val_loss: 0.0126\n",
      "Epoch 1113/1200\n",
      "367/367 [==============================] - 0s 22us/step - loss: 0.0122 - val_loss: 0.0125\n",
      "Epoch 1114/1200\n",
      "367/367 [==============================] - 0s 31us/step - loss: 0.0121 - val_loss: 0.0125\n",
      "Epoch 1115/1200\n",
      "367/367 [==============================] - 0s 22us/step - loss: 0.0122 - val_loss: 0.0125\n",
      "Epoch 1116/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0122 - val_loss: 0.0124\n",
      "Epoch 1117/1200\n",
      "367/367 [==============================] - 0s 21us/step - loss: 0.0120 - val_loss: 0.0125\n",
      "Epoch 1118/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0121 - val_loss: 0.0124\n",
      "Epoch 1119/1200\n",
      "367/367 [==============================] - 0s 23us/step - loss: 0.0120 - val_loss: 0.0124\n",
      "Epoch 1120/1200\n",
      "367/367 [==============================] - 0s 29us/step - loss: 0.0121 - val_loss: 0.0124\n",
      "Epoch 1121/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0121 - val_loss: 0.0124\n",
      "Epoch 1122/1200\n",
      "367/367 [==============================] - 0s 14us/step - loss: 0.0121 - val_loss: 0.0124\n",
      "Epoch 1123/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0120 - val_loss: 0.0125\n",
      "Epoch 1124/1200\n",
      "367/367 [==============================] - 0s 14us/step - loss: 0.0121 - val_loss: 0.0124\n",
      "Epoch 1125/1200\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.012 - 0s 21us/step - loss: 0.0120 - val_loss: 0.0126\n",
      "Epoch 1126/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0122 - val_loss: 0.0125\n",
      "Epoch 1127/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0123 - val_loss: 0.0127\n",
      "Epoch 1128/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0121 - val_loss: 0.0125\n",
      "Epoch 1129/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0122 - val_loss: 0.0131\n",
      "Epoch 1130/1200\n",
      "367/367 [==============================] - 0s 19us/step - loss: 0.0125 - val_loss: 0.0124\n",
      "Epoch 1131/1200\n",
      "367/367 [==============================] - 0s 14us/step - loss: 0.0121 - val_loss: 0.0128\n",
      "Epoch 1132/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0123 - val_loss: 0.0125\n",
      "Epoch 1133/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0120 - val_loss: 0.0124\n",
      "Epoch 1134/1200\n",
      "367/367 [==============================] - 0s 14us/step - loss: 0.0120 - val_loss: 0.0125\n",
      "Epoch 1135/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0121 - val_loss: 0.0123\n",
      "Epoch 1136/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0120 - val_loss: 0.0126\n",
      "Epoch 1137/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0122 - val_loss: 0.0124\n",
      "Epoch 1138/1200\n",
      "367/367 [==============================] - 0s 13us/step - loss: 0.0121 - val_loss: 0.0123\n",
      "Epoch 1139/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0119 - val_loss: 0.0123\n",
      "Epoch 1140/1200\n",
      "367/367 [==============================] - 0s 14us/step - loss: 0.0119 - val_loss: 0.0124\n",
      "Epoch 1141/1200\n",
      "367/367 [==============================] - 0s 14us/step - loss: 0.0119 - val_loss: 0.0125\n",
      "Epoch 1142/1200\n",
      "367/367 [==============================] - 0s 19us/step - loss: 0.0120 - val_loss: 0.0124\n",
      "Epoch 1143/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0120 - val_loss: 0.0125\n",
      "Epoch 1144/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0121 - val_loss: 0.0124\n",
      "Epoch 1145/1200\n",
      "367/367 [==============================] - 0s 14us/step - loss: 0.0120 - val_loss: 0.0126\n",
      "Epoch 1146/1200\n",
      "367/367 [==============================] - 0s 14us/step - loss: 0.0122 - val_loss: 0.0123\n",
      "Epoch 1147/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0119 - val_loss: 0.0125\n",
      "Epoch 1148/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0120 - val_loss: 0.0123\n",
      "Epoch 1149/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0119 - val_loss: 0.0124\n",
      "Epoch 1150/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0121 - val_loss: 0.0124\n",
      "Epoch 1151/1200\n",
      "367/367 [==============================] - 0s 19us/step - loss: 0.0120 - val_loss: 0.0125\n",
      "Epoch 1152/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0121 - val_loss: 0.0125\n",
      "Epoch 1153/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0120 - val_loss: 0.0124\n",
      "Epoch 1154/1200\n",
      "367/367 [==============================] - 0s 23us/step - loss: 0.0120 - val_loss: 0.0124\n",
      "Epoch 1155/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0119 - val_loss: 0.0123\n",
      "Epoch 1156/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0119 - val_loss: 0.0123\n",
      "Epoch 1157/1200\n",
      "367/367 [==============================] - 0s 14us/step - loss: 0.0118 - val_loss: 0.0124\n",
      "Epoch 1158/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0120 - val_loss: 0.0123\n",
      "Epoch 1159/1200\n",
      "367/367 [==============================] - 0s 14us/step - loss: 0.0119 - val_loss: 0.0125\n",
      "Epoch 1160/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0120 - val_loss: 0.0123\n",
      "Epoch 1161/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0118 - val_loss: 0.0124\n",
      "Epoch 1162/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0120 - val_loss: 0.0124\n",
      "Epoch 1163/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0119 - val_loss: 0.0123\n",
      "Epoch 1164/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0121 - val_loss: 0.0123\n",
      "Epoch 1165/1200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "367/367 [==============================] - 0s 14us/step - loss: 0.0118 - val_loss: 0.0124\n",
      "Epoch 1166/1200\n",
      "367/367 [==============================] - 0s 19us/step - loss: 0.0120 - val_loss: 0.0125\n",
      "Epoch 1167/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0120 - val_loss: 0.0122\n",
      "Epoch 1168/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0118 - val_loss: 0.0125\n",
      "Epoch 1169/1200\n",
      "367/367 [==============================] - 0s 14us/step - loss: 0.0120 - val_loss: 0.0124\n",
      "Epoch 1170/1200\n",
      "367/367 [==============================] - 0s 14us/step - loss: 0.0118 - val_loss: 0.0124\n",
      "Epoch 1171/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0118 - val_loss: 0.0125\n",
      "Epoch 1172/1200\n",
      "367/367 [==============================] - 0s 14us/step - loss: 0.0120 - val_loss: 0.0124\n",
      "Epoch 1173/1200\n",
      "367/367 [==============================] - 0s 19us/step - loss: 0.0119 - val_loss: 0.0124\n",
      "Epoch 1174/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0120 - val_loss: 0.0123\n",
      "Epoch 1175/1200\n",
      "367/367 [==============================] - 0s 18us/step - loss: 0.0117 - val_loss: 0.0126\n",
      "Epoch 1176/1200\n",
      "367/367 [==============================] - 0s 14us/step - loss: 0.0121 - val_loss: 0.0124\n",
      "Epoch 1177/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0119 - val_loss: 0.0123\n",
      "Epoch 1178/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0118 - val_loss: 0.0123\n",
      "Epoch 1179/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0119 - val_loss: 0.0122\n",
      "Epoch 1180/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0119 - val_loss: 0.0122\n",
      "Epoch 1181/1200\n",
      "367/367 [==============================] - 0s 14us/step - loss: 0.0118 - val_loss: 0.0122\n",
      "Epoch 1182/1200\n",
      "367/367 [==============================] - 0s 14us/step - loss: 0.0118 - val_loss: 0.0122\n",
      "Epoch 1183/1200\n",
      "367/367 [==============================] - 0s 14us/step - loss: 0.0118 - val_loss: 0.0122\n",
      "Epoch 1184/1200\n",
      "367/367 [==============================] - 0s 14us/step - loss: 0.0118 - val_loss: 0.0121\n",
      "Epoch 1185/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0117 - val_loss: 0.0124\n",
      "Epoch 1186/1200\n",
      "367/367 [==============================] - 0s 14us/step - loss: 0.0118 - val_loss: 0.0123\n",
      "Epoch 1187/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0118 - val_loss: 0.0121\n",
      "Epoch 1188/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0117 - val_loss: 0.0123\n",
      "Epoch 1189/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0120 - val_loss: 0.0123\n",
      "Epoch 1190/1200\n",
      "367/367 [==============================] - 0s 14us/step - loss: 0.0118 - val_loss: 0.0122\n",
      "Epoch 1191/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0117 - val_loss: 0.0124\n",
      "Epoch 1192/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0118 - val_loss: 0.0122\n",
      "Epoch 1193/1200\n",
      "367/367 [==============================] - 0s 13us/step - loss: 0.0117 - val_loss: 0.0123\n",
      "Epoch 1194/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0118 - val_loss: 0.0122\n",
      "Epoch 1195/1200\n",
      "367/367 [==============================] - 0s 15us/step - loss: 0.0117 - val_loss: 0.0123\n",
      "Epoch 1196/1200\n",
      "367/367 [==============================] - 0s 16us/step - loss: 0.0117 - val_loss: 0.0124\n",
      "Epoch 1197/1200\n",
      "367/367 [==============================] - 0s 17us/step - loss: 0.0118 - val_loss: 0.0122\n",
      "Epoch 1198/1200\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.011 - 0s 15us/step - loss: 0.0117 - val_loss: 0.0123\n",
      "Epoch 1199/1200\n",
      "367/367 [==============================] - 0s 13us/step - loss: 0.0118 - val_loss: 0.0123\n",
      "Epoch 1200/1200\n",
      "367/367 [==============================] - 0s 14us/step - loss: 0.0119 - val_loss: 0.0123\n"
     ]
    }
   ],
   "source": [
    "# %load solutions/NN_1_layer_16_nodes.py\n",
    "\n",
    "H = 40 # number of nodes in the layer\n",
    "input_dim = 1 # input dimension: just x\n",
    "\n",
    "model2 = models.Sequential() # create sequential multi-layer perceptron\n",
    "\n",
    "# layer 0, our hidden layer\n",
    "model2.add(layers.Dense(H, input_dim=input_dim, \n",
    "                kernel_initializer='normal', \n",
    "                activation='relu')) \n",
    "# layer 1\n",
    "model2.add(layers.Dense(1, kernel_initializer='normal', \n",
    "                activation='linear')) \n",
    "\n",
    "# compile the model\n",
    "model2.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "# fit the model\n",
    "model2_history = model2.fit(X_train, Y_train, batch_size=256, epochs=1200, verbose=1, \\\n",
    "                          shuffle = True, validation_split=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is the loss smaller now? You may access the results in a model by its `.history`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.011873616179710188"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2_history.history['loss'][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again let's use the new model to predict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use our model to predict in the range we want\n",
    "X_range = np.linspace(-10, 10, 1000)\n",
    "y_pred = model2.predict(X_range)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=FIG_SIZE)\n",
    "ax.scatter(X_train, Y_train, label='Training data', alpha=0.3)\n",
    "ax.scatter(X_test, Y_test, label='Testing data' , alpha=0.3)\n",
    "ax.plot(X_range, y_pred, lw=4, color='r', label=f'NN with one hidden layer and {H} nodes')\n",
    "ax.set_xlabel(r'$X$', fontsize=FONT_SIZE)\n",
    "ax.set_ylabel(r'$Y$', fontsize=FONT_SIZE)\n",
    "ax.set_title(f'NN with {len(model2_history.model.layers)-1} layers, {H} nodes in each layer', fontsize=LABEL_SIZE)\n",
    "ax.tick_params(labelsize=LABEL_SIZE)\n",
    "\n",
    "ax.legend(loc=0, fontsize=FONT_SIZE)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"><b>Exercise 3:</b>\n",
    "</div>\n",
    "\n",
    "Plot the loss function as a function of the epochs. <b>Hint:</b> You can access the loss function values with the command:`model_history.history['loss']`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %load solutions/print_history.py\n",
    "fig, ax = plt.subplots(1, 1, figsize=FIG_SIZE)\n",
    "ax.plot(np.sqrt(model2_history.history['loss']), 'r')\n",
    "ax.plot(np.sqrt(model2_history.history['val_loss']), 'b' ,label='Val')\n",
    "ax.set_xlabel(r'Epoch', fontsize=FONT_SIZE)\n",
    "ax.set_ylabel(r'Loss', fontsize=FONT_SIZE)\n",
    "ax.legend()\n",
    "ax.tick_params(labelsize=LABEL_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How good is the model?  We can compute the $R^{2}$ score to get a sense of the model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "735/735 [==============================] - 0s 135us/step\n",
      "Train loss: 0.019467148611790875\n",
      "Train R2: 0.6963152912327268\n",
      "315/315 [==============================] - 0s 16us/step\n",
      "Test loss: 0.019549841132192384\n",
      "Test R2: 0.7128816191964373\n"
     ]
    }
   ],
   "source": [
    "# evaluate the training and testing performance of your model \n",
    "# note: you should extract and check both the loss function and your evaluation metric\n",
    "from sklearn.metrics import r2_score as r2\n",
    "\n",
    "train_score = model.evaluate(X_train, Y_train, verbose=1)\n",
    "print('Train loss:', train_score)\n",
    "print('Train R2:', r2(Y_train, model.predict(X_train)))\n",
    "\n",
    "test_score = model.evaluate(X_test, Y_test, verbose=1)\n",
    "print('Test loss:', test_score)\n",
    "print('Test R2:', r2(Y_test, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"><b>Exercise 4</b> </div>\n",
    "\n",
    "Let's add more layers. Fix the width $H$ and fit a MLP network with <b>multiple</b> hidden layers, each with the same width. Start with logistic or hyperbolic-tan activation functions for the hidden nodes and linear activation for the output. Experiment with the number of layers and observe the effect of this on the quality of the fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load solutions/NN_10_layers_100_nodes.py\n",
    "\n",
    "# number of hidden nodes\n",
    "H =  100\n",
    "# input dimension\n",
    "input_dim = 1\n",
    "\n",
    "# create sequential multi-layer perceptron\n",
    "model3 = models.Sequential()\n",
    "# layer 0\n",
    "model3.add(layers.Dense(H, input_dim=input_dim,  \n",
    "                activation='tanh')) \n",
    "# layer 1\n",
    "model3.add(layers.Dense(H,\n",
    "                activation='tanh')) \n",
    "# layer 2\n",
    "model3.add(layers.Dense(H,\n",
    "                activation='tanh')) \n",
    "# layer 3\n",
    "model3.add(layers.Dense(H,  \n",
    "                activation='tanh')) \n",
    "# layer 4\n",
    "model3.add(layers.Dense(H,  \n",
    "                activation='tanh')) \n",
    "# layer 5\n",
    "model3.add(layers.Dense(H,  \n",
    "                activation='tanh')) \n",
    "# layer 6\n",
    "model3.add(layers.Dense(H,  \n",
    "                activation='tanh'))\n",
    "# layer 7\n",
    "model3.add(layers.Dense(H,  \n",
    "                activation='tanh'))\n",
    "# layer 8\n",
    "model3.add(layers.Dense(H,  \n",
    "                activation='tanh'))\n",
    "# layer 9\n",
    "model3.add(layers.Dense(H,  \n",
    "                activation='tanh'))\n",
    "# layer 10 - output\n",
    "model3.add(layers.Dense(1, \n",
    "                activation='linear')) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure the model\n",
    "model3.compile(loss='mean_squared_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 514 samples, validate on 221 samples\n",
      "Epoch 1/1500\n",
      "514/514 [==============================] - 1s 2ms/step - loss: 0.5448 - val_loss: 0.1384\n",
      "Epoch 2/1500\n",
      "514/514 [==============================] - 0s 52us/step - loss: 0.1764 - val_loss: 0.1026\n",
      "Epoch 3/1500\n",
      "514/514 [==============================] - 0s 41us/step - loss: 0.1401 - val_loss: 0.0621\n",
      "Epoch 4/1500\n",
      "514/514 [==============================] - 0s 50us/step - loss: 0.0848 - val_loss: 0.0982\n",
      "Epoch 5/1500\n",
      "514/514 [==============================] - 0s 43us/step - loss: 0.0829 - val_loss: 0.1091\n",
      "Epoch 6/1500\n",
      "514/514 [==============================] - 0s 48us/step - loss: 0.1102 - val_loss: 0.0611\n",
      "Epoch 7/1500\n",
      "514/514 [==============================] - 0s 51us/step - loss: 0.0693 - val_loss: 0.0898\n",
      "Epoch 8/1500\n",
      "514/514 [==============================] - 0s 39us/step - loss: 0.0891 - val_loss: 0.0625\n",
      "Epoch 9/1500\n",
      "514/514 [==============================] - 0s 55us/step - loss: 0.0765 - val_loss: 0.1151\n",
      "Epoch 10/1500\n",
      "514/514 [==============================] - 0s 43us/step - loss: 0.0999 - val_loss: 0.0553\n",
      "Epoch 11/1500\n",
      "514/514 [==============================] - 0s 55us/step - loss: 0.0605 - val_loss: 0.0750\n",
      "Epoch 12/1500\n",
      "514/514 [==============================] - 0s 46us/step - loss: 0.0798 - val_loss: 0.0568\n",
      "Epoch 13/1500\n",
      "514/514 [==============================] - 0s 67us/step - loss: 0.0592 - val_loss: 0.0675\n",
      "Epoch 14/1500\n",
      "514/514 [==============================] - 0s 51us/step - loss: 0.0701 - val_loss: 0.0612\n",
      "Epoch 15/1500\n",
      "514/514 [==============================] - 0s 54us/step - loss: 0.0606 - val_loss: 0.0564\n",
      "Epoch 16/1500\n",
      "514/514 [==============================] - 0s 40us/step - loss: 0.0626 - val_loss: 0.0517\n",
      "Epoch 17/1500\n",
      "514/514 [==============================] - 0s 58us/step - loss: 0.0525 - val_loss: 0.0519\n",
      "Epoch 18/1500\n",
      "514/514 [==============================] - 0s 45us/step - loss: 0.0534 - val_loss: 0.0609\n",
      "Epoch 19/1500\n",
      "514/514 [==============================] - ETA: 0s - loss: 0.059 - 0s 56us/step - loss: 0.0626 - val_loss: 0.0436\n",
      "Epoch 20/1500\n",
      "514/514 [==============================] - 0s 48us/step - loss: 0.0488 - val_loss: 0.0670\n",
      "Epoch 21/1500\n",
      "514/514 [==============================] - 0s 50us/step - loss: 0.0691 - val_loss: 0.0350\n",
      "Epoch 22/1500\n",
      "514/514 [==============================] - 0s 58us/step - loss: 0.0448 - val_loss: 0.0600\n",
      "Epoch 23/1500\n",
      "514/514 [==============================] - 0s 42us/step - loss: 0.0558 - val_loss: 0.0383\n",
      "Epoch 24/1500\n",
      "514/514 [==============================] - 0s 43us/step - loss: 0.0479 - val_loss: 0.0498\n",
      "Epoch 25/1500\n",
      "514/514 [==============================] - 0s 53us/step - loss: 0.0531 - val_loss: 0.0189\n",
      "Epoch 26/1500\n",
      "514/514 [==============================] - 0s 48us/step - loss: 0.0304 - val_loss: 0.0530\n",
      "Epoch 27/1500\n",
      "514/514 [==============================] - 0s 47us/step - loss: 0.0464 - val_loss: 0.0339\n",
      "Epoch 28/1500\n",
      "514/514 [==============================] - 0s 57us/step - loss: 0.0375 - val_loss: 0.0196\n",
      "Epoch 29/1500\n",
      "514/514 [==============================] - 0s 51us/step - loss: 0.0269 - val_loss: 0.0163\n",
      "Epoch 30/1500\n",
      "514/514 [==============================] - 0s 47us/step - loss: 0.0181 - val_loss: 0.0165\n",
      "Epoch 31/1500\n",
      "514/514 [==============================] - 0s 47us/step - loss: 0.0167 - val_loss: 0.0178\n",
      "Epoch 32/1500\n",
      "514/514 [==============================] - 0s 46us/step - loss: 0.0204 - val_loss: 0.0106\n",
      "Epoch 33/1500\n",
      "514/514 [==============================] - 0s 47us/step - loss: 0.0146 - val_loss: 0.0266\n",
      "Epoch 34/1500\n",
      "514/514 [==============================] - 0s 55us/step - loss: 0.0282 - val_loss: 0.0139\n",
      "Epoch 35/1500\n",
      "514/514 [==============================] - 0s 45us/step - loss: 0.0177 - val_loss: 0.0161\n",
      "Epoch 36/1500\n",
      "514/514 [==============================] - 0s 52us/step - loss: 0.0174 - val_loss: 0.0118\n",
      "Epoch 37/1500\n",
      "514/514 [==============================] - 0s 51us/step - loss: 0.0178 - val_loss: 0.0319\n",
      "Epoch 38/1500\n",
      "514/514 [==============================] - 0s 47us/step - loss: 0.0331 - val_loss: 0.0127\n",
      "Epoch 39/1500\n",
      "514/514 [==============================] - 0s 53us/step - loss: 0.0174 - val_loss: 0.0175\n",
      "Epoch 40/1500\n",
      "514/514 [==============================] - 0s 58us/step - loss: 0.0224 - val_loss: 0.0093\n",
      "Epoch 41/1500\n",
      "514/514 [==============================] - 0s 59us/step - loss: 0.0135 - val_loss: 0.0352\n",
      "Epoch 42/1500\n",
      "514/514 [==============================] - 0s 49us/step - loss: 0.0373 - val_loss: 0.0380\n",
      "Epoch 43/1500\n",
      "514/514 [==============================] - 0s 49us/step - loss: 0.0324 - val_loss: 0.0186\n",
      "Epoch 44/1500\n",
      "514/514 [==============================] - 0s 55us/step - loss: 0.0171 - val_loss: 0.0177\n",
      "Epoch 45/1500\n",
      "514/514 [==============================] - 0s 46us/step - loss: 0.0196 - val_loss: 0.0119\n",
      "Epoch 46/1500\n",
      "514/514 [==============================] - 0s 54us/step - loss: 0.0203 - val_loss: 0.0186\n",
      "Epoch 47/1500\n",
      "514/514 [==============================] - 0s 49us/step - loss: 0.0203 - val_loss: 0.0252\n",
      "Epoch 48/1500\n",
      "514/514 [==============================] - 0s 50us/step - loss: 0.0286 - val_loss: 0.0281\n",
      "Epoch 49/1500\n",
      "514/514 [==============================] - 0s 47us/step - loss: 0.0303 - val_loss: 0.0149\n",
      "Epoch 50/1500\n",
      "514/514 [==============================] - 0s 39us/step - loss: 0.0205 - val_loss: 0.0266\n",
      "Epoch 51/1500\n",
      "514/514 [==============================] - 0s 33us/step - loss: 0.0304 - val_loss: 0.0188\n",
      "Epoch 52/1500\n",
      "514/514 [==============================] - 0s 35us/step - loss: 0.0216 - val_loss: 0.0107\n",
      "Epoch 53/1500\n",
      "514/514 [==============================] - 0s 43us/step - loss: 0.0136 - val_loss: 0.0152\n",
      "Epoch 54/1500\n",
      "514/514 [==============================] - 0s 35us/step - loss: 0.0169 - val_loss: 0.0167\n",
      "Epoch 55/1500\n",
      "514/514 [==============================] - 0s 32us/step - loss: 0.0175 - val_loss: 0.0169\n",
      "Epoch 56/1500\n",
      "514/514 [==============================] - 0s 32us/step - loss: 0.0171 - val_loss: 0.0142\n",
      "Epoch 57/1500\n",
      "514/514 [==============================] - 0s 34us/step - loss: 0.0172 - val_loss: 0.0125\n",
      "Epoch 58/1500\n",
      "514/514 [==============================] - 0s 31us/step - loss: 0.0139 - val_loss: 0.0095\n",
      "Epoch 59/1500\n",
      "514/514 [==============================] - 0s 41us/step - loss: 0.0128 - val_loss: 0.0105\n",
      "Epoch 60/1500\n",
      "514/514 [==============================] - 0s 39us/step - loss: 0.0137 - val_loss: 0.0126\n",
      "Epoch 61/1500\n",
      "514/514 [==============================] - 0s 43us/step - loss: 0.0181 - val_loss: 0.0103\n",
      "Epoch 62/1500\n",
      "514/514 [==============================] - 0s 48us/step - loss: 0.0170 - val_loss: 0.0114\n",
      "Epoch 63/1500\n",
      "514/514 [==============================] - 0s 50us/step - loss: 0.0150 - val_loss: 0.0161\n",
      "Epoch 64/1500\n",
      "514/514 [==============================] - 0s 63us/step - loss: 0.0179 - val_loss: 0.0114\n",
      "Epoch 65/1500\n",
      "514/514 [==============================] - 0s 53us/step - loss: 0.0175 - val_loss: 0.0082\n",
      "Epoch 66/1500\n",
      "514/514 [==============================] - 0s 43us/step - loss: 0.0145 - val_loss: 0.0158\n",
      "Epoch 67/1500\n",
      "514/514 [==============================] - 0s 60us/step - loss: 0.0162 - val_loss: 0.0131\n",
      "Epoch 68/1500\n",
      "514/514 [==============================] - 0s 48us/step - loss: 0.0177 - val_loss: 0.0091\n",
      "Epoch 69/1500\n",
      "514/514 [==============================] - 0s 47us/step - loss: 0.0146 - val_loss: 0.0200\n",
      "Epoch 70/1500\n",
      "514/514 [==============================] - 0s 46us/step - loss: 0.0186 - val_loss: 0.0102\n",
      "Epoch 71/1500\n",
      "514/514 [==============================] - 0s 52us/step - loss: 0.0131 - val_loss: 0.0094\n",
      "Epoch 72/1500\n",
      "514/514 [==============================] - 0s 45us/step - loss: 0.0116 - val_loss: 0.0086\n",
      "Epoch 73/1500\n",
      "514/514 [==============================] - 0s 44us/step - loss: 0.0117 - val_loss: 0.0084\n",
      "Epoch 74/1500\n",
      "514/514 [==============================] - 0s 48us/step - loss: 0.0130 - val_loss: 0.0192\n",
      "Epoch 75/1500\n",
      "514/514 [==============================] - 0s 44us/step - loss: 0.0207 - val_loss: 0.0113\n",
      "Epoch 76/1500\n",
      "514/514 [==============================] - 0s 52us/step - loss: 0.0150 - val_loss: 0.0215\n",
      "Epoch 77/1500\n",
      "514/514 [==============================] - 0s 48us/step - loss: 0.0204 - val_loss: 0.0175\n",
      "Epoch 78/1500\n",
      "514/514 [==============================] - 0s 56us/step - loss: 0.0167 - val_loss: 0.0090\n",
      "Epoch 79/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "514/514 [==============================] - 0s 53us/step - loss: 0.0121 - val_loss: 0.0124\n",
      "Epoch 80/1500\n",
      "514/514 [==============================] - 0s 56us/step - loss: 0.0181 - val_loss: 0.0214\n",
      "Epoch 81/1500\n",
      "514/514 [==============================] - 0s 47us/step - loss: 0.0208 - val_loss: 0.0136\n",
      "Epoch 82/1500\n",
      "514/514 [==============================] - 0s 56us/step - loss: 0.0210 - val_loss: 0.0278\n",
      "Epoch 83/1500\n",
      "514/514 [==============================] - 0s 53us/step - loss: 0.0268 - val_loss: 0.0215\n",
      "Epoch 84/1500\n",
      "514/514 [==============================] - 0s 54us/step - loss: 0.0222 - val_loss: 0.0094\n",
      "Epoch 85/1500\n",
      "514/514 [==============================] - 0s 50us/step - loss: 0.0127 - val_loss: 0.0134\n",
      "Epoch 86/1500\n",
      "514/514 [==============================] - 0s 52us/step - loss: 0.0163 - val_loss: 0.0121\n",
      "Epoch 87/1500\n",
      "514/514 [==============================] - 0s 47us/step - loss: 0.0158 - val_loss: 0.0107\n",
      "Epoch 88/1500\n",
      "514/514 [==============================] - 0s 49us/step - loss: 0.0140 - val_loss: 0.0087\n",
      "Epoch 89/1500\n",
      "514/514 [==============================] - 0s 50us/step - loss: 0.0122 - val_loss: 0.0086\n",
      "Epoch 90/1500\n",
      "514/514 [==============================] - 0s 47us/step - loss: 0.0117 - val_loss: 0.0153\n",
      "Epoch 91/1500\n",
      "514/514 [==============================] - 0s 45us/step - loss: 0.0191 - val_loss: 0.0120\n",
      "Epoch 92/1500\n",
      "514/514 [==============================] - 0s 53us/step - loss: 0.0133 - val_loss: 0.0195\n",
      "Epoch 93/1500\n",
      "514/514 [==============================] - 0s 47us/step - loss: 0.0258 - val_loss: 0.0142\n",
      "Epoch 94/1500\n",
      "514/514 [==============================] - 0s 42us/step - loss: 0.0136 - val_loss: 0.0141\n",
      "Epoch 95/1500\n",
      "514/514 [==============================] - 0s 47us/step - loss: 0.0168 - val_loss: 0.0095\n",
      "Epoch 96/1500\n",
      "514/514 [==============================] - 0s 59us/step - loss: 0.0155 - val_loss: 0.0118\n",
      "Epoch 97/1500\n",
      "514/514 [==============================] - 0s 51us/step - loss: 0.0126 - val_loss: 0.0176\n",
      "Epoch 98/1500\n",
      "514/514 [==============================] - 0s 47us/step - loss: 0.0215 - val_loss: 0.0098\n",
      "Epoch 99/1500\n",
      "514/514 [==============================] - 0s 61us/step - loss: 0.0119 - val_loss: 0.0156\n",
      "Epoch 100/1500\n",
      "514/514 [==============================] - 0s 65us/step - loss: 0.0178 - val_loss: 0.0122\n",
      "Epoch 101/1500\n",
      "514/514 [==============================] - 0s 54us/step - loss: 0.0242 - val_loss: 0.0152\n",
      "Epoch 102/1500\n",
      "514/514 [==============================] - 0s 56us/step - loss: 0.0173 - val_loss: 0.0376\n",
      "Epoch 103/1500\n",
      "514/514 [==============================] - 0s 66us/step - loss: 0.0378 - val_loss: 0.0098\n",
      "Epoch 104/1500\n",
      "514/514 [==============================] - 0s 60us/step - loss: 0.0157 - val_loss: 0.0163\n",
      "Epoch 105/1500\n",
      "514/514 [==============================] - 0s 55us/step - loss: 0.0162 - val_loss: 0.0147\n",
      "Epoch 106/1500\n",
      "514/514 [==============================] - 0s 52us/step - loss: 0.0190 - val_loss: 0.0162\n",
      "Epoch 107/1500\n",
      "514/514 [==============================] - 0s 52us/step - loss: 0.0164 - val_loss: 0.0116\n",
      "Epoch 108/1500\n",
      "514/514 [==============================] - 0s 51us/step - loss: 0.0151 - val_loss: 0.0137\n",
      "Epoch 109/1500\n",
      "514/514 [==============================] - 0s 46us/step - loss: 0.0147 - val_loss: 0.0180\n",
      "Epoch 110/1500\n",
      "514/514 [==============================] - 0s 46us/step - loss: 0.0208 - val_loss: 0.0130\n",
      "Epoch 111/1500\n",
      "514/514 [==============================] - 0s 47us/step - loss: 0.0134 - val_loss: 0.0191\n",
      "Epoch 112/1500\n",
      "514/514 [==============================] - 0s 42us/step - loss: 0.0240 - val_loss: 0.0130\n",
      "Epoch 113/1500\n",
      "514/514 [==============================] - 0s 46us/step - loss: 0.0166 - val_loss: 0.0324\n",
      "Epoch 114/1500\n",
      "514/514 [==============================] - 0s 39us/step - loss: 0.0320 - val_loss: 0.0103\n",
      "Epoch 115/1500\n",
      "514/514 [==============================] - 0s 43us/step - loss: 0.0128 - val_loss: 0.0094\n",
      "Epoch 116/1500\n",
      "514/514 [==============================] - 0s 48us/step - loss: 0.0122 - val_loss: 0.0125\n",
      "Epoch 117/1500\n",
      "514/514 [==============================] - 0s 50us/step - loss: 0.0160 - val_loss: 0.0085\n",
      "Epoch 118/1500\n",
      "514/514 [==============================] - 0s 49us/step - loss: 0.0146 - val_loss: 0.0143\n",
      "Epoch 119/1500\n",
      "514/514 [==============================] - 0s 50us/step - loss: 0.0150 - val_loss: 0.0093\n",
      "Epoch 120/1500\n",
      "514/514 [==============================] - 0s 60us/step - loss: 0.0115 - val_loss: 0.0142\n",
      "Epoch 121/1500\n",
      "514/514 [==============================] - 0s 53us/step - loss: 0.0176 - val_loss: 0.0097\n",
      "Epoch 122/1500\n",
      "514/514 [==============================] - 0s 55us/step - loss: 0.0122 - val_loss: 0.0153\n",
      "Epoch 123/1500\n",
      "514/514 [==============================] - 0s 47us/step - loss: 0.0166 - val_loss: 0.0090\n",
      "Epoch 124/1500\n",
      "514/514 [==============================] - 0s 51us/step - loss: 0.0121 - val_loss: 0.0086\n",
      "Epoch 125/1500\n",
      "514/514 [==============================] - 0s 55us/step - loss: 0.0111 - val_loss: 0.0088\n",
      "Epoch 126/1500\n",
      "514/514 [==============================] - 0s 67us/step - loss: 0.0116 - val_loss: 0.0105\n",
      "Epoch 127/1500\n",
      "514/514 [==============================] - 0s 63us/step - loss: 0.0131 - val_loss: 0.0098\n",
      "Epoch 128/1500\n",
      "514/514 [==============================] - 0s 63us/step - loss: 0.0133 - val_loss: 0.0170\n",
      "Epoch 129/1500\n",
      "514/514 [==============================] - 0s 51us/step - loss: 0.0184 - val_loss: 0.0110\n",
      "Epoch 130/1500\n",
      "514/514 [==============================] - 0s 53us/step - loss: 0.0152 - val_loss: 0.0304\n",
      "Epoch 131/1500\n",
      "514/514 [==============================] - 0s 63us/step - loss: 0.0300 - val_loss: 0.0133\n",
      "Epoch 132/1500\n",
      "514/514 [==============================] - 0s 51us/step - loss: 0.0139 - val_loss: 0.0100\n",
      "Epoch 133/1500\n",
      "514/514 [==============================] - 0s 50us/step - loss: 0.0134 - val_loss: 0.0086\n",
      "Epoch 134/1500\n",
      "514/514 [==============================] - 0s 53us/step - loss: 0.0125 - val_loss: 0.0158\n",
      "Epoch 135/1500\n",
      "514/514 [==============================] - 0s 55us/step - loss: 0.0191 - val_loss: 0.0122\n",
      "Epoch 136/1500\n",
      "514/514 [==============================] - ETA: 0s - loss: 0.016 - 0s 44us/step - loss: 0.0145 - val_loss: 0.0081\n",
      "Epoch 137/1500\n",
      "514/514 [==============================] - 0s 46us/step - loss: 0.0113 - val_loss: 0.0082\n",
      "Epoch 138/1500\n",
      "514/514 [==============================] - 0s 48us/step - loss: 0.0109 - val_loss: 0.0091\n",
      "Epoch 139/1500\n",
      "514/514 [==============================] - ETA: 0s - loss: 0.011 - 0s 42us/step - loss: 0.0112 - val_loss: 0.0114\n",
      "Epoch 140/1500\n",
      "514/514 [==============================] - 0s 42us/step - loss: 0.0137 - val_loss: 0.0124\n",
      "Epoch 141/1500\n",
      "514/514 [==============================] - 0s 45us/step - loss: 0.0144 - val_loss: 0.0091\n",
      "Epoch 142/1500\n",
      "514/514 [==============================] - 0s 46us/step - loss: 0.0129 - val_loss: 0.0174\n",
      "Epoch 143/1500\n",
      "514/514 [==============================] - 0s 48us/step - loss: 0.0182 - val_loss: 0.0094\n",
      "Epoch 144/1500\n",
      "514/514 [==============================] - 0s 41us/step - loss: 0.0152 - val_loss: 0.0223\n",
      "Epoch 145/1500\n",
      "514/514 [==============================] - 0s 45us/step - loss: 0.0223 - val_loss: 0.0096\n",
      "Epoch 146/1500\n",
      "514/514 [==============================] - 0s 47us/step - loss: 0.0159 - val_loss: 0.0125\n",
      "Epoch 147/1500\n",
      "514/514 [==============================] - 0s 47us/step - loss: 0.0131 - val_loss: 0.0143\n",
      "Epoch 148/1500\n",
      "514/514 [==============================] - 0s 50us/step - loss: 0.0174 - val_loss: 0.0104\n",
      "Epoch 149/1500\n",
      "514/514 [==============================] - 0s 51us/step - loss: 0.0130 - val_loss: 0.0134\n",
      "Epoch 150/1500\n",
      "514/514 [==============================] - 0s 49us/step - loss: 0.0153 - val_loss: 0.0087\n",
      "Epoch 151/1500\n",
      "514/514 [==============================] - 0s 50us/step - loss: 0.0132 - val_loss: 0.0351\n",
      "Epoch 152/1500\n",
      "514/514 [==============================] - 0s 46us/step - loss: 0.0344 - val_loss: 0.0348\n",
      "Epoch 153/1500\n",
      "514/514 [==============================] - 0s 47us/step - loss: 0.0302 - val_loss: 0.0111\n",
      "Epoch 154/1500\n",
      "514/514 [==============================] - 0s 42us/step - loss: 0.0158 - val_loss: 0.0153\n",
      "Epoch 155/1500\n",
      "514/514 [==============================] - 0s 47us/step - loss: 0.0170 - val_loss: 0.0214\n",
      "Epoch 156/1500\n",
      "514/514 [==============================] - 0s 44us/step - loss: 0.0228 - val_loss: 0.0155\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 157/1500\n",
      "514/514 [==============================] - 0s 46us/step - loss: 0.0231 - val_loss: 0.0149\n",
      "Epoch 158/1500\n",
      "514/514 [==============================] - 0s 37us/step - loss: 0.0149 - val_loss: 0.0199\n",
      "Epoch 159/1500\n",
      "514/514 [==============================] - 0s 46us/step - loss: 0.0181 - val_loss: 0.0114\n",
      "Epoch 160/1500\n",
      "514/514 [==============================] - 0s 48us/step - loss: 0.0150 - val_loss: 0.0156\n",
      "Epoch 161/1500\n",
      "514/514 [==============================] - 0s 46us/step - loss: 0.0176 - val_loss: 0.0118\n",
      "Epoch 162/1500\n",
      "514/514 [==============================] - ETA: 0s - loss: 0.015 - 0s 53us/step - loss: 0.0148 - val_loss: 0.0122\n",
      "Epoch 163/1500\n",
      "514/514 [==============================] - 0s 50us/step - loss: 0.0159 - val_loss: 0.0109\n",
      "Epoch 164/1500\n",
      "514/514 [==============================] - 0s 58us/step - loss: 0.0141 - val_loss: 0.0109\n",
      "Epoch 165/1500\n",
      "514/514 [==============================] - 0s 51us/step - loss: 0.0136 - val_loss: 0.0130\n",
      "Epoch 166/1500\n",
      "514/514 [==============================] - 0s 46us/step - loss: 0.0162 - val_loss: 0.0258\n",
      "Epoch 167/1500\n",
      "514/514 [==============================] - 0s 42us/step - loss: 0.0253 - val_loss: 0.0116\n",
      "Epoch 168/1500\n",
      "514/514 [==============================] - 0s 54us/step - loss: 0.0155 - val_loss: 0.0223\n",
      "Epoch 169/1500\n",
      "514/514 [==============================] - 0s 48us/step - loss: 0.0352 - val_loss: 0.0383\n",
      "Epoch 170/1500\n",
      "514/514 [==============================] - 0s 46us/step - loss: 0.0359 - val_loss: 0.0209\n",
      "Epoch 171/1500\n",
      "514/514 [==============================] - 0s 50us/step - loss: 0.0198 - val_loss: 0.0376\n",
      "Epoch 172/1500\n",
      "514/514 [==============================] - 0s 48us/step - loss: 0.0455 - val_loss: 0.0508\n",
      "Epoch 173/1500\n",
      "514/514 [==============================] - 0s 53us/step - loss: 0.0405 - val_loss: 0.0114\n",
      "Epoch 174/1500\n",
      "514/514 [==============================] - 0s 52us/step - loss: 0.0161 - val_loss: 0.0215\n",
      "Epoch 175/1500\n",
      "514/514 [==============================] - 0s 49us/step - loss: 0.0206 - val_loss: 0.0449\n",
      "Epoch 176/1500\n",
      "514/514 [==============================] - 0s 53us/step - loss: 0.0458 - val_loss: 0.0303\n",
      "Epoch 177/1500\n",
      "514/514 [==============================] - 0s 54us/step - loss: 0.0327 - val_loss: 0.0215\n",
      "Epoch 178/1500\n",
      "514/514 [==============================] - 0s 52us/step - loss: 0.0211 - val_loss: 0.0176\n",
      "Epoch 179/1500\n",
      "514/514 [==============================] - 0s 44us/step - loss: 0.0184 - val_loss: 0.0374\n",
      "Epoch 180/1500\n",
      "514/514 [==============================] - 0s 48us/step - loss: 0.0336 - val_loss: 0.0218\n",
      "Epoch 181/1500\n",
      "514/514 [==============================] - 0s 56us/step - loss: 0.0342 - val_loss: 0.0168\n",
      "Epoch 182/1500\n",
      "514/514 [==============================] - 0s 58us/step - loss: 0.0171 - val_loss: 0.0256\n",
      "Epoch 183/1500\n",
      "514/514 [==============================] - 0s 47us/step - loss: 0.0250 - val_loss: 0.0167\n",
      "Epoch 184/1500\n",
      "514/514 [==============================] - 0s 46us/step - loss: 0.0190 - val_loss: 0.0158\n",
      "Epoch 185/1500\n",
      "514/514 [==============================] - 0s 51us/step - loss: 0.0169 - val_loss: 0.0125\n",
      "Epoch 186/1500\n",
      "514/514 [==============================] - 0s 53us/step - loss: 0.0143 - val_loss: 0.0120\n",
      "Epoch 187/1500\n",
      "514/514 [==============================] - 0s 45us/step - loss: 0.0155 - val_loss: 0.0189\n",
      "Epoch 188/1500\n",
      "514/514 [==============================] - 0s 52us/step - loss: 0.0188 - val_loss: 0.0123\n",
      "Epoch 189/1500\n",
      "514/514 [==============================] - 0s 51us/step - loss: 0.0205 - val_loss: 0.0244\n",
      "Epoch 190/1500\n",
      "514/514 [==============================] - 0s 47us/step - loss: 0.0236 - val_loss: 0.0216\n",
      "Epoch 191/1500\n",
      "514/514 [==============================] - 0s 51us/step - loss: 0.0290 - val_loss: 0.0156\n",
      "Epoch 192/1500\n",
      "514/514 [==============================] - 0s 52us/step - loss: 0.0167 - val_loss: 0.0146\n",
      "Epoch 193/1500\n",
      "514/514 [==============================] - 0s 52us/step - loss: 0.0158 - val_loss: 0.0169\n",
      "Epoch 194/1500\n",
      "514/514 [==============================] - 0s 51us/step - loss: 0.0265 - val_loss: 0.0257\n",
      "Epoch 195/1500\n",
      "514/514 [==============================] - 0s 48us/step - loss: 0.0227 - val_loss: 0.0111\n",
      "Epoch 196/1500\n",
      "514/514 [==============================] - 0s 50us/step - loss: 0.0147 - val_loss: 0.0126\n",
      "Epoch 197/1500\n",
      "514/514 [==============================] - 0s 45us/step - loss: 0.0140 - val_loss: 0.0162\n",
      "Epoch 198/1500\n",
      "514/514 [==============================] - 0s 58us/step - loss: 0.0198 - val_loss: 0.0171\n",
      "Epoch 199/1500\n",
      "514/514 [==============================] - 0s 46us/step - loss: 0.0165 - val_loss: 0.0129\n",
      "Epoch 200/1500\n",
      "514/514 [==============================] - 0s 59us/step - loss: 0.0139 - val_loss: 0.0135\n",
      "Epoch 201/1500\n",
      "514/514 [==============================] - 0s 54us/step - loss: 0.0141 - val_loss: 0.0127\n",
      "Epoch 202/1500\n",
      "514/514 [==============================] - 0s 56us/step - loss: 0.0133 - val_loss: 0.0105\n",
      "Epoch 203/1500\n",
      "514/514 [==============================] - 0s 54us/step - loss: 0.0119 - val_loss: 0.0086\n",
      "Epoch 204/1500\n",
      "514/514 [==============================] - 0s 58us/step - loss: 0.0110 - val_loss: 0.0084\n",
      "Epoch 205/1500\n",
      "514/514 [==============================] - 0s 48us/step - loss: 0.0114 - val_loss: 0.0085\n",
      "Epoch 206/1500\n",
      "514/514 [==============================] - 0s 56us/step - loss: 0.0110 - val_loss: 0.0122\n",
      "Epoch 207/1500\n",
      "514/514 [==============================] - 0s 50us/step - loss: 0.0137 - val_loss: 0.0148\n",
      "Epoch 208/1500\n",
      "514/514 [==============================] - 0s 53us/step - loss: 0.0148 - val_loss: 0.0130\n",
      "Epoch 209/1500\n",
      "514/514 [==============================] - 0s 60us/step - loss: 0.0138 - val_loss: 0.0145\n",
      "Epoch 210/1500\n",
      "514/514 [==============================] - 0s 52us/step - loss: 0.0156 - val_loss: 0.0131\n",
      "Epoch 211/1500\n",
      "514/514 [==============================] - 0s 38us/step - loss: 0.0136 - val_loss: 0.0130\n",
      "Epoch 212/1500\n",
      "514/514 [==============================] - 0s 38us/step - loss: 0.0162 - val_loss: 0.0112\n",
      "Epoch 213/1500\n",
      "514/514 [==============================] - 0s 44us/step - loss: 0.0134 - val_loss: 0.0135\n",
      "Epoch 214/1500\n",
      "514/514 [==============================] - 0s 40us/step - loss: 0.0164 - val_loss: 0.0159\n",
      "Epoch 215/1500\n",
      "514/514 [==============================] - 0s 36us/step - loss: 0.0168 - val_loss: 0.0154\n",
      "Epoch 216/1500\n",
      "514/514 [==============================] - 0s 41us/step - loss: 0.0160 - val_loss: 0.0154\n",
      "Epoch 217/1500\n",
      "514/514 [==============================] - 0s 49us/step - loss: 0.0159 - val_loss: 0.0135\n",
      "Epoch 218/1500\n",
      "514/514 [==============================] - 0s 42us/step - loss: 0.0142 - val_loss: 0.0109\n",
      "Epoch 219/1500\n",
      "514/514 [==============================] - 0s 47us/step - loss: 0.0119 - val_loss: 0.0088\n",
      "Epoch 220/1500\n",
      "514/514 [==============================] - 0s 47us/step - loss: 0.0111 - val_loss: 0.0082\n",
      "Epoch 221/1500\n",
      "514/514 [==============================] - 0s 44us/step - loss: 0.0111 - val_loss: 0.0097\n",
      "Epoch 222/1500\n",
      "514/514 [==============================] - 0s 49us/step - loss: 0.0121 - val_loss: 0.0100\n",
      "Epoch 223/1500\n",
      "514/514 [==============================] - 0s 47us/step - loss: 0.0124 - val_loss: 0.0106\n",
      "Epoch 224/1500\n",
      "514/514 [==============================] - 0s 53us/step - loss: 0.0126 - val_loss: 0.0097\n",
      "Epoch 225/1500\n",
      "514/514 [==============================] - 0s 53us/step - loss: 0.0118 - val_loss: 0.0084\n",
      "Epoch 226/1500\n",
      "514/514 [==============================] - 0s 41us/step - loss: 0.0120 - val_loss: 0.0120\n",
      "Epoch 227/1500\n",
      "514/514 [==============================] - 0s 40us/step - loss: 0.0137 - val_loss: 0.0142\n",
      "Epoch 228/1500\n",
      "514/514 [==============================] - 0s 45us/step - loss: 0.0160 - val_loss: 0.0167\n",
      "Epoch 229/1500\n",
      "514/514 [==============================] - 0s 50us/step - loss: 0.0185 - val_loss: 0.0234\n",
      "Epoch 230/1500\n",
      "514/514 [==============================] - 0s 49us/step - loss: 0.0205 - val_loss: 0.0126\n",
      "Epoch 231/1500\n",
      "514/514 [==============================] - 0s 47us/step - loss: 0.0169 - val_loss: 0.0149\n",
      "Epoch 232/1500\n",
      "514/514 [==============================] - 0s 50us/step - loss: 0.0161 - val_loss: 0.0139\n",
      "Epoch 233/1500\n",
      "514/514 [==============================] - 0s 48us/step - loss: 0.0193 - val_loss: 0.0135\n",
      "Epoch 234/1500\n",
      "514/514 [==============================] - 0s 40us/step - loss: 0.0153 - val_loss: 0.0193\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 235/1500\n",
      "514/514 [==============================] - 0s 43us/step - loss: 0.0202 - val_loss: 0.0137\n",
      "Epoch 236/1500\n",
      "514/514 [==============================] - 0s 45us/step - loss: 0.0173 - val_loss: 0.0118\n",
      "Epoch 237/1500\n",
      "514/514 [==============================] - 0s 52us/step - loss: 0.0137 - val_loss: 0.0110\n",
      "Epoch 238/1500\n",
      "514/514 [==============================] - 0s 56us/step - loss: 0.0140 - val_loss: 0.0103\n",
      "Epoch 239/1500\n",
      "514/514 [==============================] - 0s 49us/step - loss: 0.0139 - val_loss: 0.0107\n",
      "Epoch 240/1500\n",
      "514/514 [==============================] - 0s 55us/step - loss: 0.0126 - val_loss: 0.0090\n",
      "Epoch 241/1500\n",
      "514/514 [==============================] - 0s 53us/step - loss: 0.0115 - val_loss: 0.0138\n",
      "Epoch 242/1500\n",
      "514/514 [==============================] - 0s 55us/step - loss: 0.0144 - val_loss: 0.0152\n",
      "Epoch 243/1500\n",
      "514/514 [==============================] - 0s 66us/step - loss: 0.0149 - val_loss: 0.0103\n",
      "Epoch 244/1500\n",
      "514/514 [==============================] - 0s 50us/step - loss: 0.0113 - val_loss: 0.0087\n",
      "Epoch 245/1500\n",
      "514/514 [==============================] - 0s 56us/step - loss: 0.0114 - val_loss: 0.0100\n",
      "Epoch 246/1500\n",
      "514/514 [==============================] - 0s 57us/step - loss: 0.0142 - val_loss: 0.0095\n",
      "Epoch 247/1500\n",
      "514/514 [==============================] - 0s 50us/step - loss: 0.0137 - val_loss: 0.0146\n",
      "Epoch 248/1500\n",
      "514/514 [==============================] - 0s 57us/step - loss: 0.0159 - val_loss: 0.0094\n",
      "Epoch 249/1500\n",
      "514/514 [==============================] - 0s 76us/step - loss: 0.0126 - val_loss: 0.0132\n",
      "Epoch 250/1500\n",
      "514/514 [==============================] - 0s 74us/step - loss: 0.0135 - val_loss: 0.0112\n",
      "Epoch 251/1500\n",
      "514/514 [==============================] - 0s 63us/step - loss: 0.0130 - val_loss: 0.0133\n",
      "Epoch 252/1500\n",
      "514/514 [==============================] - 0s 62us/step - loss: 0.0157 - val_loss: 0.0116\n",
      "Epoch 253/1500\n",
      "514/514 [==============================] - 0s 56us/step - loss: 0.0143 - val_loss: 0.0187\n",
      "Epoch 254/1500\n",
      "514/514 [==============================] - 0s 67us/step - loss: 0.0188 - val_loss: 0.0092\n",
      "Epoch 255/1500\n",
      "514/514 [==============================] - 0s 47us/step - loss: 0.0123 - val_loss: 0.0091\n",
      "Epoch 256/1500\n",
      "514/514 [==============================] - 0s 56us/step - loss: 0.0120 - val_loss: 0.0095\n",
      "Epoch 257/1500\n",
      "514/514 [==============================] - 0s 66us/step - loss: 0.0120 - val_loss: 0.0085\n",
      "Epoch 258/1500\n",
      "514/514 [==============================] - 0s 62us/step - loss: 0.0111 - val_loss: 0.0093\n",
      "Epoch 259/1500\n",
      "514/514 [==============================] - 0s 66us/step - loss: 0.0121 - val_loss: 0.0101\n",
      "Epoch 260/1500\n",
      "514/514 [==============================] - 0s 66us/step - loss: 0.0116 - val_loss: 0.0104\n",
      "Epoch 261/1500\n",
      "514/514 [==============================] - 0s 65us/step - loss: 0.0138 - val_loss: 0.0091\n",
      "Epoch 262/1500\n",
      "514/514 [==============================] - 0s 66us/step - loss: 0.0111 - val_loss: 0.0122\n",
      "Epoch 263/1500\n",
      "514/514 [==============================] - 0s 54us/step - loss: 0.0152 - val_loss: 0.0103\n",
      "Epoch 264/1500\n",
      "514/514 [==============================] - 0s 53us/step - loss: 0.0119 - val_loss: 0.0091\n",
      "Epoch 265/1500\n",
      "514/514 [==============================] - 0s 53us/step - loss: 0.0125 - val_loss: 0.0080\n",
      "Epoch 266/1500\n",
      "514/514 [==============================] - 0s 59us/step - loss: 0.0109 - val_loss: 0.0108\n",
      "Epoch 267/1500\n",
      "514/514 [==============================] - 0s 58us/step - loss: 0.0130 - val_loss: 0.0210\n",
      "Epoch 268/1500\n",
      "514/514 [==============================] - 0s 58us/step - loss: 0.0234 - val_loss: 0.0085\n",
      "Epoch 269/1500\n",
      "514/514 [==============================] - 0s 52us/step - loss: 0.0132 - val_loss: 0.0214\n",
      "Epoch 270/1500\n",
      "514/514 [==============================] - 0s 50us/step - loss: 0.0201 - val_loss: 0.0160\n",
      "Epoch 271/1500\n",
      "514/514 [==============================] - 0s 53us/step - loss: 0.0180 - val_loss: 0.0154\n",
      "Epoch 272/1500\n",
      "514/514 [==============================] - 0s 58us/step - loss: 0.0172 - val_loss: 0.0307\n",
      "Epoch 273/1500\n",
      "514/514 [==============================] - 0s 56us/step - loss: 0.0263 - val_loss: 0.0276\n",
      "Epoch 274/1500\n",
      "514/514 [==============================] - 0s 60us/step - loss: 0.0225 - val_loss: 0.0205\n",
      "Epoch 275/1500\n",
      "514/514 [==============================] - 0s 57us/step - loss: 0.0201 - val_loss: 0.0182\n",
      "Epoch 276/1500\n",
      "514/514 [==============================] - 0s 63us/step - loss: 0.0207 - val_loss: 0.0191\n",
      "Epoch 277/1500\n",
      "514/514 [==============================] - 0s 61us/step - loss: 0.0202 - val_loss: 0.0118\n",
      "Epoch 278/1500\n",
      "514/514 [==============================] - 0s 59us/step - loss: 0.0136 - val_loss: 0.0122\n",
      "Epoch 279/1500\n",
      "514/514 [==============================] - 0s 63us/step - loss: 0.0154 - val_loss: 0.0150\n",
      "Epoch 280/1500\n",
      "514/514 [==============================] - 0s 70us/step - loss: 0.0177 - val_loss: 0.0140\n",
      "Epoch 281/1500\n",
      "514/514 [==============================] - 0s 62us/step - loss: 0.0148 - val_loss: 0.0146\n",
      "Epoch 282/1500\n",
      "514/514 [==============================] - 0s 66us/step - loss: 0.0157 - val_loss: 0.0141\n",
      "Epoch 283/1500\n",
      "514/514 [==============================] - 0s 99us/step - loss: 0.0160 - val_loss: 0.0112\n",
      "Epoch 284/1500\n",
      "514/514 [==============================] - 0s 50us/step - loss: 0.0136 - val_loss: 0.0179\n",
      "Epoch 285/1500\n",
      "514/514 [==============================] - 0s 51us/step - loss: 0.0200 - val_loss: 0.0137\n",
      "Epoch 286/1500\n",
      "514/514 [==============================] - 0s 57us/step - loss: 0.0180 - val_loss: 0.0175\n",
      "Epoch 287/1500\n",
      "514/514 [==============================] - 0s 54us/step - loss: 0.0184 - val_loss: 0.0104\n",
      "Epoch 288/1500\n",
      "514/514 [==============================] - 0s 42us/step - loss: 0.0140 - val_loss: 0.0110\n",
      "Epoch 289/1500\n",
      "514/514 [==============================] - 0s 51us/step - loss: 0.0128 - val_loss: 0.0196\n",
      "Epoch 290/1500\n",
      "514/514 [==============================] - 0s 54us/step - loss: 0.0177 - val_loss: 0.0090\n",
      "Epoch 291/1500\n",
      "514/514 [==============================] - 0s 43us/step - loss: 0.0112 - val_loss: 0.0091\n",
      "Epoch 292/1500\n",
      "514/514 [==============================] - 0s 66us/step - loss: 0.0119 - val_loss: 0.0127\n",
      "Epoch 293/1500\n",
      "514/514 [==============================] - 0s 35us/step - loss: 0.0162 - val_loss: 0.0116\n",
      "Epoch 294/1500\n",
      "514/514 [==============================] - 0s 38us/step - loss: 0.0134 - val_loss: 0.0091\n",
      "Epoch 295/1500\n",
      "514/514 [==============================] - 0s 45us/step - loss: 0.0123 - val_loss: 0.0096\n",
      "Epoch 296/1500\n",
      "514/514 [==============================] - 0s 55us/step - loss: 0.0123 - val_loss: 0.0167\n",
      "Epoch 297/1500\n",
      "514/514 [==============================] - 0s 36us/step - loss: 0.0166 - val_loss: 0.0114\n",
      "Epoch 298/1500\n",
      "514/514 [==============================] - 0s 35us/step - loss: 0.0127 - val_loss: 0.0103\n",
      "Epoch 299/1500\n",
      "514/514 [==============================] - 0s 34us/step - loss: 0.0128 - val_loss: 0.0093\n",
      "Epoch 300/1500\n",
      "514/514 [==============================] - 0s 39us/step - loss: 0.0117 - val_loss: 0.0137\n",
      "Epoch 301/1500\n",
      "514/514 [==============================] - 0s 32us/step - loss: 0.0139 - val_loss: 0.0130\n",
      "Epoch 302/1500\n",
      "514/514 [==============================] - 0s 48us/step - loss: 0.0127 - val_loss: 0.0102\n",
      "Epoch 303/1500\n",
      "514/514 [==============================] - 0s 35us/step - loss: 0.0115 - val_loss: 0.0100\n",
      "Epoch 304/1500\n",
      "514/514 [==============================] - 0s 34us/step - loss: 0.0120 - val_loss: 0.0100\n",
      "Epoch 305/1500\n",
      "514/514 [==============================] - 0s 34us/step - loss: 0.0125 - val_loss: 0.0119\n",
      "Epoch 306/1500\n",
      "514/514 [==============================] - 0s 33us/step - loss: 0.0163 - val_loss: 0.0172\n",
      "Epoch 307/1500\n",
      "514/514 [==============================] - 0s 33us/step - loss: 0.0252 - val_loss: 0.0298\n",
      "Epoch 308/1500\n",
      "514/514 [==============================] - 0s 33us/step - loss: 0.0338 - val_loss: 0.0391\n",
      "Epoch 309/1500\n",
      "514/514 [==============================] - 0s 35us/step - loss: 0.0415 - val_loss: 0.0421\n",
      "Epoch 310/1500\n",
      "514/514 [==============================] - 0s 34us/step - loss: 0.0478 - val_loss: 0.0443\n",
      "Epoch 311/1500\n",
      "514/514 [==============================] - 0s 37us/step - loss: 0.0449 - val_loss: 0.0357\n",
      "Epoch 312/1500\n",
      "514/514 [==============================] - 0s 33us/step - loss: 0.0385 - val_loss: 0.0333\n",
      "Epoch 313/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "514/514 [==============================] - 0s 32us/step - loss: 0.0363 - val_loss: 0.0279\n",
      "Epoch 314/1500\n",
      "514/514 [==============================] - 0s 37us/step - loss: 0.0312 - val_loss: 0.0289\n",
      "Epoch 315/1500\n",
      "514/514 [==============================] - 0s 35us/step - loss: 0.0357 - val_loss: 0.0227\n",
      "Epoch 316/1500\n",
      "514/514 [==============================] - 0s 35us/step - loss: 0.0280 - val_loss: 0.0229\n",
      "Epoch 317/1500\n",
      "514/514 [==============================] - 0s 33us/step - loss: 0.0241 - val_loss: 0.0293\n",
      "Epoch 318/1500\n",
      "514/514 [==============================] - 0s 34us/step - loss: 0.0279 - val_loss: 0.0234\n",
      "Epoch 319/1500\n",
      "514/514 [==============================] - 0s 36us/step - loss: 0.0241 - val_loss: 0.0407\n",
      "Epoch 320/1500\n",
      "514/514 [==============================] - 0s 34us/step - loss: 0.0399 - val_loss: 0.0146\n",
      "Epoch 321/1500\n",
      "514/514 [==============================] - 0s 32us/step - loss: 0.0167 - val_loss: 0.0142\n",
      "Epoch 322/1500\n",
      "514/514 [==============================] - 0s 33us/step - loss: 0.0162 - val_loss: 0.0151\n",
      "Epoch 323/1500\n",
      "514/514 [==============================] - 0s 32us/step - loss: 0.0173 - val_loss: 0.0132\n",
      "Epoch 324/1500\n",
      "514/514 [==============================] - 0s 61us/step - loss: 0.0172 - val_loss: 0.0115\n",
      "Epoch 325/1500\n",
      "514/514 [==============================] - 0s 36us/step - loss: 0.0149 - val_loss: 0.0148\n",
      "Epoch 326/1500\n",
      "514/514 [==============================] - 0s 74us/step - loss: 0.0211 - val_loss: 0.0104\n",
      "Epoch 327/1500\n",
      "514/514 [==============================] - 0s 35us/step - loss: 0.0161 - val_loss: 0.0120\n",
      "Epoch 328/1500\n",
      "514/514 [==============================] - 0s 50us/step - loss: 0.0195 - val_loss: 0.0238\n",
      "Epoch 329/1500\n",
      "514/514 [==============================] - 0s 39us/step - loss: 0.0346 - val_loss: 0.0995\n",
      "Epoch 330/1500\n",
      "514/514 [==============================] - 0s 33us/step - loss: 0.0948 - val_loss: 0.0466\n",
      "Epoch 331/1500\n",
      "514/514 [==============================] - 0s 43us/step - loss: 0.0449 - val_loss: 0.0978\n",
      "Epoch 332/1500\n",
      "514/514 [==============================] - 0s 48us/step - loss: 0.0814 - val_loss: 0.0609\n",
      "Epoch 333/1500\n",
      "514/514 [==============================] - 0s 53us/step - loss: 0.0641 - val_loss: 0.0773\n",
      "Epoch 334/1500\n",
      "514/514 [==============================] - 0s 34us/step - loss: 0.0890 - val_loss: 0.0638\n",
      "Epoch 335/1500\n",
      "514/514 [==============================] - 0s 49us/step - loss: 0.0889 - val_loss: 0.0483\n",
      "Epoch 336/1500\n",
      "514/514 [==============================] - 0s 33us/step - loss: 0.0749 - val_loss: 0.0389\n",
      "Epoch 337/1500\n",
      "514/514 [==============================] - 0s 32us/step - loss: 0.0709 - val_loss: 0.0216\n",
      "Epoch 338/1500\n",
      "514/514 [==============================] - 0s 33us/step - loss: 0.0476 - val_loss: 0.0567\n",
      "Epoch 339/1500\n",
      "514/514 [==============================] - 0s 34us/step - loss: 0.0372 - val_loss: 0.0741\n",
      "Epoch 340/1500\n",
      "514/514 [==============================] - 0s 33us/step - loss: 0.0660 - val_loss: 0.0334\n",
      "Epoch 341/1500\n",
      "514/514 [==============================] - 0s 32us/step - loss: 0.0364 - val_loss: 0.0121\n",
      "Epoch 342/1500\n",
      "514/514 [==============================] - 0s 33us/step - loss: 0.0137 - val_loss: 0.0121\n",
      "Epoch 343/1500\n",
      "514/514 [==============================] - 0s 34us/step - loss: 0.0138 - val_loss: 0.0172\n",
      "Epoch 344/1500\n",
      "514/514 [==============================] - 0s 47us/step - loss: 0.0198 - val_loss: 0.0087\n",
      "Epoch 345/1500\n",
      "514/514 [==============================] - 0s 42us/step - loss: 0.0134 - val_loss: 0.0127\n",
      "Epoch 346/1500\n",
      "514/514 [==============================] - 0s 43us/step - loss: 0.0138 - val_loss: 0.0113\n",
      "Epoch 347/1500\n",
      "514/514 [==============================] - 0s 36us/step - loss: 0.0137 - val_loss: 0.0092\n",
      "Epoch 348/1500\n",
      "514/514 [==============================] - 0s 34us/step - loss: 0.0115 - val_loss: 0.0099\n",
      "Epoch 349/1500\n",
      "514/514 [==============================] - 0s 36us/step - loss: 0.0119 - val_loss: 0.0101\n",
      "Epoch 350/1500\n",
      "514/514 [==============================] - 0s 35us/step - loss: 0.0116 - val_loss: 0.0096\n",
      "Epoch 351/1500\n",
      "514/514 [==============================] - 0s 32us/step - loss: 0.0117 - val_loss: 0.0096\n",
      "Epoch 352/1500\n",
      "514/514 [==============================] - 0s 101us/step - loss: 0.0123 - val_loss: 0.0135\n",
      "Epoch 353/1500\n",
      "514/514 [==============================] - 0s 34us/step - loss: 0.0147 - val_loss: 0.0090\n",
      "Epoch 354/1500\n",
      "514/514 [==============================] - 0s 31us/step - loss: 0.0135 - val_loss: 0.0192\n",
      "Epoch 355/1500\n",
      "514/514 [==============================] - 0s 40us/step - loss: 0.0212 - val_loss: 0.0084\n",
      "Epoch 356/1500\n",
      "514/514 [==============================] - 0s 40us/step - loss: 0.0116 - val_loss: 0.0159\n",
      "Epoch 357/1500\n",
      "514/514 [==============================] - 0s 49us/step - loss: 0.0175 - val_loss: 0.0088\n",
      "Epoch 358/1500\n",
      "514/514 [==============================] - 0s 36us/step - loss: 0.0115 - val_loss: 0.0117\n",
      "Epoch 359/1500\n",
      "514/514 [==============================] - 0s 63us/step - loss: 0.0135 - val_loss: 0.0285\n",
      "Epoch 360/1500\n",
      "514/514 [==============================] - 0s 32us/step - loss: 0.0285 - val_loss: 0.0250\n",
      "Epoch 361/1500\n",
      "514/514 [==============================] - 0s 62us/step - loss: 0.0233 - val_loss: 0.0269\n",
      "Epoch 362/1500\n",
      "514/514 [==============================] - 0s 34us/step - loss: 0.0245 - val_loss: 0.0262\n",
      "Epoch 363/1500\n",
      "514/514 [==============================] - 0s 32us/step - loss: 0.0233 - val_loss: 0.0168\n",
      "Epoch 364/1500\n",
      "514/514 [==============================] - 0s 31us/step - loss: 0.0164 - val_loss: 0.0113\n",
      "Epoch 365/1500\n",
      "514/514 [==============================] - 0s 32us/step - loss: 0.0132 - val_loss: 0.0096\n",
      "Epoch 366/1500\n",
      "514/514 [==============================] - 0s 39us/step - loss: 0.0126 - val_loss: 0.0165\n",
      "Epoch 367/1500\n",
      "514/514 [==============================] - 0s 34us/step - loss: 0.0183 - val_loss: 0.0156\n",
      "Epoch 368/1500\n",
      "514/514 [==============================] - 0s 32us/step - loss: 0.0187 - val_loss: 0.0099\n",
      "Epoch 369/1500\n",
      "514/514 [==============================] - 0s 35us/step - loss: 0.0127 - val_loss: 0.0199\n",
      "Epoch 370/1500\n",
      "514/514 [==============================] - 0s 32us/step - loss: 0.0206 - val_loss: 0.0192\n",
      "Epoch 371/1500\n",
      "514/514 [==============================] - 0s 32us/step - loss: 0.0198 - val_loss: 0.0154\n",
      "Epoch 372/1500\n",
      "514/514 [==============================] - 0s 32us/step - loss: 0.0159 - val_loss: 0.0217\n",
      "Epoch 373/1500\n",
      "514/514 [==============================] - 0s 34us/step - loss: 0.0206 - val_loss: 0.0329\n",
      "Epoch 374/1500\n",
      "514/514 [==============================] - 0s 34us/step - loss: 0.0284 - val_loss: 0.0213\n",
      "Epoch 375/1500\n",
      "514/514 [==============================] - 0s 33us/step - loss: 0.0185 - val_loss: 0.0199\n",
      "Epoch 376/1500\n",
      "514/514 [==============================] - 0s 33us/step - loss: 0.0258 - val_loss: 0.0216\n",
      "Epoch 377/1500\n",
      "514/514 [==============================] - 0s 33us/step - loss: 0.0223 - val_loss: 0.0162\n",
      "Epoch 378/1500\n",
      "514/514 [==============================] - 0s 34us/step - loss: 0.0202 - val_loss: 0.0152\n",
      "Epoch 379/1500\n",
      "514/514 [==============================] - 0s 34us/step - loss: 0.0156 - val_loss: 0.0134\n",
      "Epoch 380/1500\n",
      "514/514 [==============================] - 0s 35us/step - loss: 0.0145 - val_loss: 0.0115\n",
      "Epoch 381/1500\n",
      "514/514 [==============================] - 0s 37us/step - loss: 0.0126 - val_loss: 0.0102\n",
      "Epoch 382/1500\n",
      "514/514 [==============================] - 0s 34us/step - loss: 0.0117 - val_loss: 0.0108\n",
      "Epoch 383/1500\n",
      "514/514 [==============================] - 0s 37us/step - loss: 0.0117 - val_loss: 0.0111\n",
      "Epoch 384/1500\n",
      "514/514 [==============================] - 0s 31us/step - loss: 0.0123 - val_loss: 0.0106\n",
      "Epoch 385/1500\n",
      "514/514 [==============================] - 0s 32us/step - loss: 0.0114 - val_loss: 0.0092\n",
      "Epoch 386/1500\n",
      "514/514 [==============================] - 0s 34us/step - loss: 0.0113 - val_loss: 0.0097\n",
      "Epoch 387/1500\n",
      "514/514 [==============================] - 0s 33us/step - loss: 0.0120 - val_loss: 0.0092\n",
      "Epoch 388/1500\n",
      "514/514 [==============================] - 0s 41us/step - loss: 0.0124 - val_loss: 0.0124\n",
      "Epoch 389/1500\n",
      "514/514 [==============================] - 0s 36us/step - loss: 0.0145 - val_loss: 0.0096\n",
      "Epoch 390/1500\n",
      "514/514 [==============================] - 0s 34us/step - loss: 0.0119 - val_loss: 0.0097\n",
      "Epoch 391/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "514/514 [==============================] - 0s 34us/step - loss: 0.0120 - val_loss: 0.0101\n",
      "Epoch 392/1500\n",
      "514/514 [==============================] - 0s 51us/step - loss: 0.0121 - val_loss: 0.0090\n",
      "Epoch 393/1500\n",
      "514/514 [==============================] - 0s 35us/step - loss: 0.0114 - val_loss: 0.0085\n",
      "Epoch 394/1500\n",
      "514/514 [==============================] - ETA: 0s - loss: 0.011 - 0s 60us/step - loss: 0.0113 - val_loss: 0.0088\n",
      "Epoch 395/1500\n",
      "514/514 [==============================] - 0s 37us/step - loss: 0.0114 - val_loss: 0.0084\n",
      "Epoch 396/1500\n",
      "514/514 [==============================] - 0s 53us/step - loss: 0.0109 - val_loss: 0.0094\n",
      "Epoch 397/1500\n",
      "514/514 [==============================] - 0s 33us/step - loss: 0.0123 - val_loss: 0.0115\n",
      "Epoch 398/1500\n",
      "514/514 [==============================] - 0s 32us/step - loss: 0.0131 - val_loss: 0.0087\n",
      "Epoch 399/1500\n",
      "514/514 [==============================] - 0s 33us/step - loss: 0.0106 - val_loss: 0.0110\n",
      "Epoch 400/1500\n",
      "514/514 [==============================] - 0s 33us/step - loss: 0.0127 - val_loss: 0.0089\n",
      "Epoch 401/1500\n",
      "514/514 [==============================] - 0s 41us/step - loss: 0.0107 - val_loss: 0.0100\n",
      "Epoch 402/1500\n",
      "514/514 [==============================] - 0s 34us/step - loss: 0.0119 - val_loss: 0.0090\n",
      "Epoch 403/1500\n",
      "514/514 [==============================] - 0s 31us/step - loss: 0.0107 - val_loss: 0.0096\n",
      "Epoch 404/1500\n",
      "514/514 [==============================] - 0s 30us/step - loss: 0.0121 - val_loss: 0.0106\n",
      "Epoch 405/1500\n",
      "514/514 [==============================] - 0s 31us/step - loss: 0.0119 - val_loss: 0.0102\n",
      "Epoch 406/1500\n",
      "514/514 [==============================] - 0s 31us/step - loss: 0.0114 - val_loss: 0.0110\n",
      "Epoch 407/1500\n",
      "514/514 [==============================] - 0s 57us/step - loss: 0.0125 - val_loss: 0.0102\n",
      "Epoch 408/1500\n",
      "514/514 [==============================] - 0s 62us/step - loss: 0.0113 - val_loss: 0.0100\n",
      "Epoch 409/1500\n",
      "514/514 [==============================] - 0s 31us/step - loss: 0.0124 - val_loss: 0.0102\n",
      "Epoch 410/1500\n",
      "514/514 [==============================] - 0s 31us/step - loss: 0.0115 - val_loss: 0.0092\n",
      "Epoch 411/1500\n",
      "514/514 [==============================] - ETA: 0s - loss: 0.011 - 0s 32us/step - loss: 0.0115 - val_loss: 0.0162\n",
      "Epoch 412/1500\n",
      "514/514 [==============================] - 0s 33us/step - loss: 0.0163 - val_loss: 0.0124\n",
      "Epoch 413/1500\n",
      "514/514 [==============================] - 0s 32us/step - loss: 0.0117 - val_loss: 0.0085\n",
      "Epoch 414/1500\n",
      "514/514 [==============================] - 0s 30us/step - loss: 0.0117 - val_loss: 0.0100\n",
      "Epoch 415/1500\n",
      "514/514 [==============================] - 0s 31us/step - loss: 0.0143 - val_loss: 0.0113\n",
      "Epoch 416/1500\n",
      "514/514 [==============================] - 0s 32us/step - loss: 0.0148 - val_loss: 0.0131\n",
      "Epoch 417/1500\n",
      "514/514 [==============================] - 0s 31us/step - loss: 0.0162 - val_loss: 0.0092\n",
      "Epoch 418/1500\n",
      "514/514 [==============================] - 0s 31us/step - loss: 0.0123 - val_loss: 0.0129\n",
      "Epoch 419/1500\n",
      "514/514 [==============================] - 0s 31us/step - loss: 0.0144 - val_loss: 0.0122\n",
      "Epoch 420/1500\n",
      "514/514 [==============================] - 0s 31us/step - loss: 0.0131 - val_loss: 0.0156\n",
      "Epoch 421/1500\n",
      "514/514 [==============================] - 0s 32us/step - loss: 0.0170 - val_loss: 0.0141\n",
      "Epoch 422/1500\n",
      "514/514 [==============================] - 0s 31us/step - loss: 0.0169 - val_loss: 0.0131\n",
      "Epoch 423/1500\n",
      "514/514 [==============================] - 0s 31us/step - loss: 0.0163 - val_loss: 0.0166\n",
      "Epoch 424/1500\n",
      "514/514 [==============================] - 0s 32us/step - loss: 0.0204 - val_loss: 0.0127\n",
      "Epoch 425/1500\n",
      "514/514 [==============================] - 0s 33us/step - loss: 0.0158 - val_loss: 0.0166\n",
      "Epoch 426/1500\n",
      "514/514 [==============================] - 0s 33us/step - loss: 0.0195 - val_loss: 0.0094\n",
      "Epoch 427/1500\n",
      "514/514 [==============================] - 0s 31us/step - loss: 0.0119 - val_loss: 0.0208\n",
      "Epoch 428/1500\n",
      "514/514 [==============================] - 0s 31us/step - loss: 0.0241 - val_loss: 0.0136\n",
      "Epoch 429/1500\n",
      "514/514 [==============================] - 0s 31us/step - loss: 0.0133 - val_loss: 0.0185\n",
      "Epoch 430/1500\n",
      "514/514 [==============================] - 0s 35us/step - loss: 0.0226 - val_loss: 0.0184\n",
      "Epoch 431/1500\n",
      "514/514 [==============================] - 0s 73us/step - loss: 0.0180 - val_loss: 0.0091\n",
      "Epoch 432/1500\n",
      "514/514 [==============================] - 0s 44us/step - loss: 0.0125 - val_loss: 0.0090\n",
      "Epoch 433/1500\n",
      "514/514 [==============================] - 0s 35us/step - loss: 0.0121 - val_loss: 0.0112\n",
      "Epoch 434/1500\n",
      "514/514 [==============================] - 0s 33us/step - loss: 0.0126 - val_loss: 0.0095\n",
      "Epoch 435/1500\n",
      "514/514 [==============================] - 0s 35us/step - loss: 0.0120 - val_loss: 0.0112\n",
      "Epoch 436/1500\n",
      "514/514 [==============================] - 0s 34us/step - loss: 0.0140 - val_loss: 0.0091\n",
      "Epoch 437/1500\n",
      "514/514 [==============================] - 0s 31us/step - loss: 0.0121 - val_loss: 0.0094\n",
      "Epoch 438/1500\n",
      "514/514 [==============================] - 0s 31us/step - loss: 0.0127 - val_loss: 0.0087\n",
      "Epoch 439/1500\n",
      "514/514 [==============================] - 0s 31us/step - loss: 0.0117 - val_loss: 0.0085\n",
      "Epoch 440/1500\n",
      "514/514 [==============================] - 0s 34us/step - loss: 0.0108 - val_loss: 0.0095\n",
      "Epoch 441/1500\n",
      "514/514 [==============================] - 0s 40us/step - loss: 0.0113 - val_loss: 0.0118\n",
      "Epoch 442/1500\n",
      "514/514 [==============================] - 0s 31us/step - loss: 0.0126 - val_loss: 0.0111\n",
      "Epoch 443/1500\n",
      "514/514 [==============================] - 0s 33us/step - loss: 0.0125 - val_loss: 0.0103\n",
      "Epoch 444/1500\n",
      "514/514 [==============================] - 0s 33us/step - loss: 0.0126 - val_loss: 0.0097\n",
      "Epoch 445/1500\n",
      "514/514 [==============================] - 0s 32us/step - loss: 0.0141 - val_loss: 0.0179\n",
      "Epoch 446/1500\n",
      "514/514 [==============================] - 0s 35us/step - loss: 0.0193 - val_loss: 0.0150\n",
      "Epoch 447/1500\n",
      "514/514 [==============================] - 0s 35us/step - loss: 0.0154 - val_loss: 0.0124\n",
      "Epoch 448/1500\n",
      "514/514 [==============================] - 0s 33us/step - loss: 0.0137 - val_loss: 0.0129\n",
      "Epoch 449/1500\n",
      "514/514 [==============================] - 0s 35us/step - loss: 0.0148 - val_loss: 0.0179\n",
      "Epoch 450/1500\n",
      "514/514 [==============================] - 0s 32us/step - loss: 0.0184 - val_loss: 0.0209\n",
      "Epoch 451/1500\n",
      "514/514 [==============================] - 0s 36us/step - loss: 0.0204 - val_loss: 0.0178\n",
      "Epoch 452/1500\n",
      "514/514 [==============================] - 0s 36us/step - loss: 0.0169 - val_loss: 0.0131\n",
      "Epoch 453/1500\n",
      "514/514 [==============================] - 0s 31us/step - loss: 0.0136 - val_loss: 0.0109\n",
      "Epoch 454/1500\n",
      "514/514 [==============================] - 0s 33us/step - loss: 0.0121 - val_loss: 0.0097\n",
      "Epoch 455/1500\n",
      "514/514 [==============================] - 0s 33us/step - loss: 0.0125 - val_loss: 0.0098\n",
      "Epoch 456/1500\n",
      "514/514 [==============================] - 0s 33us/step - loss: 0.0118 - val_loss: 0.0107\n",
      "Epoch 457/1500\n",
      "514/514 [==============================] - 0s 35us/step - loss: 0.0135 - val_loss: 0.0110\n",
      "Epoch 458/1500\n",
      "514/514 [==============================] - 0s 35us/step - loss: 0.0118 - val_loss: 0.0128\n",
      "Epoch 459/1500\n",
      "514/514 [==============================] - 0s 49us/step - loss: 0.0142 - val_loss: 0.0092\n",
      "Epoch 460/1500\n",
      "514/514 [==============================] - 0s 39us/step - loss: 0.0113 - val_loss: 0.0150\n",
      "Epoch 461/1500\n",
      "514/514 [==============================] - 0s 47us/step - loss: 0.0159 - val_loss: 0.0180\n",
      "Epoch 462/1500\n",
      "514/514 [==============================] - 0s 42us/step - loss: 0.0186 - val_loss: 0.0192\n",
      "Epoch 463/1500\n",
      "514/514 [==============================] - 0s 48us/step - loss: 0.0188 - val_loss: 0.0173\n",
      "Epoch 464/1500\n",
      "514/514 [==============================] - 0s 45us/step - loss: 0.0166 - val_loss: 0.0145\n",
      "Epoch 465/1500\n",
      "514/514 [==============================] - 0s 50us/step - loss: 0.0143 - val_loss: 0.0132\n",
      "Epoch 466/1500\n",
      "514/514 [==============================] - 0s 69us/step - loss: 0.0137 - val_loss: 0.0105\n",
      "Epoch 467/1500\n",
      "514/514 [==============================] - 0s 103us/step - loss: 0.0119 - val_loss: 0.0090\n",
      "Epoch 468/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "514/514 [==============================] - 0s 117us/step - loss: 0.0113 - val_loss: 0.0120\n",
      "Epoch 469/1500\n",
      "514/514 [==============================] - 0s 63us/step - loss: 0.0137 - val_loss: 0.0101\n",
      "Epoch 470/1500\n",
      "514/514 [==============================] - 0s 62us/step - loss: 0.0118 - val_loss: 0.0103\n",
      "Epoch 471/1500\n",
      "514/514 [==============================] - 0s 59us/step - loss: 0.0121 - val_loss: 0.0088\n",
      "Epoch 472/1500\n",
      "514/514 [==============================] - 0s 56us/step - loss: 0.0110 - val_loss: 0.0084\n",
      "Epoch 473/1500\n",
      "514/514 [==============================] - 0s 59us/step - loss: 0.0108 - val_loss: 0.0087\n",
      "Epoch 474/1500\n",
      "514/514 [==============================] - ETA: 0s - loss: 0.011 - 0s 74us/step - loss: 0.0111 - val_loss: 0.0094\n",
      "Epoch 475/1500\n",
      "514/514 [==============================] - 0s 62us/step - loss: 0.0116 - val_loss: 0.0088\n",
      "Epoch 476/1500\n",
      "514/514 [==============================] - 0s 73us/step - loss: 0.0109 - val_loss: 0.0095\n",
      "Epoch 477/1500\n",
      "514/514 [==============================] - 0s 55us/step - loss: 0.0117 - val_loss: 0.0095\n",
      "Epoch 478/1500\n",
      "514/514 [==============================] - 0s 64us/step - loss: 0.0112 - val_loss: 0.0096\n",
      "Epoch 479/1500\n",
      "514/514 [==============================] - 0s 65us/step - loss: 0.0110 - val_loss: 0.0099\n",
      "Epoch 480/1500\n",
      "514/514 [==============================] - 0s 58us/step - loss: 0.0110 - val_loss: 0.0109\n",
      "Epoch 481/1500\n",
      "514/514 [==============================] - 0s 58us/step - loss: 0.0126 - val_loss: 0.0141\n",
      "Epoch 482/1500\n",
      "514/514 [==============================] - 0s 66us/step - loss: 0.0162 - val_loss: 0.0089\n",
      "Epoch 483/1500\n",
      "514/514 [==============================] - 0s 51us/step - loss: 0.0130 - val_loss: 0.0332\n",
      "Epoch 484/1500\n",
      "514/514 [==============================] - 0s 47us/step - loss: 0.0330 - val_loss: 0.0174\n",
      "Epoch 485/1500\n",
      "514/514 [==============================] - 0s 46us/step - loss: 0.0160 - val_loss: 0.0184\n",
      "Epoch 486/1500\n",
      "514/514 [==============================] - 0s 44us/step - loss: 0.0200 - val_loss: 0.0100\n",
      "Epoch 487/1500\n",
      "514/514 [==============================] - 0s 48us/step - loss: 0.0117 - val_loss: 0.0140\n",
      "Epoch 488/1500\n",
      "514/514 [==============================] - 0s 60us/step - loss: 0.0177 - val_loss: 0.0091\n",
      "Epoch 489/1500\n",
      "514/514 [==============================] - 0s 48us/step - loss: 0.0115 - val_loss: 0.0150\n",
      "Epoch 490/1500\n",
      "514/514 [==============================] - 0s 57us/step - loss: 0.0172 - val_loss: 0.0091\n",
      "Epoch 491/1500\n",
      "514/514 [==============================] - 0s 44us/step - loss: 0.0119 - val_loss: 0.0115\n",
      "Epoch 492/1500\n",
      "514/514 [==============================] - 0s 51us/step - loss: 0.0126 - val_loss: 0.0119\n",
      "Epoch 493/1500\n",
      "514/514 [==============================] - 0s 45us/step - loss: 0.0154 - val_loss: 0.0159\n",
      "Epoch 494/1500\n",
      "514/514 [==============================] - 0s 64us/step - loss: 0.0160 - val_loss: 0.0095\n",
      "Epoch 495/1500\n",
      "514/514 [==============================] - 0s 65us/step - loss: 0.0116 - val_loss: 0.0177\n",
      "Epoch 496/1500\n",
      "514/514 [==============================] - 0s 58us/step - loss: 0.0208 - val_loss: 0.0165\n",
      "Epoch 497/1500\n",
      "514/514 [==============================] - 0s 62us/step - loss: 0.0163 - val_loss: 0.0122\n",
      "Epoch 498/1500\n",
      "514/514 [==============================] - 0s 60us/step - loss: 0.0164 - val_loss: 0.0137\n",
      "Epoch 499/1500\n",
      "514/514 [==============================] - 0s 58us/step - loss: 0.0145 - val_loss: 0.0152\n",
      "Epoch 500/1500\n",
      "514/514 [==============================] - 0s 52us/step - loss: 0.0194 - val_loss: 0.0112\n",
      "Epoch 501/1500\n",
      "514/514 [==============================] - 0s 48us/step - loss: 0.0134 - val_loss: 0.0210\n",
      "Epoch 502/1500\n",
      "514/514 [==============================] - 0s 55us/step - loss: 0.0225 - val_loss: 0.0103\n",
      "Epoch 503/1500\n",
      "514/514 [==============================] - 0s 49us/step - loss: 0.0169 - val_loss: 0.0109\n",
      "Epoch 504/1500\n",
      "514/514 [==============================] - 0s 51us/step - loss: 0.0118 - val_loss: 0.0186\n",
      "Epoch 505/1500\n",
      "514/514 [==============================] - 0s 49us/step - loss: 0.0204 - val_loss: 0.0093\n",
      "Epoch 506/1500\n",
      "514/514 [==============================] - 0s 50us/step - loss: 0.0140 - val_loss: 0.0103\n",
      "Epoch 507/1500\n",
      "514/514 [==============================] - 0s 41us/step - loss: 0.0131 - val_loss: 0.0140\n",
      "Epoch 508/1500\n",
      "514/514 [==============================] - 0s 41us/step - loss: 0.0159 - val_loss: 0.0090\n",
      "Epoch 509/1500\n",
      "514/514 [==============================] - 0s 39us/step - loss: 0.0116 - val_loss: 0.0121\n",
      "Epoch 510/1500\n",
      "514/514 [==============================] - 0s 50us/step - loss: 0.0132 - val_loss: 0.0091\n",
      "Epoch 511/1500\n",
      "514/514 [==============================] - 0s 57us/step - loss: 0.0107 - val_loss: 0.0095\n",
      "Epoch 512/1500\n",
      "514/514 [==============================] - 0s 54us/step - loss: 0.0110 - val_loss: 0.0096\n",
      "Epoch 513/1500\n",
      "514/514 [==============================] - 0s 51us/step - loss: 0.0117 - val_loss: 0.0106\n",
      "Epoch 514/1500\n",
      "514/514 [==============================] - 0s 51us/step - loss: 0.0118 - val_loss: 0.0084\n",
      "Epoch 515/1500\n",
      "514/514 [==============================] - 0s 52us/step - loss: 0.0115 - val_loss: 0.0106\n",
      "Epoch 516/1500\n",
      "514/514 [==============================] - 0s 56us/step - loss: 0.0143 - val_loss: 0.0100\n",
      "Epoch 517/1500\n",
      "514/514 [==============================] - 0s 50us/step - loss: 0.0127 - val_loss: 0.0095\n",
      "Epoch 518/1500\n",
      "514/514 [==============================] - 0s 42us/step - loss: 0.0131 - val_loss: 0.0111\n",
      "Epoch 519/1500\n",
      "514/514 [==============================] - 0s 44us/step - loss: 0.0130 - val_loss: 0.0086\n",
      "Epoch 520/1500\n",
      "514/514 [==============================] - 0s 53us/step - loss: 0.0108 - val_loss: 0.0085\n",
      "Epoch 521/1500\n",
      "514/514 [==============================] - 0s 51us/step - loss: 0.0113 - val_loss: 0.0096\n",
      "Epoch 522/1500\n",
      "514/514 [==============================] - 0s 50us/step - loss: 0.0134 - val_loss: 0.0109\n",
      "Epoch 523/1500\n",
      "514/514 [==============================] - 0s 51us/step - loss: 0.0146 - val_loss: 0.0109\n",
      "Epoch 524/1500\n",
      "514/514 [==============================] - 0s 44us/step - loss: 0.0143 - val_loss: 0.0099\n",
      "Epoch 525/1500\n",
      "514/514 [==============================] - 0s 42us/step - loss: 0.0130 - val_loss: 0.0093\n",
      "Epoch 526/1500\n",
      "514/514 [==============================] - 0s 46us/step - loss: 0.0119 - val_loss: 0.0130\n",
      "Epoch 527/1500\n",
      "514/514 [==============================] - 0s 46us/step - loss: 0.0137 - val_loss: 0.0151\n",
      "Epoch 528/1500\n",
      "514/514 [==============================] - 0s 44us/step - loss: 0.0145 - val_loss: 0.0130\n",
      "Epoch 529/1500\n",
      "514/514 [==============================] - 0s 49us/step - loss: 0.0130 - val_loss: 0.0114\n",
      "Epoch 530/1500\n",
      "514/514 [==============================] - 0s 50us/step - loss: 0.0125 - val_loss: 0.0121\n",
      "Epoch 531/1500\n",
      "514/514 [==============================] - 0s 54us/step - loss: 0.0130 - val_loss: 0.0119\n",
      "Epoch 532/1500\n",
      "514/514 [==============================] - 0s 52us/step - loss: 0.0132 - val_loss: 0.0125\n",
      "Epoch 533/1500\n",
      "514/514 [==============================] - 0s 49us/step - loss: 0.0145 - val_loss: 0.0108\n",
      "Epoch 534/1500\n",
      "514/514 [==============================] - 0s 53us/step - loss: 0.0119 - val_loss: 0.0094\n",
      "Epoch 535/1500\n",
      "514/514 [==============================] - 0s 47us/step - loss: 0.0115 - val_loss: 0.0095\n",
      "Epoch 536/1500\n",
      "514/514 [==============================] - 0s 51us/step - loss: 0.0130 - val_loss: 0.0108\n",
      "Epoch 537/1500\n",
      "514/514 [==============================] - 0s 51us/step - loss: 0.0135 - val_loss: 0.0110\n",
      "Epoch 538/1500\n",
      "514/514 [==============================] - 0s 45us/step - loss: 0.0147 - val_loss: 0.0103\n",
      "Epoch 539/1500\n",
      "514/514 [==============================] - 0s 47us/step - loss: 0.0127 - val_loss: 0.0132\n",
      "Epoch 540/1500\n",
      "514/514 [==============================] - 0s 54us/step - loss: 0.0164 - val_loss: 0.0124\n",
      "Epoch 541/1500\n",
      "514/514 [==============================] - 0s 48us/step - loss: 0.0137 - val_loss: 0.0108\n",
      "Epoch 542/1500\n",
      "514/514 [==============================] - 0s 55us/step - loss: 0.0141 - val_loss: 0.0187\n",
      "Epoch 543/1500\n",
      "514/514 [==============================] - 0s 59us/step - loss: 0.0188 - val_loss: 0.0123\n",
      "Epoch 544/1500\n",
      "514/514 [==============================] - 0s 61us/step - loss: 0.0142 - val_loss: 0.0157\n",
      "Epoch 545/1500\n",
      "514/514 [==============================] - 0s 56us/step - loss: 0.0172 - val_loss: 0.0119\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 546/1500\n",
      "514/514 [==============================] - 0s 59us/step - loss: 0.0132 - val_loss: 0.0103\n",
      "Epoch 547/1500\n",
      "514/514 [==============================] - 0s 61us/step - loss: 0.0118 - val_loss: 0.0089\n",
      "Epoch 548/1500\n",
      "514/514 [==============================] - 0s 57us/step - loss: 0.0109 - val_loss: 0.0105\n",
      "Epoch 549/1500\n",
      "514/514 [==============================] - 0s 54us/step - loss: 0.0118 - val_loss: 0.0106\n",
      "Epoch 550/1500\n",
      "514/514 [==============================] - 0s 48us/step - loss: 0.0117 - val_loss: 0.0091\n",
      "Epoch 551/1500\n",
      "514/514 [==============================] - 0s 47us/step - loss: 0.0107 - val_loss: 0.0085\n",
      "Epoch 552/1500\n",
      "514/514 [==============================] - 0s 34us/step - loss: 0.0107 - val_loss: 0.0085\n",
      "Epoch 553/1500\n",
      "514/514 [==============================] - 0s 46us/step - loss: 0.0111 - val_loss: 0.0096\n",
      "Epoch 554/1500\n",
      "514/514 [==============================] - 0s 47us/step - loss: 0.0119 - val_loss: 0.0086\n",
      "Epoch 555/1500\n",
      "514/514 [==============================] - 0s 40us/step - loss: 0.0108 - val_loss: 0.0087\n",
      "Epoch 556/1500\n",
      "514/514 [==============================] - 0s 35us/step - loss: 0.0110 - val_loss: 0.0090\n",
      "Epoch 557/1500\n",
      "514/514 [==============================] - 0s 50us/step - loss: 0.0112 - val_loss: 0.0088\n",
      "Epoch 558/1500\n",
      "514/514 [==============================] - 0s 35us/step - loss: 0.0111 - val_loss: 0.0083\n",
      "Epoch 559/1500\n",
      "514/514 [==============================] - 0s 46us/step - loss: 0.0113 - val_loss: 0.0090\n",
      "Epoch 560/1500\n",
      "514/514 [==============================] - 0s 36us/step - loss: 0.0125 - val_loss: 0.0110\n",
      "Epoch 561/1500\n",
      "514/514 [==============================] - 0s 39us/step - loss: 0.0138 - val_loss: 0.0101\n",
      "Epoch 562/1500\n",
      "514/514 [==============================] - 0s 56us/step - loss: 0.0134 - val_loss: 0.0090\n",
      "Epoch 563/1500\n",
      "514/514 [==============================] - 0s 82us/step - loss: 0.0116 - val_loss: 0.0119\n",
      "Epoch 564/1500\n",
      "514/514 [==============================] - 0s 59us/step - loss: 0.0158 - val_loss: 0.0154\n",
      "Epoch 565/1500\n",
      "514/514 [==============================] - 0s 50us/step - loss: 0.0167 - val_loss: 0.0096\n",
      "Epoch 566/1500\n",
      "514/514 [==============================] - 0s 56us/step - loss: 0.0118 - val_loss: 0.0150\n",
      "Epoch 567/1500\n",
      "514/514 [==============================] - 0s 47us/step - loss: 0.0163 - val_loss: 0.0097\n",
      "Epoch 568/1500\n",
      "514/514 [==============================] - 0s 43us/step - loss: 0.0122 - val_loss: 0.0130\n",
      "Epoch 569/1500\n",
      "514/514 [==============================] - 0s 51us/step - loss: 0.0152 - val_loss: 0.0093\n",
      "Epoch 570/1500\n",
      "514/514 [==============================] - 0s 48us/step - loss: 0.0113 - val_loss: 0.0096\n",
      "Epoch 571/1500\n",
      "514/514 [==============================] - 0s 51us/step - loss: 0.0120 - val_loss: 0.0107\n",
      "Epoch 572/1500\n",
      "514/514 [==============================] - 0s 49us/step - loss: 0.0123 - val_loss: 0.0100\n",
      "Epoch 573/1500\n",
      "514/514 [==============================] - 0s 52us/step - loss: 0.0142 - val_loss: 0.0156\n",
      "Epoch 574/1500\n",
      "514/514 [==============================] - 0s 46us/step - loss: 0.0163 - val_loss: 0.0114\n",
      "Epoch 575/1500\n",
      "514/514 [==============================] - 0s 45us/step - loss: 0.0137 - val_loss: 0.0227\n",
      "Epoch 576/1500\n",
      "514/514 [==============================] - 0s 49us/step - loss: 0.0225 - val_loss: 0.0131\n",
      "Epoch 577/1500\n",
      "514/514 [==============================] - 0s 55us/step - loss: 0.0142 - val_loss: 0.0189\n",
      "Epoch 578/1500\n",
      "514/514 [==============================] - 0s 72us/step - loss: 0.0209 - val_loss: 0.0115\n",
      "Epoch 579/1500\n",
      "514/514 [==============================] - 0s 58us/step - loss: 0.0130 - val_loss: 0.0112\n",
      "Epoch 580/1500\n",
      "514/514 [==============================] - 0s 56us/step - loss: 0.0137 - val_loss: 0.0105\n",
      "Epoch 581/1500\n",
      "514/514 [==============================] - 0s 57us/step - loss: 0.0128 - val_loss: 0.0094\n",
      "Epoch 582/1500\n",
      "514/514 [==============================] - 0s 47us/step - loss: 0.0112 - val_loss: 0.0101\n",
      "Epoch 583/1500\n",
      "514/514 [==============================] - 0s 49us/step - loss: 0.0111 - val_loss: 0.0098\n",
      "Epoch 584/1500\n",
      "514/514 [==============================] - 0s 51us/step - loss: 0.0111 - val_loss: 0.0088\n",
      "Epoch 585/1500\n",
      "514/514 [==============================] - 0s 54us/step - loss: 0.0112 - val_loss: 0.0115\n",
      "Epoch 586/1500\n",
      "514/514 [==============================] - 0s 47us/step - loss: 0.0143 - val_loss: 0.0108\n",
      "Epoch 587/1500\n",
      "514/514 [==============================] - 0s 51us/step - loss: 0.0137 - val_loss: 0.0099\n",
      "Epoch 588/1500\n",
      "514/514 [==============================] - 0s 53us/step - loss: 0.0122 - val_loss: 0.0097\n",
      "Epoch 589/1500\n",
      "514/514 [==============================] - 0s 46us/step - loss: 0.0115 - val_loss: 0.0125\n",
      "Epoch 590/1500\n",
      "514/514 [==============================] - 0s 46us/step - loss: 0.0129 - val_loss: 0.0142\n",
      "Epoch 591/1500\n",
      "514/514 [==============================] - 0s 48us/step - loss: 0.0136 - val_loss: 0.0126\n",
      "Epoch 592/1500\n",
      "514/514 [==============================] - ETA: 0s - loss: 0.012 - 0s 46us/step - loss: 0.0127 - val_loss: 0.0103\n",
      "Epoch 593/1500\n",
      "514/514 [==============================] - 0s 44us/step - loss: 0.0114 - val_loss: 0.0091\n",
      "Epoch 594/1500\n",
      "514/514 [==============================] - 0s 51us/step - loss: 0.0112 - val_loss: 0.0091\n",
      "Epoch 595/1500\n",
      "514/514 [==============================] - 0s 63us/step - loss: 0.0114 - val_loss: 0.0095\n",
      "Epoch 596/1500\n",
      "514/514 [==============================] - 0s 87us/step - loss: 0.0115 - val_loss: 0.0188\n",
      "Epoch 597/1500\n",
      "514/514 [==============================] - 0s 84us/step - loss: 0.0195 - val_loss: 0.0205\n",
      "Epoch 598/1500\n",
      "514/514 [==============================] - 0s 74us/step - loss: 0.0187 - val_loss: 0.0117\n",
      "Epoch 599/1500\n",
      "514/514 [==============================] - ETA: 0s - loss: 0.012 - 0s 57us/step - loss: 0.0122 - val_loss: 0.0109\n",
      "Epoch 600/1500\n",
      "514/514 [==============================] - 0s 56us/step - loss: 0.0127 - val_loss: 0.0094\n",
      "Epoch 601/1500\n",
      "514/514 [==============================] - 0s 50us/step - loss: 0.0124 - val_loss: 0.0106\n",
      "Epoch 602/1500\n",
      "514/514 [==============================] - 0s 53us/step - loss: 0.0148 - val_loss: 0.0131\n",
      "Epoch 603/1500\n",
      "514/514 [==============================] - 0s 61us/step - loss: 0.0146 - val_loss: 0.0089\n",
      "Epoch 604/1500\n",
      "514/514 [==============================] - 0s 85us/step - loss: 0.0112 - val_loss: 0.0145\n",
      "Epoch 605/1500\n",
      "514/514 [==============================] - 0s 53us/step - loss: 0.0192 - val_loss: 0.0159\n",
      "Epoch 606/1500\n",
      "514/514 [==============================] - 0s 53us/step - loss: 0.0150 - val_loss: 0.0098\n",
      "Epoch 607/1500\n",
      "514/514 [==============================] - 0s 57us/step - loss: 0.0113 - val_loss: 0.0093\n",
      "Epoch 608/1500\n",
      "514/514 [==============================] - 0s 59us/step - loss: 0.0108 - val_loss: 0.0093\n",
      "Epoch 609/1500\n",
      "514/514 [==============================] - ETA: 0s - loss: 0.012 - 0s 53us/step - loss: 0.0109 - val_loss: 0.0097\n",
      "Epoch 610/1500\n",
      "514/514 [==============================] - 0s 52us/step - loss: 0.0117 - val_loss: 0.0102\n",
      "Epoch 611/1500\n",
      "514/514 [==============================] - 0s 55us/step - loss: 0.0111 - val_loss: 0.0092\n",
      "Epoch 612/1500\n",
      "514/514 [==============================] - 0s 58us/step - loss: 0.0109 - val_loss: 0.0091\n",
      "Epoch 613/1500\n",
      "514/514 [==============================] - 0s 51us/step - loss: 0.0115 - val_loss: 0.0102\n",
      "Epoch 614/1500\n",
      "514/514 [==============================] - 0s 75us/step - loss: 0.0118 - val_loss: 0.0100\n",
      "Epoch 615/1500\n",
      "514/514 [==============================] - 0s 64us/step - loss: 0.0119 - val_loss: 0.0104\n",
      "Epoch 616/1500\n",
      "514/514 [==============================] - 0s 63us/step - loss: 0.0112 - val_loss: 0.0109\n",
      "Epoch 617/1500\n",
      "514/514 [==============================] - 0s 55us/step - loss: 0.0133 - val_loss: 0.0152\n",
      "Epoch 618/1500\n",
      "514/514 [==============================] - 0s 66us/step - loss: 0.0153 - val_loss: 0.0103\n",
      "Epoch 619/1500\n",
      "514/514 [==============================] - 0s 61us/step - loss: 0.0122 - val_loss: 0.0179\n",
      "Epoch 620/1500\n",
      "514/514 [==============================] - 0s 55us/step - loss: 0.0199 - val_loss: 0.0114\n",
      "Epoch 621/1500\n",
      "514/514 [==============================] - 0s 57us/step - loss: 0.0124 - val_loss: 0.0109\n",
      "Epoch 622/1500\n",
      "514/514 [==============================] - 0s 49us/step - loss: 0.0142 - val_loss: 0.0122\n",
      "Epoch 623/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "514/514 [==============================] - 0s 49us/step - loss: 0.0137 - val_loss: 0.0098\n",
      "Epoch 624/1500\n",
      "514/514 [==============================] - 0s 51us/step - loss: 0.0119 - val_loss: 0.0192\n",
      "Epoch 625/1500\n",
      "514/514 [==============================] - 0s 52us/step - loss: 0.0201 - val_loss: 0.0183\n",
      "Epoch 626/1500\n",
      "514/514 [==============================] - 0s 46us/step - loss: 0.0188 - val_loss: 0.0229\n",
      "Epoch 627/1500\n",
      "514/514 [==============================] - 0s 55us/step - loss: 0.0221 - val_loss: 0.0172\n",
      "Epoch 628/1500\n",
      "514/514 [==============================] - 0s 54us/step - loss: 0.0174 - val_loss: 0.0210\n",
      "Epoch 629/1500\n",
      "514/514 [==============================] - 0s 62us/step - loss: 0.0214 - val_loss: 0.0124\n",
      "Epoch 630/1500\n",
      "514/514 [==============================] - 0s 54us/step - loss: 0.0152 - val_loss: 0.0177\n",
      "Epoch 631/1500\n",
      "514/514 [==============================] - 0s 51us/step - loss: 0.0201 - val_loss: 0.0097\n",
      "Epoch 632/1500\n",
      "514/514 [==============================] - 0s 42us/step - loss: 0.0136 - val_loss: 0.0115\n",
      "Epoch 633/1500\n",
      "514/514 [==============================] - 0s 38us/step - loss: 0.0137 - val_loss: 0.0129\n",
      "Epoch 634/1500\n",
      "514/514 [==============================] - 0s 49us/step - loss: 0.0160 - val_loss: 0.0125\n",
      "Epoch 635/1500\n",
      "514/514 [==============================] - 0s 40us/step - loss: 0.0137 - val_loss: 0.0087\n",
      "Epoch 636/1500\n",
      "514/514 [==============================] - 0s 39us/step - loss: 0.0115 - val_loss: 0.0111\n",
      "Epoch 637/1500\n",
      "514/514 [==============================] - 0s 36us/step - loss: 0.0143 - val_loss: 0.0137\n",
      "Epoch 638/1500\n",
      "514/514 [==============================] - 0s 33us/step - loss: 0.0165 - val_loss: 0.0138\n",
      "Epoch 639/1500\n",
      "514/514 [==============================] - 0s 35us/step - loss: 0.0162 - val_loss: 0.0121\n",
      "Epoch 640/1500\n",
      "514/514 [==============================] - 0s 42us/step - loss: 0.0145 - val_loss: 0.0103\n",
      "Epoch 641/1500\n",
      "514/514 [==============================] - 0s 41us/step - loss: 0.0135 - val_loss: 0.0128\n",
      "Epoch 642/1500\n",
      "514/514 [==============================] - 0s 38us/step - loss: 0.0145 - val_loss: 0.0117\n",
      "Epoch 643/1500\n",
      "514/514 [==============================] - 0s 36us/step - loss: 0.0131 - val_loss: 0.0135\n",
      "Epoch 644/1500\n",
      "514/514 [==============================] - 0s 40us/step - loss: 0.0143 - val_loss: 0.0119\n",
      "Epoch 645/1500\n",
      "514/514 [==============================] - 0s 46us/step - loss: 0.0129 - val_loss: 0.0110\n",
      "Epoch 646/1500\n",
      "514/514 [==============================] - 0s 50us/step - loss: 0.0136 - val_loss: 0.0110\n",
      "Epoch 647/1500\n",
      "514/514 [==============================] - 0s 53us/step - loss: 0.0130 - val_loss: 0.0098\n",
      "Epoch 648/1500\n",
      "514/514 [==============================] - 0s 60us/step - loss: 0.0127 - val_loss: 0.0127\n",
      "Epoch 649/1500\n",
      "514/514 [==============================] - 0s 62us/step - loss: 0.0146 - val_loss: 0.0090\n",
      "Epoch 650/1500\n",
      "514/514 [==============================] - 0s 67us/step - loss: 0.0109 - val_loss: 0.0110\n",
      "Epoch 651/1500\n",
      "514/514 [==============================] - 0s 56us/step - loss: 0.0126 - val_loss: 0.0103\n",
      "Epoch 652/1500\n",
      "514/514 [==============================] - 0s 59us/step - loss: 0.0114 - val_loss: 0.0091\n",
      "Epoch 653/1500\n",
      "514/514 [==============================] - 0s 59us/step - loss: 0.0109 - val_loss: 0.0093\n",
      "Epoch 654/1500\n",
      "514/514 [==============================] - 0s 56us/step - loss: 0.0111 - val_loss: 0.0086\n",
      "Epoch 655/1500\n",
      "514/514 [==============================] - 0s 59us/step - loss: 0.0107 - val_loss: 0.0091\n",
      "Epoch 656/1500\n",
      "514/514 [==============================] - 0s 69us/step - loss: 0.0113 - val_loss: 0.0095\n",
      "Epoch 657/1500\n",
      "514/514 [==============================] - 0s 61us/step - loss: 0.0112 - val_loss: 0.0092\n",
      "Epoch 658/1500\n",
      "514/514 [==============================] - 0s 59us/step - loss: 0.0108 - val_loss: 0.0097\n",
      "Epoch 659/1500\n",
      "514/514 [==============================] - 0s 58us/step - loss: 0.0110 - val_loss: 0.0099\n",
      "Epoch 660/1500\n",
      "514/514 [==============================] - 0s 61us/step - loss: 0.0111 - val_loss: 0.0095\n",
      "Epoch 661/1500\n",
      "514/514 [==============================] - 0s 52us/step - loss: 0.0113 - val_loss: 0.0113\n",
      "Epoch 662/1500\n",
      "514/514 [==============================] - 0s 48us/step - loss: 0.0125 - val_loss: 0.0108\n",
      "Epoch 663/1500\n",
      "514/514 [==============================] - 0s 48us/step - loss: 0.0124 - val_loss: 0.0112\n",
      "Epoch 664/1500\n",
      "514/514 [==============================] - 0s 48us/step - loss: 0.0119 - val_loss: 0.0119\n",
      "Epoch 665/1500\n",
      "514/514 [==============================] - 0s 48us/step - loss: 0.0140 - val_loss: 0.0132\n",
      "Epoch 666/1500\n",
      "514/514 [==============================] - 0s 55us/step - loss: 0.0135 - val_loss: 0.0099\n",
      "Epoch 667/1500\n",
      "514/514 [==============================] - 0s 55us/step - loss: 0.0108 - val_loss: 0.0093\n",
      "Epoch 668/1500\n",
      "514/514 [==============================] - 0s 67us/step - loss: 0.0109 - val_loss: 0.0087\n",
      "Epoch 669/1500\n",
      "514/514 [==============================] - 0s 58us/step - loss: 0.0118 - val_loss: 0.0094\n",
      "Epoch 670/1500\n",
      "514/514 [==============================] - 0s 62us/step - loss: 0.0128 - val_loss: 0.0117\n",
      "Epoch 671/1500\n",
      "514/514 [==============================] - 0s 59us/step - loss: 0.0163 - val_loss: 0.0123\n",
      "Epoch 672/1500\n",
      "514/514 [==============================] - 0s 50us/step - loss: 0.0140 - val_loss: 0.0097\n",
      "Epoch 673/1500\n",
      "514/514 [==============================] - 0s 52us/step - loss: 0.0137 - val_loss: 0.0212\n",
      "Epoch 674/1500\n",
      "514/514 [==============================] - 0s 51us/step - loss: 0.0218 - val_loss: 0.0104\n",
      "Epoch 675/1500\n",
      "514/514 [==============================] - 0s 39us/step - loss: 0.0115 - val_loss: 0.0203\n",
      "Epoch 676/1500\n",
      "514/514 [==============================] - 0s 42us/step - loss: 0.0199 - val_loss: 0.0113\n",
      "Epoch 677/1500\n",
      "514/514 [==============================] - 0s 40us/step - loss: 0.0157 - val_loss: 0.0142\n",
      "Epoch 678/1500\n",
      "514/514 [==============================] - 0s 39us/step - loss: 0.0143 - val_loss: 0.0095\n",
      "Epoch 679/1500\n",
      "514/514 [==============================] - 0s 38us/step - loss: 0.0134 - val_loss: 0.0102\n",
      "Epoch 680/1500\n",
      "514/514 [==============================] - 0s 48us/step - loss: 0.0117 - val_loss: 0.0095\n",
      "Epoch 681/1500\n",
      "514/514 [==============================] - 0s 53us/step - loss: 0.0114 - val_loss: 0.0087\n",
      "Epoch 682/1500\n",
      "514/514 [==============================] - 0s 52us/step - loss: 0.0129 - val_loss: 0.0103\n",
      "Epoch 683/1500\n",
      "514/514 [==============================] - 0s 49us/step - loss: 0.0133 - val_loss: 0.0105\n",
      "Epoch 684/1500\n",
      "514/514 [==============================] - 0s 46us/step - loss: 0.0136 - val_loss: 0.0086\n",
      "Epoch 685/1500\n",
      "514/514 [==============================] - 0s 47us/step - loss: 0.0120 - val_loss: 0.0116\n",
      "Epoch 686/1500\n",
      "514/514 [==============================] - 0s 46us/step - loss: 0.0144 - val_loss: 0.0100\n",
      "Epoch 687/1500\n",
      "514/514 [==============================] - 0s 51us/step - loss: 0.0118 - val_loss: 0.0106\n",
      "Epoch 688/1500\n",
      "514/514 [==============================] - 0s 49us/step - loss: 0.0131 - val_loss: 0.0150\n",
      "Epoch 689/1500\n",
      "514/514 [==============================] - 0s 56us/step - loss: 0.0166 - val_loss: 0.0205\n",
      "Epoch 690/1500\n",
      "514/514 [==============================] - 0s 57us/step - loss: 0.0207 - val_loss: 0.0176\n",
      "Epoch 691/1500\n",
      "514/514 [==============================] - 0s 55us/step - loss: 0.0171 - val_loss: 0.0142\n",
      "Epoch 692/1500\n",
      "514/514 [==============================] - 0s 54us/step - loss: 0.0140 - val_loss: 0.0105\n",
      "Epoch 693/1500\n",
      "514/514 [==============================] - 0s 45us/step - loss: 0.0121 - val_loss: 0.0089\n",
      "Epoch 694/1500\n",
      "514/514 [==============================] - 0s 50us/step - loss: 0.0110 - val_loss: 0.0101\n",
      "Epoch 695/1500\n",
      "514/514 [==============================] - 0s 50us/step - loss: 0.0127 - val_loss: 0.0142\n",
      "Epoch 696/1500\n",
      "514/514 [==============================] - 0s 54us/step - loss: 0.0159 - val_loss: 0.0116\n",
      "Epoch 697/1500\n",
      "514/514 [==============================] - 0s 43us/step - loss: 0.0125 - val_loss: 0.0138\n",
      "Epoch 698/1500\n",
      "514/514 [==============================] - 0s 47us/step - loss: 0.0169 - val_loss: 0.0124\n",
      "Epoch 699/1500\n",
      "514/514 [==============================] - 0s 43us/step - loss: 0.0147 - val_loss: 0.0101\n",
      "Epoch 700/1500\n",
      "514/514 [==============================] - 0s 59us/step - loss: 0.0131 - val_loss: 0.0130\n",
      "Epoch 701/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "514/514 [==============================] - 0s 68us/step - loss: 0.0150 - val_loss: 0.0104\n",
      "Epoch 702/1500\n",
      "514/514 [==============================] - 0s 60us/step - loss: 0.0120 - val_loss: 0.0095\n",
      "Epoch 703/1500\n",
      "514/514 [==============================] - 0s 57us/step - loss: 0.0120 - val_loss: 0.0113\n",
      "Epoch 704/1500\n",
      "514/514 [==============================] - 0s 60us/step - loss: 0.0126 - val_loss: 0.0150\n",
      "Epoch 705/1500\n",
      "514/514 [==============================] - 0s 47us/step - loss: 0.0150 - val_loss: 0.0119\n",
      "Epoch 706/1500\n",
      "514/514 [==============================] - 0s 60us/step - loss: 0.0139 - val_loss: 0.0127\n",
      "Epoch 707/1500\n",
      "514/514 [==============================] - 0s 58us/step - loss: 0.0137 - val_loss: 0.0086\n",
      "Epoch 708/1500\n",
      "514/514 [==============================] - 0s 58us/step - loss: 0.0118 - val_loss: 0.0121\n",
      "Epoch 709/1500\n",
      "514/514 [==============================] - 0s 62us/step - loss: 0.0152 - val_loss: 0.0120\n",
      "Epoch 710/1500\n",
      "514/514 [==============================] - 0s 47us/step - loss: 0.0142 - val_loss: 0.0091\n",
      "Epoch 711/1500\n",
      "514/514 [==============================] - 0s 50us/step - loss: 0.0112 - val_loss: 0.0178\n",
      "Epoch 712/1500\n",
      "514/514 [==============================] - 0s 54us/step - loss: 0.0205 - val_loss: 0.0124\n",
      "Epoch 713/1500\n",
      "514/514 [==============================] - 0s 49us/step - loss: 0.0126 - val_loss: 0.0106\n",
      "Epoch 714/1500\n",
      "514/514 [==============================] - 0s 73us/step - loss: 0.0131 - val_loss: 0.0108\n",
      "Epoch 715/1500\n",
      "514/514 [==============================] - 0s 72us/step - loss: 0.0119 - val_loss: 0.0094\n",
      "Epoch 716/1500\n",
      "514/514 [==============================] - 0s 56us/step - loss: 0.0118 - val_loss: 0.0125\n",
      "Epoch 717/1500\n",
      "514/514 [==============================] - 0s 59us/step - loss: 0.0150 - val_loss: 0.0083\n",
      "Epoch 718/1500\n",
      "514/514 [==============================] - 0s 53us/step - loss: 0.0113 - val_loss: 0.0126\n",
      "Epoch 719/1500\n",
      "514/514 [==============================] - 0s 53us/step - loss: 0.0145 - val_loss: 0.0083\n",
      "Epoch 720/1500\n",
      "514/514 [==============================] - 0s 50us/step - loss: 0.0119 - val_loss: 0.0155\n",
      "Epoch 721/1500\n",
      "514/514 [==============================] - 0s 50us/step - loss: 0.0186 - val_loss: 0.0105\n",
      "Epoch 722/1500\n",
      "514/514 [==============================] - 0s 52us/step - loss: 0.0123 - val_loss: 0.0102\n",
      "Epoch 723/1500\n",
      "514/514 [==============================] - 0s 48us/step - loss: 0.0129 - val_loss: 0.0110\n",
      "Epoch 724/1500\n",
      "514/514 [==============================] - 0s 63us/step - loss: 0.0120 - val_loss: 0.0108\n",
      "Epoch 725/1500\n",
      "514/514 [==============================] - 0s 63us/step - loss: 0.0128 - val_loss: 0.0103\n",
      "Epoch 726/1500\n",
      "514/514 [==============================] - 0s 58us/step - loss: 0.0121 - val_loss: 0.0113\n",
      "Epoch 727/1500\n",
      "514/514 [==============================] - 0s 53us/step - loss: 0.0152 - val_loss: 0.0111\n",
      "Epoch 728/1500\n",
      "514/514 [==============================] - 0s 56us/step - loss: 0.0139 - val_loss: 0.0147\n",
      "Epoch 729/1500\n",
      "514/514 [==============================] - 0s 67us/step - loss: 0.0175 - val_loss: 0.0155\n",
      "Epoch 730/1500\n",
      "514/514 [==============================] - 0s 51us/step - loss: 0.0153 - val_loss: 0.0173\n",
      "Epoch 731/1500\n",
      "514/514 [==============================] - 0s 49us/step - loss: 0.0173 - val_loss: 0.0130\n",
      "Epoch 732/1500\n",
      "514/514 [==============================] - 0s 53us/step - loss: 0.0140 - val_loss: 0.0117\n",
      "Epoch 733/1500\n",
      "514/514 [==============================] - 0s 58us/step - loss: 0.0134 - val_loss: 0.0126\n",
      "Epoch 734/1500\n",
      "514/514 [==============================] - 0s 51us/step - loss: 0.0141 - val_loss: 0.0146\n",
      "Epoch 735/1500\n",
      "514/514 [==============================] - 0s 64us/step - loss: 0.0163 - val_loss: 0.0129\n",
      "Epoch 736/1500\n",
      "514/514 [==============================] - 0s 48us/step - loss: 0.0145 - val_loss: 0.0088\n",
      "Epoch 737/1500\n",
      "514/514 [==============================] - 0s 49us/step - loss: 0.0120 - val_loss: 0.0101\n",
      "Epoch 738/1500\n",
      "514/514 [==============================] - 0s 54us/step - loss: 0.0129 - val_loss: 0.0118\n",
      "Epoch 739/1500\n",
      "514/514 [==============================] - 0s 49us/step - loss: 0.0136 - val_loss: 0.0093\n",
      "Epoch 740/1500\n",
      "514/514 [==============================] - 0s 49us/step - loss: 0.0118 - val_loss: 0.0120\n",
      "Epoch 741/1500\n",
      "514/514 [==============================] - 0s 59us/step - loss: 0.0134 - val_loss: 0.0102\n",
      "Epoch 742/1500\n",
      "514/514 [==============================] - 0s 62us/step - loss: 0.0120 - val_loss: 0.0104\n",
      "Epoch 743/1500\n",
      "514/514 [==============================] - 0s 55us/step - loss: 0.0137 - val_loss: 0.0144\n",
      "Epoch 744/1500\n",
      "514/514 [==============================] - 0s 59us/step - loss: 0.0145 - val_loss: 0.0188\n",
      "Epoch 745/1500\n",
      "514/514 [==============================] - 0s 52us/step - loss: 0.0182 - val_loss: 0.0172\n",
      "Epoch 746/1500\n",
      "514/514 [==============================] - 0s 51us/step - loss: 0.0154 - val_loss: 0.0097\n",
      "Epoch 747/1500\n",
      "514/514 [==============================] - 0s 58us/step - loss: 0.0113 - val_loss: 0.0118\n",
      "Epoch 748/1500\n",
      "514/514 [==============================] - 0s 56us/step - loss: 0.0149 - val_loss: 0.0112\n",
      "Epoch 749/1500\n",
      "514/514 [==============================] - 0s 48us/step - loss: 0.0137 - val_loss: 0.0087\n",
      "Epoch 750/1500\n",
      "514/514 [==============================] - 0s 57us/step - loss: 0.0117 - val_loss: 0.0087\n",
      "Epoch 751/1500\n",
      "514/514 [==============================] - 0s 46us/step - loss: 0.0112 - val_loss: 0.0090\n",
      "Epoch 752/1500\n",
      "514/514 [==============================] - 0s 52us/step - loss: 0.0111 - val_loss: 0.0090\n",
      "Epoch 753/1500\n",
      "514/514 [==============================] - 0s 56us/step - loss: 0.0108 - val_loss: 0.0149\n",
      "Epoch 754/1500\n",
      "514/514 [==============================] - 0s 63us/step - loss: 0.0197 - val_loss: 0.0446\n",
      "Epoch 755/1500\n",
      "514/514 [==============================] - 0s 59us/step - loss: 0.0437 - val_loss: 0.0177\n",
      "Epoch 756/1500\n",
      "514/514 [==============================] - 0s 58us/step - loss: 0.0178 - val_loss: 0.0153\n",
      "Epoch 757/1500\n",
      "514/514 [==============================] - 0s 55us/step - loss: 0.0178 - val_loss: 0.0124\n",
      "Epoch 758/1500\n",
      "514/514 [==============================] - 0s 55us/step - loss: 0.0160 - val_loss: 0.0127\n",
      "Epoch 759/1500\n",
      "514/514 [==============================] - 0s 46us/step - loss: 0.0153 - val_loss: 0.0168\n",
      "Epoch 760/1500\n",
      "514/514 [==============================] - 0s 49us/step - loss: 0.0189 - val_loss: 0.0122\n",
      "Epoch 761/1500\n",
      "514/514 [==============================] - 0s 50us/step - loss: 0.0120 - val_loss: 0.0139\n",
      "Epoch 762/1500\n",
      "514/514 [==============================] - 0s 47us/step - loss: 0.0145 - val_loss: 0.0151\n",
      "Epoch 763/1500\n",
      "514/514 [==============================] - 0s 48us/step - loss: 0.0159 - val_loss: 0.0148\n",
      "Epoch 764/1500\n",
      "514/514 [==============================] - 0s 46us/step - loss: 0.0134 - val_loss: 0.0108\n",
      "Epoch 765/1500\n",
      "514/514 [==============================] - 0s 46us/step - loss: 0.0142 - val_loss: 0.0095\n",
      "Epoch 766/1500\n",
      "514/514 [==============================] - 0s 44us/step - loss: 0.0117 - val_loss: 0.0115\n",
      "Epoch 767/1500\n",
      "514/514 [==============================] - 0s 43us/step - loss: 0.0130 - val_loss: 0.0099\n",
      "Epoch 768/1500\n",
      "514/514 [==============================] - 0s 49us/step - loss: 0.0114 - val_loss: 0.0106\n",
      "Epoch 769/1500\n",
      "514/514 [==============================] - 0s 38us/step - loss: 0.0120 - val_loss: 0.0148\n",
      "Epoch 770/1500\n",
      "514/514 [==============================] - 0s 46us/step - loss: 0.0178 - val_loss: 0.0126\n",
      "Epoch 771/1500\n",
      "514/514 [==============================] - 0s 48us/step - loss: 0.0132 - val_loss: 0.0099\n",
      "Epoch 772/1500\n",
      "514/514 [==============================] - 0s 48us/step - loss: 0.0136 - val_loss: 0.0100\n",
      "Epoch 773/1500\n",
      "514/514 [==============================] - 0s 48us/step - loss: 0.0119 - val_loss: 0.0109\n",
      "Epoch 774/1500\n",
      "514/514 [==============================] - 0s 58us/step - loss: 0.0151 - val_loss: 0.0132\n",
      "Epoch 775/1500\n",
      "514/514 [==============================] - 0s 61us/step - loss: 0.0137 - val_loss: 0.0096\n",
      "Epoch 776/1500\n",
      "514/514 [==============================] - ETA: 0s - loss: 0.010 - 0s 54us/step - loss: 0.0118 - val_loss: 0.0098\n",
      "Epoch 777/1500\n",
      "514/514 [==============================] - 0s 62us/step - loss: 0.0110 - val_loss: 0.0125\n",
      "Epoch 778/1500\n",
      "514/514 [==============================] - 0s 56us/step - loss: 0.0147 - val_loss: 0.0103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 779/1500\n",
      "514/514 [==============================] - 0s 58us/step - loss: 0.0120 - val_loss: 0.0115\n",
      "Epoch 780/1500\n",
      "514/514 [==============================] - 0s 57us/step - loss: 0.0146 - val_loss: 0.0093\n",
      "Epoch 781/1500\n",
      "514/514 [==============================] - 0s 44us/step - loss: 0.0117 - val_loss: 0.0106\n",
      "Epoch 782/1500\n",
      "514/514 [==============================] - 0s 45us/step - loss: 0.0131 - val_loss: 0.0112\n",
      "Epoch 783/1500\n",
      "514/514 [==============================] - 0s 49us/step - loss: 0.0129 - val_loss: 0.0095\n",
      "Epoch 784/1500\n",
      "514/514 [==============================] - 0s 42us/step - loss: 0.0120 - val_loss: 0.0151\n",
      "Epoch 785/1500\n",
      "514/514 [==============================] - 0s 49us/step - loss: 0.0160 - val_loss: 0.0109\n",
      "Epoch 786/1500\n",
      "514/514 [==============================] - 0s 49us/step - loss: 0.0121 - val_loss: 0.0092\n",
      "Epoch 787/1500\n",
      "514/514 [==============================] - 0s 53us/step - loss: 0.0117 - val_loss: 0.0128\n",
      "Epoch 788/1500\n",
      "514/514 [==============================] - 0s 53us/step - loss: 0.0140 - val_loss: 0.0090\n",
      "Epoch 789/1500\n",
      "514/514 [==============================] - 0s 52us/step - loss: 0.0112 - val_loss: 0.0120\n",
      "Epoch 790/1500\n",
      "514/514 [==============================] - 0s 47us/step - loss: 0.0140 - val_loss: 0.0111\n",
      "Epoch 791/1500\n",
      "514/514 [==============================] - 0s 46us/step - loss: 0.0124 - val_loss: 0.0096\n",
      "Epoch 792/1500\n",
      "514/514 [==============================] - 0s 49us/step - loss: 0.0109 - val_loss: 0.0097\n",
      "Epoch 793/1500\n",
      "514/514 [==============================] - 0s 45us/step - loss: 0.0109 - val_loss: 0.0095\n",
      "Epoch 794/1500\n",
      "514/514 [==============================] - 0s 42us/step - loss: 0.0107 - val_loss: 0.0114\n",
      "Epoch 795/1500\n",
      "514/514 [==============================] - 0s 54us/step - loss: 0.0149 - val_loss: 0.0218\n",
      "Epoch 796/1500\n",
      "514/514 [==============================] - 0s 50us/step - loss: 0.0199 - val_loss: 0.0140\n",
      "Epoch 797/1500\n",
      "514/514 [==============================] - 0s 45us/step - loss: 0.0132 - val_loss: 0.0100\n",
      "Epoch 798/1500\n",
      "514/514 [==============================] - 0s 55us/step - loss: 0.0142 - val_loss: 0.0131\n",
      "Epoch 799/1500\n",
      "514/514 [==============================] - 0s 46us/step - loss: 0.0162 - val_loss: 0.0112\n",
      "Epoch 800/1500\n",
      "514/514 [==============================] - 0s 47us/step - loss: 0.0151 - val_loss: 0.0138\n",
      "Epoch 801/1500\n",
      "514/514 [==============================] - 0s 49us/step - loss: 0.0165 - val_loss: 0.0083\n",
      "Epoch 802/1500\n",
      "514/514 [==============================] - 0s 60us/step - loss: 0.0110 - val_loss: 0.0161\n",
      "Epoch 803/1500\n",
      "514/514 [==============================] - 0s 48us/step - loss: 0.0195 - val_loss: 0.0125\n",
      "Epoch 804/1500\n",
      "514/514 [==============================] - 0s 52us/step - loss: 0.0123 - val_loss: 0.0118\n",
      "Epoch 805/1500\n",
      "514/514 [==============================] - 0s 49us/step - loss: 0.0142 - val_loss: 0.0139\n",
      "Epoch 806/1500\n",
      "514/514 [==============================] - 0s 44us/step - loss: 0.0166 - val_loss: 0.0227\n",
      "Epoch 807/1500\n",
      "514/514 [==============================] - 0s 50us/step - loss: 0.0240 - val_loss: 0.0294\n",
      "Epoch 808/1500\n",
      "514/514 [==============================] - 0s 48us/step - loss: 0.0310 - val_loss: 0.0151\n",
      "Epoch 809/1500\n",
      "514/514 [==============================] - 0s 46us/step - loss: 0.0206 - val_loss: 0.0153\n",
      "Epoch 810/1500\n",
      "514/514 [==============================] - 0s 42us/step - loss: 0.0173 - val_loss: 0.0284\n",
      "Epoch 811/1500\n",
      "514/514 [==============================] - 0s 69us/step - loss: 0.0272 - val_loss: 0.0338\n",
      "Epoch 812/1500\n",
      "514/514 [==============================] - 0s 73us/step - loss: 0.0310 - val_loss: 0.0231\n",
      "Epoch 813/1500\n",
      "514/514 [==============================] - 0s 75us/step - loss: 0.0206 - val_loss: 0.0146\n",
      "Epoch 814/1500\n",
      "514/514 [==============================] - 0s 144us/step - loss: 0.0154 - val_loss: 0.0156\n",
      "Epoch 815/1500\n",
      "514/514 [==============================] - 0s 81us/step - loss: 0.0184 - val_loss: 0.0186\n",
      "Epoch 816/1500\n",
      "514/514 [==============================] - 0s 65us/step - loss: 0.0227 - val_loss: 0.0219\n",
      "Epoch 817/1500\n",
      "514/514 [==============================] - 0s 66us/step - loss: 0.0223 - val_loss: 0.0380\n",
      "Epoch 818/1500\n",
      "514/514 [==============================] - 0s 65us/step - loss: 0.0272 - val_loss: 0.0239\n",
      "Epoch 819/1500\n",
      "514/514 [==============================] - 0s 56us/step - loss: 0.0252 - val_loss: 0.0119\n",
      "Epoch 820/1500\n",
      "514/514 [==============================] - 0s 53us/step - loss: 0.0195 - val_loss: 0.0481\n",
      "Epoch 821/1500\n",
      "514/514 [==============================] - 0s 46us/step - loss: 0.0481 - val_loss: 0.0358\n",
      "Epoch 822/1500\n",
      "514/514 [==============================] - 0s 39us/step - loss: 0.0318 - val_loss: 0.0465\n",
      "Epoch 823/1500\n",
      "514/514 [==============================] - 0s 34us/step - loss: 0.0433 - val_loss: 0.0686\n",
      "Epoch 824/1500\n",
      "514/514 [==============================] - 0s 38us/step - loss: 0.0534 - val_loss: 0.0160\n",
      "Epoch 825/1500\n",
      "514/514 [==============================] - 0s 51us/step - loss: 0.0200 - val_loss: 0.0210\n",
      "Epoch 826/1500\n",
      "514/514 [==============================] - 0s 37us/step - loss: 0.0249 - val_loss: 0.0181\n",
      "Epoch 827/1500\n",
      "514/514 [==============================] - 0s 34us/step - loss: 0.0212 - val_loss: 0.0241\n",
      "Epoch 828/1500\n",
      "514/514 [==============================] - 0s 39us/step - loss: 0.0519 - val_loss: 0.0692\n",
      "Epoch 829/1500\n",
      "514/514 [==============================] - 0s 33us/step - loss: 0.0573 - val_loss: 0.0129\n",
      "Epoch 830/1500\n",
      "514/514 [==============================] - 0s 63us/step - loss: 0.0143 - val_loss: 0.0422\n",
      "Epoch 831/1500\n",
      "514/514 [==============================] - 0s 41us/step - loss: 0.0378 - val_loss: 0.0408\n",
      "Epoch 832/1500\n",
      "514/514 [==============================] - 0s 32us/step - loss: 0.0437 - val_loss: 0.0167\n",
      "Epoch 833/1500\n",
      "514/514 [==============================] - 0s 33us/step - loss: 0.0174 - val_loss: 0.0401\n",
      "Epoch 834/1500\n",
      "514/514 [==============================] - 0s 32us/step - loss: 0.0347 - val_loss: 0.0438\n",
      "Epoch 835/1500\n",
      "514/514 [==============================] - 0s 32us/step - loss: 0.0490 - val_loss: 0.0160\n",
      "Epoch 836/1500\n",
      "514/514 [==============================] - 0s 33us/step - loss: 0.0246 - val_loss: 0.0152\n",
      "Epoch 837/1500\n",
      "514/514 [==============================] - 0s 33us/step - loss: 0.0149 - val_loss: 0.0184\n",
      "Epoch 838/1500\n",
      "514/514 [==============================] - 0s 33us/step - loss: 0.0185 - val_loss: 0.0122\n",
      "Epoch 839/1500\n",
      "514/514 [==============================] - 0s 35us/step - loss: 0.0140 - val_loss: 0.0118\n",
      "Epoch 840/1500\n",
      "514/514 [==============================] - 0s 34us/step - loss: 0.0116 - val_loss: 0.0109\n",
      "Epoch 841/1500\n",
      "514/514 [==============================] - 0s 35us/step - loss: 0.0125 - val_loss: 0.0128\n",
      "Epoch 842/1500\n",
      "514/514 [==============================] - 0s 35us/step - loss: 0.0135 - val_loss: 0.0109\n",
      "Epoch 843/1500\n",
      "514/514 [==============================] - 0s 37us/step - loss: 0.0117 - val_loss: 0.0114\n",
      "Epoch 844/1500\n",
      "514/514 [==============================] - 0s 36us/step - loss: 0.0124 - val_loss: 0.0126\n",
      "Epoch 845/1500\n",
      "514/514 [==============================] - 0s 65us/step - loss: 0.0139 - val_loss: 0.0121\n",
      "Epoch 846/1500\n",
      "514/514 [==============================] - 0s 94us/step - loss: 0.0133 - val_loss: 0.0110\n",
      "Epoch 847/1500\n",
      "514/514 [==============================] - 0s 67us/step - loss: 0.0128 - val_loss: 0.0095\n",
      "Epoch 848/1500\n",
      "514/514 [==============================] - 0s 74us/step - loss: 0.0118 - val_loss: 0.0097\n",
      "Epoch 849/1500\n",
      "514/514 [==============================] - 0s 72us/step - loss: 0.0111 - val_loss: 0.0130\n",
      "Epoch 850/1500\n",
      "514/514 [==============================] - 0s 66us/step - loss: 0.0139 - val_loss: 0.0101\n",
      "Epoch 851/1500\n",
      "514/514 [==============================] - 0s 65us/step - loss: 0.0152 - val_loss: 0.0178\n",
      "Epoch 852/1500\n",
      "514/514 [==============================] - 0s 60us/step - loss: 0.0209 - val_loss: 0.0176\n",
      "Epoch 853/1500\n",
      "514/514 [==============================] - 0s 50us/step - loss: 0.0209 - val_loss: 0.0149\n",
      "Epoch 854/1500\n",
      "514/514 [==============================] - 0s 45us/step - loss: 0.0182 - val_loss: 0.0119\n",
      "Epoch 855/1500\n",
      "514/514 [==============================] - 0s 46us/step - loss: 0.0145 - val_loss: 0.0115\n",
      "Epoch 856/1500\n",
      "514/514 [==============================] - 0s 37us/step - loss: 0.0134 - val_loss: 0.0106\n",
      "Epoch 857/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "514/514 [==============================] - 0s 46us/step - loss: 0.0126 - val_loss: 0.0118\n",
      "Epoch 858/1500\n",
      "514/514 [==============================] - 0s 40us/step - loss: 0.0133 - val_loss: 0.0139\n",
      "Epoch 859/1500\n",
      "514/514 [==============================] - 0s 54us/step - loss: 0.0140 - val_loss: 0.0153\n",
      "Epoch 860/1500\n",
      "514/514 [==============================] - 0s 46us/step - loss: 0.0147 - val_loss: 0.0124\n",
      "Epoch 861/1500\n",
      "514/514 [==============================] - 0s 44us/step - loss: 0.0127 - val_loss: 0.0116\n",
      "Epoch 862/1500\n",
      "514/514 [==============================] - 0s 66us/step - loss: 0.0134 - val_loss: 0.0123\n",
      "Epoch 863/1500\n",
      "514/514 [==============================] - 0s 62us/step - loss: 0.0141 - val_loss: 0.0109\n",
      "Epoch 864/1500\n",
      "514/514 [==============================] - 0s 59us/step - loss: 0.0132 - val_loss: 0.0135\n",
      "Epoch 865/1500\n",
      "514/514 [==============================] - 0s 51us/step - loss: 0.0165 - val_loss: 0.0156\n",
      "Epoch 866/1500\n",
      "514/514 [==============================] - 0s 55us/step - loss: 0.0150 - val_loss: 0.0118\n",
      "Epoch 867/1500\n",
      "514/514 [==============================] - 0s 50us/step - loss: 0.0126 - val_loss: 0.0095\n",
      "Epoch 868/1500\n",
      "514/514 [==============================] - 0s 54us/step - loss: 0.0112 - val_loss: 0.0108\n",
      "Epoch 869/1500\n",
      "514/514 [==============================] - 0s 50us/step - loss: 0.0146 - val_loss: 0.0213\n",
      "Epoch 870/1500\n",
      "514/514 [==============================] - 0s 43us/step - loss: 0.0232 - val_loss: 0.0103\n",
      "Epoch 871/1500\n",
      "514/514 [==============================] - 0s 36us/step - loss: 0.0120 - val_loss: 0.0125\n",
      "Epoch 872/1500\n",
      "514/514 [==============================] - 0s 34us/step - loss: 0.0159 - val_loss: 0.0108\n",
      "Epoch 873/1500\n",
      "514/514 [==============================] - 0s 33us/step - loss: 0.0124 - val_loss: 0.0117\n",
      "Epoch 874/1500\n",
      "514/514 [==============================] - 0s 36us/step - loss: 0.0126 - val_loss: 0.0108\n",
      "Epoch 875/1500\n",
      "514/514 [==============================] - 0s 32us/step - loss: 0.0124 - val_loss: 0.0100\n",
      "Epoch 876/1500\n",
      "514/514 [==============================] - 0s 33us/step - loss: 0.0127 - val_loss: 0.0119\n",
      "Epoch 877/1500\n",
      "514/514 [==============================] - 0s 40us/step - loss: 0.0162 - val_loss: 0.0150\n",
      "Epoch 878/1500\n",
      "514/514 [==============================] - 0s 35us/step - loss: 0.0192 - val_loss: 0.0172\n",
      "Epoch 879/1500\n",
      "514/514 [==============================] - 0s 40us/step - loss: 0.0207 - val_loss: 0.0131\n",
      "Epoch 880/1500\n",
      "514/514 [==============================] - 0s 51us/step - loss: 0.0163 - val_loss: 0.0121\n",
      "Epoch 881/1500\n",
      "514/514 [==============================] - 0s 39us/step - loss: 0.0145 - val_loss: 0.0115\n",
      "Epoch 882/1500\n",
      "514/514 [==============================] - 0s 56us/step - loss: 0.0150 - val_loss: 0.0123\n",
      "Epoch 883/1500\n",
      "514/514 [==============================] - 0s 57us/step - loss: 0.0132 - val_loss: 0.0096\n",
      "Epoch 884/1500\n",
      "514/514 [==============================] - 0s 55us/step - loss: 0.0120 - val_loss: 0.0111\n",
      "Epoch 885/1500\n",
      "514/514 [==============================] - 0s 58us/step - loss: 0.0125 - val_loss: 0.0092\n",
      "Epoch 886/1500\n",
      "514/514 [==============================] - 0s 71us/step - loss: 0.0114 - val_loss: 0.0176\n",
      "Epoch 887/1500\n",
      "514/514 [==============================] - 0s 51us/step - loss: 0.0207 - val_loss: 0.0157\n",
      "Epoch 888/1500\n",
      "514/514 [==============================] - 0s 58us/step - loss: 0.0145 - val_loss: 0.0110\n",
      "Epoch 889/1500\n",
      "514/514 [==============================] - 0s 50us/step - loss: 0.0147 - val_loss: 0.0159\n",
      "Epoch 890/1500\n",
      "514/514 [==============================] - 0s 73us/step - loss: 0.0161 - val_loss: 0.0119\n",
      "Epoch 891/1500\n",
      "514/514 [==============================] - 0s 57us/step - loss: 0.0160 - val_loss: 0.0135\n",
      "Epoch 892/1500\n",
      "514/514 [==============================] - 0s 45us/step - loss: 0.0132 - val_loss: 0.0161\n",
      "Epoch 893/1500\n",
      "514/514 [==============================] - 0s 36us/step - loss: 0.0192 - val_loss: 0.0160\n",
      "Epoch 894/1500\n",
      "514/514 [==============================] - 0s 36us/step - loss: 0.0159 - val_loss: 0.0132\n",
      "Epoch 895/1500\n",
      "514/514 [==============================] - 0s 35us/step - loss: 0.0150 - val_loss: 0.0248\n",
      "Epoch 896/1500\n",
      "514/514 [==============================] - 0s 36us/step - loss: 0.0230 - val_loss: 0.0134\n",
      "Epoch 897/1500\n",
      "514/514 [==============================] - 0s 35us/step - loss: 0.0133 - val_loss: 0.0120\n",
      "Epoch 898/1500\n",
      "514/514 [==============================] - 0s 41us/step - loss: 0.0128 - val_loss: 0.0111\n",
      "Epoch 899/1500\n",
      "514/514 [==============================] - 0s 85us/step - loss: 0.0125 - val_loss: 0.0137\n",
      "Epoch 900/1500\n",
      "514/514 [==============================] - 0s 99us/step - loss: 0.0146 - val_loss: 0.0121\n",
      "Epoch 901/1500\n",
      "514/514 [==============================] - 0s 83us/step - loss: 0.0130 - val_loss: 0.0099\n",
      "Epoch 902/1500\n",
      "514/514 [==============================] - 0s 74us/step - loss: 0.0125 - val_loss: 0.0116\n",
      "Epoch 903/1500\n",
      "514/514 [==============================] - 0s 149us/step - loss: 0.0131 - val_loss: 0.0120\n",
      "Epoch 904/1500\n",
      "514/514 [==============================] - 0s 68us/step - loss: 0.0135 - val_loss: 0.0149\n",
      "Epoch 905/1500\n",
      "514/514 [==============================] - 0s 54us/step - loss: 0.0151 - val_loss: 0.0095\n",
      "Epoch 906/1500\n",
      "514/514 [==============================] - 0s 56us/step - loss: 0.0117 - val_loss: 0.0147\n",
      "Epoch 907/1500\n",
      "514/514 [==============================] - 0s 52us/step - loss: 0.0179 - val_loss: 0.0092\n",
      "Epoch 908/1500\n",
      "514/514 [==============================] - 0s 48us/step - loss: 0.0120 - val_loss: 0.0115\n",
      "Epoch 909/1500\n",
      "514/514 [==============================] - 0s 51us/step - loss: 0.0130 - val_loss: 0.0114\n",
      "Epoch 910/1500\n",
      "514/514 [==============================] - 0s 55us/step - loss: 0.0123 - val_loss: 0.0115\n",
      "Epoch 911/1500\n",
      "514/514 [==============================] - 0s 59us/step - loss: 0.0118 - val_loss: 0.0112\n",
      "Epoch 912/1500\n",
      "514/514 [==============================] - 0s 42us/step - loss: 0.0131 - val_loss: 0.0116\n",
      "Epoch 913/1500\n",
      "514/514 [==============================] - 0s 54us/step - loss: 0.0124 - val_loss: 0.0111\n",
      "Epoch 914/1500\n",
      "514/514 [==============================] - 0s 49us/step - loss: 0.0145 - val_loss: 0.0171\n",
      "Epoch 915/1500\n",
      "514/514 [==============================] - 0s 46us/step - loss: 0.0183 - val_loss: 0.0095\n",
      "Epoch 916/1500\n",
      "514/514 [==============================] - 0s 49us/step - loss: 0.0114 - val_loss: 0.0145\n",
      "Epoch 917/1500\n",
      "514/514 [==============================] - 0s 57us/step - loss: 0.0160 - val_loss: 0.0109\n",
      "Epoch 918/1500\n",
      "514/514 [==============================] - 0s 48us/step - loss: 0.0120 - val_loss: 0.0144\n",
      "Epoch 919/1500\n",
      "514/514 [==============================] - 0s 48us/step - loss: 0.0156 - val_loss: 0.0096\n",
      "Epoch 920/1500\n",
      "514/514 [==============================] - 0s 49us/step - loss: 0.0134 - val_loss: 0.0110\n",
      "Epoch 921/1500\n",
      "514/514 [==============================] - 0s 51us/step - loss: 0.0132 - val_loss: 0.0104\n",
      "Epoch 922/1500\n",
      "514/514 [==============================] - 0s 52us/step - loss: 0.0124 - val_loss: 0.0085\n",
      "Epoch 923/1500\n",
      "514/514 [==============================] - 0s 50us/step - loss: 0.0109 - val_loss: 0.0099\n",
      "Epoch 924/1500\n",
      "514/514 [==============================] - 0s 45us/step - loss: 0.0121 - val_loss: 0.0096\n",
      "Epoch 925/1500\n",
      "514/514 [==============================] - 0s 47us/step - loss: 0.0112 - val_loss: 0.0112\n",
      "Epoch 926/1500\n",
      "514/514 [==============================] - 0s 47us/step - loss: 0.0163 - val_loss: 0.0159\n",
      "Epoch 927/1500\n",
      "514/514 [==============================] - 0s 52us/step - loss: 0.0160 - val_loss: 0.0085\n",
      "Epoch 928/1500\n",
      "514/514 [==============================] - 0s 49us/step - loss: 0.0105 - val_loss: 0.0091\n",
      "Epoch 929/1500\n",
      "514/514 [==============================] - 0s 57us/step - loss: 0.0108 - val_loss: 0.0097\n",
      "Epoch 930/1500\n",
      "514/514 [==============================] - 0s 49us/step - loss: 0.0115 - val_loss: 0.0099\n",
      "Epoch 931/1500\n",
      "514/514 [==============================] - 0s 51us/step - loss: 0.0113 - val_loss: 0.0098\n",
      "Epoch 932/1500\n",
      "514/514 [==============================] - 0s 51us/step - loss: 0.0116 - val_loss: 0.0110\n",
      "Epoch 933/1500\n",
      "514/514 [==============================] - 0s 56us/step - loss: 0.0131 - val_loss: 0.0185\n",
      "Epoch 934/1500\n",
      "514/514 [==============================] - 0s 50us/step - loss: 0.0180 - val_loss: 0.0122\n",
      "Epoch 935/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "514/514 [==============================] - 0s 46us/step - loss: 0.0123 - val_loss: 0.0094\n",
      "Epoch 936/1500\n",
      "514/514 [==============================] - 0s 53us/step - loss: 0.0108 - val_loss: 0.0093\n",
      "Epoch 937/1500\n",
      "514/514 [==============================] - 0s 47us/step - loss: 0.0122 - val_loss: 0.0115\n",
      "Epoch 938/1500\n",
      "514/514 [==============================] - 0s 45us/step - loss: 0.0136 - val_loss: 0.0165\n",
      "Epoch 939/1500\n",
      "514/514 [==============================] - 0s 50us/step - loss: 0.0187 - val_loss: 0.0449\n",
      "Epoch 940/1500\n",
      "514/514 [==============================] - 0s 52us/step - loss: 0.0389 - val_loss: 0.0263\n",
      "Epoch 941/1500\n",
      "514/514 [==============================] - 0s 62us/step - loss: 0.0224 - val_loss: 0.0147\n",
      "Epoch 942/1500\n",
      "514/514 [==============================] - ETA: 0s - loss: 0.016 - 0s 55us/step - loss: 0.0150 - val_loss: 0.0116\n",
      "Epoch 943/1500\n",
      "514/514 [==============================] - 0s 61us/step - loss: 0.0139 - val_loss: 0.0129\n",
      "Epoch 944/1500\n",
      "514/514 [==============================] - 0s 57us/step - loss: 0.0165 - val_loss: 0.0111\n",
      "Epoch 945/1500\n",
      "514/514 [==============================] - 0s 47us/step - loss: 0.0146 - val_loss: 0.0119\n",
      "Epoch 946/1500\n",
      "514/514 [==============================] - 0s 42us/step - loss: 0.0158 - val_loss: 0.0110\n",
      "Epoch 947/1500\n",
      "514/514 [==============================] - 0s 45us/step - loss: 0.0139 - val_loss: 0.0105\n",
      "Epoch 948/1500\n",
      "514/514 [==============================] - 0s 48us/step - loss: 0.0133 - val_loss: 0.0087\n",
      "Epoch 949/1500\n",
      "514/514 [==============================] - 0s 44us/step - loss: 0.0111 - val_loss: 0.0094\n",
      "Epoch 950/1500\n",
      "514/514 [==============================] - 0s 44us/step - loss: 0.0115 - val_loss: 0.0091\n",
      "Epoch 951/1500\n",
      "514/514 [==============================] - 0s 47us/step - loss: 0.0113 - val_loss: 0.0099\n",
      "Epoch 952/1500\n",
      "514/514 [==============================] - 0s 50us/step - loss: 0.0119 - val_loss: 0.0093\n",
      "Epoch 953/1500\n",
      "514/514 [==============================] - 0s 47us/step - loss: 0.0111 - val_loss: 0.0141\n",
      "Epoch 954/1500\n",
      "514/514 [==============================] - 0s 49us/step - loss: 0.0160 - val_loss: 0.0207\n",
      "Epoch 955/1500\n",
      "514/514 [==============================] - 0s 50us/step - loss: 0.0201 - val_loss: 0.0090\n",
      "Epoch 956/1500\n",
      "514/514 [==============================] - 0s 46us/step - loss: 0.0127 - val_loss: 0.0215\n",
      "Epoch 957/1500\n",
      "514/514 [==============================] - 0s 52us/step - loss: 0.0266 - val_loss: 0.0269\n",
      "Epoch 958/1500\n",
      "514/514 [==============================] - 0s 49us/step - loss: 0.0300 - val_loss: 0.0286\n",
      "Epoch 959/1500\n",
      "514/514 [==============================] - 0s 48us/step - loss: 0.0317 - val_loss: 0.0261\n",
      "Epoch 960/1500\n",
      "514/514 [==============================] - 0s 48us/step - loss: 0.0319 - val_loss: 0.0505\n",
      "Epoch 961/1500\n",
      "514/514 [==============================] - 0s 46us/step - loss: 0.0489 - val_loss: 0.0223\n",
      "Epoch 962/1500\n",
      "514/514 [==============================] - 0s 52us/step - loss: 0.0276 - val_loss: 0.0255\n",
      "Epoch 963/1500\n",
      "514/514 [==============================] - 0s 52us/step - loss: 0.0410 - val_loss: 0.0850\n",
      "Epoch 964/1500\n",
      "514/514 [==============================] - 0s 57us/step - loss: 0.0719 - val_loss: 0.0204\n",
      "Epoch 965/1500\n",
      "514/514 [==============================] - 0s 50us/step - loss: 0.0230 - val_loss: 0.0157\n",
      "Epoch 966/1500\n",
      "514/514 [==============================] - 0s 53us/step - loss: 0.0188 - val_loss: 0.0190\n",
      "Epoch 967/1500\n",
      "514/514 [==============================] - 0s 51us/step - loss: 0.0198 - val_loss: 0.0191\n",
      "Epoch 968/1500\n",
      "514/514 [==============================] - 0s 48us/step - loss: 0.0200 - val_loss: 0.0162\n",
      "Epoch 969/1500\n",
      "514/514 [==============================] - 0s 49us/step - loss: 0.0206 - val_loss: 0.0128\n",
      "Epoch 970/1500\n",
      "514/514 [==============================] - 0s 49us/step - loss: 0.0134 - val_loss: 0.0328\n",
      "Epoch 971/1500\n",
      "514/514 [==============================] - 0s 44us/step - loss: 0.0322 - val_loss: 0.0093\n",
      "Epoch 972/1500\n",
      "514/514 [==============================] - 0s 44us/step - loss: 0.0151 - val_loss: 0.0109\n",
      "Epoch 973/1500\n",
      "514/514 [==============================] - 0s 44us/step - loss: 0.0136 - val_loss: 0.0152\n",
      "Epoch 974/1500\n",
      "514/514 [==============================] - 0s 44us/step - loss: 0.0166 - val_loss: 0.0132\n",
      "Epoch 975/1500\n",
      "514/514 [==============================] - 0s 47us/step - loss: 0.0164 - val_loss: 0.0130\n",
      "Epoch 976/1500\n",
      "514/514 [==============================] - 0s 49us/step - loss: 0.0136 - val_loss: 0.0100\n",
      "Epoch 977/1500\n",
      "514/514 [==============================] - 0s 53us/step - loss: 0.0118 - val_loss: 0.0098\n",
      "Epoch 978/1500\n",
      "514/514 [==============================] - ETA: 0s - loss: 0.012 - 0s 58us/step - loss: 0.0131 - val_loss: 0.0154\n",
      "Epoch 979/1500\n",
      "514/514 [==============================] - 0s 45us/step - loss: 0.0159 - val_loss: 0.0108\n",
      "Epoch 980/1500\n",
      "514/514 [==============================] - 0s 53us/step - loss: 0.0120 - val_loss: 0.0121\n",
      "Epoch 981/1500\n",
      "514/514 [==============================] - 0s 47us/step - loss: 0.0132 - val_loss: 0.0118\n",
      "Epoch 982/1500\n",
      "514/514 [==============================] - 0s 52us/step - loss: 0.0121 - val_loss: 0.0125\n",
      "Epoch 983/1500\n",
      "514/514 [==============================] - 0s 49us/step - loss: 0.0123 - val_loss: 0.0115\n",
      "Epoch 984/1500\n",
      "514/514 [==============================] - 0s 49us/step - loss: 0.0120 - val_loss: 0.0106\n",
      "Epoch 985/1500\n",
      "514/514 [==============================] - 0s 51us/step - loss: 0.0122 - val_loss: 0.0112\n",
      "Epoch 986/1500\n",
      "514/514 [==============================] - 0s 44us/step - loss: 0.0131 - val_loss: 0.0108\n",
      "Epoch 987/1500\n",
      "514/514 [==============================] - 0s 49us/step - loss: 0.0127 - val_loss: 0.0095\n",
      "Epoch 988/1500\n",
      "514/514 [==============================] - 0s 51us/step - loss: 0.0111 - val_loss: 0.0098\n",
      "Epoch 989/1500\n",
      "514/514 [==============================] - 0s 53us/step - loss: 0.0121 - val_loss: 0.0108\n",
      "Epoch 990/1500\n",
      "514/514 [==============================] - 0s 53us/step - loss: 0.0136 - val_loss: 0.0101\n",
      "Epoch 991/1500\n",
      "514/514 [==============================] - 0s 45us/step - loss: 0.0128 - val_loss: 0.0095\n",
      "Epoch 992/1500\n",
      "514/514 [==============================] - 0s 50us/step - loss: 0.0126 - val_loss: 0.0099\n",
      "Epoch 993/1500\n",
      "514/514 [==============================] - 0s 37us/step - loss: 0.0130 - val_loss: 0.0096\n",
      "Epoch 994/1500\n",
      "514/514 [==============================] - 0s 51us/step - loss: 0.0128 - val_loss: 0.0090\n",
      "Epoch 995/1500\n",
      "514/514 [==============================] - 0s 44us/step - loss: 0.0118 - val_loss: 0.0087\n",
      "Epoch 996/1500\n",
      "514/514 [==============================] - 0s 47us/step - loss: 0.0112 - val_loss: 0.0115\n",
      "Epoch 997/1500\n",
      "514/514 [==============================] - 0s 50us/step - loss: 0.0148 - val_loss: 0.0165\n",
      "Epoch 998/1500\n",
      "514/514 [==============================] - 0s 42us/step - loss: 0.0180 - val_loss: 0.0111\n",
      "Epoch 999/1500\n",
      "514/514 [==============================] - 0s 47us/step - loss: 0.0137 - val_loss: 0.0181\n",
      "Epoch 1000/1500\n",
      "514/514 [==============================] - 0s 46us/step - loss: 0.0227 - val_loss: 0.0132\n",
      "Epoch 1001/1500\n",
      "514/514 [==============================] - 0s 50us/step - loss: 0.0152 - val_loss: 0.0124\n",
      "Epoch 1002/1500\n",
      "514/514 [==============================] - 0s 49us/step - loss: 0.0157 - val_loss: 0.0112\n",
      "Epoch 1003/1500\n",
      "514/514 [==============================] - 0s 46us/step - loss: 0.0128 - val_loss: 0.0097\n",
      "Epoch 1004/1500\n",
      "514/514 [==============================] - 0s 48us/step - loss: 0.0114 - val_loss: 0.0100\n",
      "Epoch 1005/1500\n",
      "514/514 [==============================] - 0s 45us/step - loss: 0.0110 - val_loss: 0.0119\n",
      "Epoch 1006/1500\n",
      "514/514 [==============================] - 0s 46us/step - loss: 0.0134 - val_loss: 0.0128\n",
      "Epoch 1007/1500\n",
      "514/514 [==============================] - 0s 50us/step - loss: 0.0140 - val_loss: 0.0102\n",
      "Epoch 1008/1500\n",
      "514/514 [==============================] - 0s 46us/step - loss: 0.0111 - val_loss: 0.0107\n",
      "Epoch 1009/1500\n",
      "514/514 [==============================] - 0s 46us/step - loss: 0.0127 - val_loss: 0.0101\n",
      "Epoch 1010/1500\n",
      "514/514 [==============================] - 0s 53us/step - loss: 0.0112 - val_loss: 0.0118\n",
      "Epoch 1011/1500\n",
      "514/514 [==============================] - 0s 57us/step - loss: 0.0132 - val_loss: 0.0100\n",
      "Epoch 1012/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "514/514 [==============================] - 0s 55us/step - loss: 0.0112 - val_loss: 0.0103\n",
      "Epoch 1013/1500\n",
      "514/514 [==============================] - 0s 52us/step - loss: 0.0125 - val_loss: 0.0096\n",
      "Epoch 1014/1500\n",
      "514/514 [==============================] - 0s 46us/step - loss: 0.0115 - val_loss: 0.0090\n",
      "Epoch 1015/1500\n",
      "514/514 [==============================] - 0s 59us/step - loss: 0.0111 - val_loss: 0.0092\n",
      "Epoch 1016/1500\n",
      "514/514 [==============================] - 0s 54us/step - loss: 0.0110 - val_loss: 0.0090\n",
      "Epoch 1017/1500\n",
      "514/514 [==============================] - 0s 48us/step - loss: 0.0108 - val_loss: 0.0087\n",
      "Epoch 1018/1500\n",
      "514/514 [==============================] - 0s 40us/step - loss: 0.0106 - val_loss: 0.0089\n",
      "Epoch 1019/1500\n",
      "514/514 [==============================] - 0s 38us/step - loss: 0.0109 - val_loss: 0.0088\n",
      "Epoch 1020/1500\n",
      "514/514 [==============================] - 0s 37us/step - loss: 0.0108 - val_loss: 0.0089\n",
      "Epoch 1021/1500\n",
      "514/514 [==============================] - 0s 41us/step - loss: 0.0109 - val_loss: 0.0127\n",
      "Epoch 1022/1500\n",
      "514/514 [==============================] - 0s 36us/step - loss: 0.0155 - val_loss: 0.0107\n",
      "Epoch 1023/1500\n",
      "514/514 [==============================] - 0s 45us/step - loss: 0.0117 - val_loss: 0.0101\n",
      "Epoch 1024/1500\n",
      "514/514 [==============================] - 0s 45us/step - loss: 0.0125 - val_loss: 0.0095\n",
      "Epoch 1025/1500\n",
      "514/514 [==============================] - 0s 47us/step - loss: 0.0113 - val_loss: 0.0092\n",
      "Epoch 1026/1500\n",
      "514/514 [==============================] - 0s 46us/step - loss: 0.0118 - val_loss: 0.0095\n",
      "Epoch 1027/1500\n",
      "514/514 [==============================] - 0s 51us/step - loss: 0.0113 - val_loss: 0.0109\n",
      "Epoch 1028/1500\n",
      "514/514 [==============================] - 0s 50us/step - loss: 0.0128 - val_loss: 0.0102\n",
      "Epoch 1029/1500\n",
      "514/514 [==============================] - 0s 51us/step - loss: 0.0114 - val_loss: 0.0110\n",
      "Epoch 1030/1500\n",
      "514/514 [==============================] - 0s 51us/step - loss: 0.0130 - val_loss: 0.0112\n",
      "Epoch 1031/1500\n",
      "514/514 [==============================] - 0s 46us/step - loss: 0.0137 - val_loss: 0.0130\n",
      "Epoch 1032/1500\n",
      "514/514 [==============================] - 0s 54us/step - loss: 0.0171 - val_loss: 0.0172\n",
      "Epoch 1033/1500\n",
      "514/514 [==============================] - 0s 54us/step - loss: 0.0212 - val_loss: 0.0176\n",
      "Epoch 1034/1500\n",
      "514/514 [==============================] - 0s 59us/step - loss: 0.0206 - val_loss: 0.0162\n",
      "Epoch 1035/1500\n",
      "514/514 [==============================] - 0s 47us/step - loss: 0.0199 - val_loss: 0.0196\n",
      "Epoch 1036/1500\n",
      "514/514 [==============================] - 0s 57us/step - loss: 0.0244 - val_loss: 0.0189\n",
      "Epoch 1037/1500\n",
      "514/514 [==============================] - 0s 50us/step - loss: 0.0194 - val_loss: 0.0141\n",
      "Epoch 1038/1500\n",
      "514/514 [==============================] - 0s 53us/step - loss: 0.0190 - val_loss: 0.0137\n",
      "Epoch 1039/1500\n",
      "514/514 [==============================] - 0s 56us/step - loss: 0.0139 - val_loss: 0.0210\n",
      "Epoch 1040/1500\n",
      "514/514 [==============================] - 0s 52us/step - loss: 0.0198 - val_loss: 0.0124\n",
      "Epoch 1041/1500\n",
      "514/514 [==============================] - 0s 48us/step - loss: 0.0168 - val_loss: 0.0186\n",
      "Epoch 1042/1500\n",
      "514/514 [==============================] - 0s 48us/step - loss: 0.0174 - val_loss: 0.0165\n",
      "Epoch 1043/1500\n",
      "514/514 [==============================] - 0s 52us/step - loss: 0.0156 - val_loss: 0.0117\n",
      "Epoch 1044/1500\n",
      "514/514 [==============================] - 0s 50us/step - loss: 0.0130 - val_loss: 0.0119\n",
      "Epoch 1045/1500\n",
      "514/514 [==============================] - 0s 46us/step - loss: 0.0143 - val_loss: 0.0134\n",
      "Epoch 1046/1500\n",
      "514/514 [==============================] - 0s 49us/step - loss: 0.0159 - val_loss: 0.0119\n",
      "Epoch 1047/1500\n",
      "514/514 [==============================] - 0s 48us/step - loss: 0.0138 - val_loss: 0.0105\n",
      "Epoch 1048/1500\n",
      "514/514 [==============================] - 0s 48us/step - loss: 0.0143 - val_loss: 0.0175\n",
      "Epoch 1049/1500\n",
      "514/514 [==============================] - 0s 49us/step - loss: 0.0156 - val_loss: 0.0118\n",
      "Epoch 1050/1500\n",
      "514/514 [==============================] - 0s 43us/step - loss: 0.0122 - val_loss: 0.0105\n",
      "Epoch 1051/1500\n",
      "514/514 [==============================] - 0s 50us/step - loss: 0.0117 - val_loss: 0.0119\n",
      "Epoch 1052/1500\n",
      "514/514 [==============================] - 0s 52us/step - loss: 0.0127 - val_loss: 0.0128\n",
      "Epoch 1053/1500\n",
      "514/514 [==============================] - 0s 50us/step - loss: 0.0130 - val_loss: 0.0112\n",
      "Epoch 1054/1500\n",
      "514/514 [==============================] - 0s 50us/step - loss: 0.0118 - val_loss: 0.0120\n",
      "Epoch 1055/1500\n",
      "514/514 [==============================] - 0s 44us/step - loss: 0.0152 - val_loss: 0.0170\n",
      "Epoch 1056/1500\n",
      "514/514 [==============================] - 0s 52us/step - loss: 0.0202 - val_loss: 0.0207\n",
      "Epoch 1057/1500\n",
      "514/514 [==============================] - 0s 50us/step - loss: 0.0250 - val_loss: 0.0766\n",
      "Epoch 1058/1500\n",
      "514/514 [==============================] - 0s 43us/step - loss: 0.0692 - val_loss: 0.0298\n",
      "Epoch 1059/1500\n",
      "514/514 [==============================] - 0s 44us/step - loss: 0.0244 - val_loss: 0.0131\n",
      "Epoch 1060/1500\n",
      "514/514 [==============================] - 0s 53us/step - loss: 0.0185 - val_loss: 0.0389\n",
      "Epoch 1061/1500\n",
      "514/514 [==============================] - 0s 53us/step - loss: 0.0394 - val_loss: 0.0400\n",
      "Epoch 1062/1500\n",
      "514/514 [==============================] - 0s 53us/step - loss: 0.0447 - val_loss: 0.0370\n",
      "Epoch 1063/1500\n",
      "514/514 [==============================] - 0s 51us/step - loss: 0.0340 - val_loss: 0.0326\n",
      "Epoch 1064/1500\n",
      "514/514 [==============================] - 0s 56us/step - loss: 0.0338 - val_loss: 0.0167\n",
      "Epoch 1065/1500\n",
      "514/514 [==============================] - 0s 50us/step - loss: 0.0179 - val_loss: 0.0227\n",
      "Epoch 1066/1500\n",
      "514/514 [==============================] - 0s 45us/step - loss: 0.0212 - val_loss: 0.0174\n",
      "Epoch 1067/1500\n",
      "514/514 [==============================] - 0s 48us/step - loss: 0.0166 - val_loss: 0.0133\n",
      "Epoch 1068/1500\n",
      "514/514 [==============================] - 0s 48us/step - loss: 0.0135 - val_loss: 0.0096\n",
      "Epoch 1069/1500\n",
      "514/514 [==============================] - 0s 47us/step - loss: 0.0114 - val_loss: 0.0132\n",
      "Epoch 1070/1500\n",
      "514/514 [==============================] - 0s 42us/step - loss: 0.0166 - val_loss: 0.0109\n",
      "Epoch 1071/1500\n",
      "514/514 [==============================] - 0s 46us/step - loss: 0.0154 - val_loss: 0.0139\n",
      "Epoch 1072/1500\n",
      "514/514 [==============================] - 0s 46us/step - loss: 0.0159 - val_loss: 0.0095\n",
      "Epoch 1073/1500\n",
      "514/514 [==============================] - 0s 47us/step - loss: 0.0119 - val_loss: 0.0116\n",
      "Epoch 1074/1500\n",
      "514/514 [==============================] - 0s 51us/step - loss: 0.0127 - val_loss: 0.0110\n",
      "Epoch 1075/1500\n",
      "514/514 [==============================] - 0s 50us/step - loss: 0.0117 - val_loss: 0.0095\n",
      "Epoch 1076/1500\n",
      "514/514 [==============================] - 0s 43us/step - loss: 0.0111 - val_loss: 0.0093\n",
      "Epoch 1077/1500\n",
      "514/514 [==============================] - 0s 58us/step - loss: 0.0113 - val_loss: 0.0092\n",
      "Epoch 1078/1500\n",
      "514/514 [==============================] - 0s 49us/step - loss: 0.0109 - val_loss: 0.0093\n",
      "Epoch 1079/1500\n",
      "514/514 [==============================] - 0s 47us/step - loss: 0.0112 - val_loss: 0.0092\n",
      "Epoch 1080/1500\n",
      "514/514 [==============================] - 0s 56us/step - loss: 0.0111 - val_loss: 0.0092\n",
      "Epoch 1081/1500\n",
      "514/514 [==============================] - 0s 46us/step - loss: 0.0111 - val_loss: 0.0120\n",
      "Epoch 1082/1500\n",
      "514/514 [==============================] - 0s 51us/step - loss: 0.0146 - val_loss: 0.0147\n",
      "Epoch 1083/1500\n",
      "514/514 [==============================] - 0s 51us/step - loss: 0.0160 - val_loss: 0.0098\n",
      "Epoch 1084/1500\n",
      "514/514 [==============================] - 0s 51us/step - loss: 0.0112 - val_loss: 0.0105\n",
      "Epoch 1085/1500\n",
      "514/514 [==============================] - 0s 57us/step - loss: 0.0129 - val_loss: 0.0112\n",
      "Epoch 1086/1500\n",
      "514/514 [==============================] - 0s 53us/step - loss: 0.0139 - val_loss: 0.0100\n",
      "Epoch 1087/1500\n",
      "514/514 [==============================] - 0s 50us/step - loss: 0.0124 - val_loss: 0.0088\n",
      "Epoch 1088/1500\n",
      "514/514 [==============================] - 0s 49us/step - loss: 0.0116 - val_loss: 0.0095\n",
      "Epoch 1089/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "514/514 [==============================] - 0s 47us/step - loss: 0.0113 - val_loss: 0.0099\n",
      "Epoch 1090/1500\n",
      "514/514 [==============================] - 0s 44us/step - loss: 0.0116 - val_loss: 0.0102\n",
      "Epoch 1091/1500\n",
      "514/514 [==============================] - 0s 48us/step - loss: 0.0114 - val_loss: 0.0094\n",
      "Epoch 1092/1500\n",
      "514/514 [==============================] - 0s 42us/step - loss: 0.0111 - val_loss: 0.0094\n",
      "Epoch 1093/1500\n",
      "514/514 [==============================] - 0s 52us/step - loss: 0.0112 - val_loss: 0.0092\n",
      "Epoch 1094/1500\n",
      "514/514 [==============================] - 0s 48us/step - loss: 0.0112 - val_loss: 0.0097\n",
      "Epoch 1095/1500\n",
      "514/514 [==============================] - 0s 54us/step - loss: 0.0119 - val_loss: 0.0105\n",
      "Epoch 1096/1500\n",
      "514/514 [==============================] - 0s 49us/step - loss: 0.0124 - val_loss: 0.0162\n",
      "Epoch 1097/1500\n",
      "514/514 [==============================] - 0s 50us/step - loss: 0.0213 - val_loss: 0.0399\n",
      "Epoch 1098/1500\n",
      "514/514 [==============================] - 0s 53us/step - loss: 0.0372 - val_loss: 0.0119\n",
      "Epoch 1099/1500\n",
      "514/514 [==============================] - 0s 52us/step - loss: 0.0196 - val_loss: 0.0396\n",
      "Epoch 1100/1500\n",
      "514/514 [==============================] - 0s 50us/step - loss: 0.0417 - val_loss: 0.0557\n",
      "Epoch 1101/1500\n",
      "514/514 [==============================] - 0s 56us/step - loss: 0.0502 - val_loss: 0.0660\n",
      "Epoch 1102/1500\n",
      "514/514 [==============================] - 0s 46us/step - loss: 0.0618 - val_loss: 0.0457\n",
      "Epoch 1103/1500\n",
      "514/514 [==============================] - 0s 47us/step - loss: 0.0506 - val_loss: 0.0404\n",
      "Epoch 1104/1500\n",
      "514/514 [==============================] - 0s 51us/step - loss: 0.0498 - val_loss: 0.0943\n",
      "Epoch 1105/1500\n",
      "514/514 [==============================] - 0s 52us/step - loss: 0.0741 - val_loss: 0.0551\n",
      "Epoch 1106/1500\n",
      "514/514 [==============================] - 0s 54us/step - loss: 0.0574 - val_loss: 0.0456\n",
      "Epoch 1107/1500\n",
      "514/514 [==============================] - 0s 49us/step - loss: 0.0481 - val_loss: 0.0454\n",
      "Epoch 1108/1500\n",
      "514/514 [==============================] - 0s 47us/step - loss: 0.0363 - val_loss: 0.0473\n",
      "Epoch 1109/1500\n",
      "514/514 [==============================] - 0s 48us/step - loss: 0.0472 - val_loss: 0.0487\n",
      "Epoch 1110/1500\n",
      "514/514 [==============================] - 0s 47us/step - loss: 0.0517 - val_loss: 0.0322\n",
      "Epoch 1111/1500\n",
      "514/514 [==============================] - 0s 51us/step - loss: 0.0287 - val_loss: 0.0238\n",
      "Epoch 1112/1500\n",
      "514/514 [==============================] - 0s 87us/step - loss: 0.0242 - val_loss: 0.0182\n",
      "Epoch 1113/1500\n",
      "514/514 [==============================] - 0s 73us/step - loss: 0.0178 - val_loss: 0.0136\n",
      "Epoch 1114/1500\n",
      "514/514 [==============================] - 0s 72us/step - loss: 0.0132 - val_loss: 0.0120\n",
      "Epoch 1115/1500\n",
      "514/514 [==============================] - 0s 65us/step - loss: 0.0138 - val_loss: 0.0151\n",
      "Epoch 1116/1500\n",
      "514/514 [==============================] - ETA: 0s - loss: 0.016 - 0s 64us/step - loss: 0.0194 - val_loss: 0.0198\n",
      "Epoch 1117/1500\n",
      "514/514 [==============================] - 0s 62us/step - loss: 0.0230 - val_loss: 0.0252\n",
      "Epoch 1118/1500\n",
      "514/514 [==============================] - 0s 63us/step - loss: 0.0292 - val_loss: 0.0378\n",
      "Epoch 1119/1500\n",
      "514/514 [==============================] - 0s 65us/step - loss: 0.0378 - val_loss: 0.0241\n",
      "Epoch 1120/1500\n",
      "514/514 [==============================] - 0s 67us/step - loss: 0.0238 - val_loss: 0.0133\n",
      "Epoch 1121/1500\n",
      "514/514 [==============================] - 0s 114us/step - loss: 0.0154 - val_loss: 0.0119\n",
      "Epoch 1122/1500\n",
      "514/514 [==============================] - 0s 57us/step - loss: 0.0142 - val_loss: 0.0165\n",
      "Epoch 1123/1500\n",
      "514/514 [==============================] - 0s 55us/step - loss: 0.0155 - val_loss: 0.0233\n",
      "Epoch 1124/1500\n",
      "514/514 [==============================] - 0s 54us/step - loss: 0.0243 - val_loss: 0.0139\n",
      "Epoch 1125/1500\n",
      "514/514 [==============================] - 0s 56us/step - loss: 0.0198 - val_loss: 0.0271\n",
      "Epoch 1126/1500\n",
      "514/514 [==============================] - 0s 52us/step - loss: 0.0274 - val_loss: 0.0251\n",
      "Epoch 1127/1500\n",
      "514/514 [==============================] - 0s 59us/step - loss: 0.0257 - val_loss: 0.0226\n",
      "Epoch 1128/1500\n",
      "514/514 [==============================] - 0s 63us/step - loss: 0.0241 - val_loss: 0.0199\n",
      "Epoch 1129/1500\n",
      "514/514 [==============================] - 0s 40us/step - loss: 0.0229 - val_loss: 0.0155\n",
      "Epoch 1130/1500\n",
      "514/514 [==============================] - 0s 42us/step - loss: 0.0184 - val_loss: 0.0240\n",
      "Epoch 1131/1500\n",
      "514/514 [==============================] - 0s 36us/step - loss: 0.0244 - val_loss: 0.0210\n",
      "Epoch 1132/1500\n",
      "514/514 [==============================] - 0s 51us/step - loss: 0.0228 - val_loss: 0.0353\n",
      "Epoch 1133/1500\n",
      "514/514 [==============================] - 0s 34us/step - loss: 0.0310 - val_loss: 0.0269\n",
      "Epoch 1134/1500\n",
      "514/514 [==============================] - 0s 33us/step - loss: 0.0238 - val_loss: 0.0269\n",
      "Epoch 1135/1500\n",
      "514/514 [==============================] - 0s 34us/step - loss: 0.0271 - val_loss: 0.0213\n",
      "Epoch 1136/1500\n",
      "514/514 [==============================] - 0s 49us/step - loss: 0.0252 - val_loss: 0.0400\n",
      "Epoch 1137/1500\n",
      "514/514 [==============================] - 0s 58us/step - loss: 0.0336 - val_loss: 0.0171\n",
      "Epoch 1138/1500\n",
      "514/514 [==============================] - 0s 62us/step - loss: 0.0145 - val_loss: 0.0313\n",
      "Epoch 1139/1500\n",
      "514/514 [==============================] - 0s 61us/step - loss: 0.0280 - val_loss: 0.0347\n",
      "Epoch 1140/1500\n",
      "514/514 [==============================] - 0s 45us/step - loss: 0.0311 - val_loss: 0.0159\n",
      "Epoch 1141/1500\n",
      "514/514 [==============================] - 0s 48us/step - loss: 0.0203 - val_loss: 0.0114\n",
      "Epoch 1142/1500\n",
      "514/514 [==============================] - 0s 38us/step - loss: 0.0158 - val_loss: 0.0144\n",
      "Epoch 1143/1500\n",
      "514/514 [==============================] - 0s 36us/step - loss: 0.0219 - val_loss: 0.0184\n",
      "Epoch 1144/1500\n",
      "514/514 [==============================] - 0s 41us/step - loss: 0.0153 - val_loss: 0.0194\n",
      "Epoch 1145/1500\n",
      "514/514 [==============================] - 0s 35us/step - loss: 0.0205 - val_loss: 0.0118\n",
      "Epoch 1146/1500\n",
      "514/514 [==============================] - 0s 37us/step - loss: 0.0139 - val_loss: 0.0188\n",
      "Epoch 1147/1500\n",
      "514/514 [==============================] - 0s 34us/step - loss: 0.0205 - val_loss: 0.0123\n",
      "Epoch 1148/1500\n",
      "514/514 [==============================] - 0s 49us/step - loss: 0.0140 - val_loss: 0.0156\n",
      "Epoch 1149/1500\n",
      "514/514 [==============================] - 0s 38us/step - loss: 0.0134 - val_loss: 0.0145\n",
      "Epoch 1150/1500\n",
      "514/514 [==============================] - 0s 53us/step - loss: 0.0166 - val_loss: 0.0116\n",
      "Epoch 1151/1500\n",
      "514/514 [==============================] - 0s 38us/step - loss: 0.0127 - val_loss: 0.0112\n",
      "Epoch 1152/1500\n",
      "514/514 [==============================] - 0s 36us/step - loss: 0.0130 - val_loss: 0.0128\n",
      "Epoch 1153/1500\n",
      "514/514 [==============================] - 0s 78us/step - loss: 0.0125 - val_loss: 0.0124\n",
      "Epoch 1154/1500\n",
      "514/514 [==============================] - 0s 83us/step - loss: 0.0122 - val_loss: 0.0109\n",
      "Epoch 1155/1500\n",
      "514/514 [==============================] - 0s 54us/step - loss: 0.0118 - val_loss: 0.0115\n",
      "Epoch 1156/1500\n",
      "514/514 [==============================] - 0s 71us/step - loss: 0.0133 - val_loss: 0.0126\n",
      "Epoch 1157/1500\n",
      "514/514 [==============================] - 0s 54us/step - loss: 0.0139 - val_loss: 0.0124\n",
      "Epoch 1158/1500\n",
      "514/514 [==============================] - 0s 53us/step - loss: 0.0137 - val_loss: 0.0100\n",
      "Epoch 1159/1500\n",
      "514/514 [==============================] - 0s 50us/step - loss: 0.0114 - val_loss: 0.0111\n",
      "Epoch 1160/1500\n",
      "514/514 [==============================] - 0s 48us/step - loss: 0.0137 - val_loss: 0.0184\n",
      "Epoch 1161/1500\n",
      "514/514 [==============================] - 0s 51us/step - loss: 0.0177 - val_loss: 0.0307\n",
      "Epoch 1162/1500\n",
      "514/514 [==============================] - 0s 60us/step - loss: 0.0263 - val_loss: 0.0349\n",
      "Epoch 1163/1500\n",
      "514/514 [==============================] - 0s 56us/step - loss: 0.0277 - val_loss: 0.0244\n",
      "Epoch 1164/1500\n",
      "514/514 [==============================] - 0s 45us/step - loss: 0.0220 - val_loss: 0.0181\n",
      "Epoch 1165/1500\n",
      "514/514 [==============================] - 0s 46us/step - loss: 0.0170 - val_loss: 0.0202\n",
      "Epoch 1166/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "514/514 [==============================] - 0s 41us/step - loss: 0.0205 - val_loss: 0.0159\n",
      "Epoch 1167/1500\n",
      "514/514 [==============================] - 0s 49us/step - loss: 0.0163 - val_loss: 0.0123\n",
      "Epoch 1168/1500\n",
      "514/514 [==============================] - 0s 36us/step - loss: 0.0145 - val_loss: 0.0126\n",
      "Epoch 1169/1500\n",
      "514/514 [==============================] - 0s 32us/step - loss: 0.0141 - val_loss: 0.0129\n",
      "Epoch 1170/1500\n",
      "514/514 [==============================] - 0s 34us/step - loss: 0.0132 - val_loss: 0.0112\n",
      "Epoch 1171/1500\n",
      "514/514 [==============================] - 0s 37us/step - loss: 0.0120 - val_loss: 0.0151\n",
      "Epoch 1172/1500\n",
      "514/514 [==============================] - 0s 34us/step - loss: 0.0191 - val_loss: 0.0165\n",
      "Epoch 1173/1500\n",
      "514/514 [==============================] - 0s 36us/step - loss: 0.0154 - val_loss: 0.0131\n",
      "Epoch 1174/1500\n",
      "514/514 [==============================] - 0s 34us/step - loss: 0.0136 - val_loss: 0.0124\n",
      "Epoch 1175/1500\n",
      "514/514 [==============================] - ETA: 0s - loss: 0.012 - 0s 35us/step - loss: 0.0125 - val_loss: 0.0117\n",
      "Epoch 1176/1500\n",
      "514/514 [==============================] - 0s 51us/step - loss: 0.0119 - val_loss: 0.0107\n",
      "Epoch 1177/1500\n",
      "514/514 [==============================] - 0s 41us/step - loss: 0.0116 - val_loss: 0.0100\n",
      "Epoch 1178/1500\n",
      "514/514 [==============================] - 0s 36us/step - loss: 0.0116 - val_loss: 0.0104\n",
      "Epoch 1179/1500\n",
      "514/514 [==============================] - 0s 32us/step - loss: 0.0133 - val_loss: 0.0124\n",
      "Epoch 1180/1500\n",
      "514/514 [==============================] - 0s 34us/step - loss: 0.0157 - val_loss: 0.0103\n",
      "Epoch 1181/1500\n",
      "514/514 [==============================] - 0s 34us/step - loss: 0.0125 - val_loss: 0.0123\n",
      "Epoch 1182/1500\n",
      "514/514 [==============================] - 0s 37us/step - loss: 0.0153 - val_loss: 0.0147\n",
      "Epoch 1183/1500\n",
      "514/514 [==============================] - 0s 37us/step - loss: 0.0154 - val_loss: 0.0189\n",
      "Epoch 1184/1500\n",
      "514/514 [==============================] - 0s 34us/step - loss: 0.0169 - val_loss: 0.0161\n",
      "Epoch 1185/1500\n",
      "514/514 [==============================] - 0s 36us/step - loss: 0.0147 - val_loss: 0.0104\n",
      "Epoch 1186/1500\n",
      "514/514 [==============================] - 0s 126us/step - loss: 0.0118 - val_loss: 0.0111\n",
      "Epoch 1187/1500\n",
      "514/514 [==============================] - 0s 68us/step - loss: 0.0139 - val_loss: 0.0156\n",
      "Epoch 1188/1500\n",
      "514/514 [==============================] - 0s 66us/step - loss: 0.0165 - val_loss: 0.0110\n",
      "Epoch 1189/1500\n",
      "514/514 [==============================] - 0s 60us/step - loss: 0.0119 - val_loss: 0.0118\n",
      "Epoch 1190/1500\n",
      "514/514 [==============================] - 0s 63us/step - loss: 0.0150 - val_loss: 0.0158\n",
      "Epoch 1191/1500\n",
      "514/514 [==============================] - 0s 56us/step - loss: 0.0160 - val_loss: 0.0126\n",
      "Epoch 1192/1500\n",
      "514/514 [==============================] - 0s 57us/step - loss: 0.0140 - val_loss: 0.0142\n",
      "Epoch 1193/1500\n",
      "514/514 [==============================] - 0s 57us/step - loss: 0.0138 - val_loss: 0.0113\n",
      "Epoch 1194/1500\n",
      "514/514 [==============================] - 0s 59us/step - loss: 0.0141 - val_loss: 0.0127\n",
      "Epoch 1195/1500\n",
      "514/514 [==============================] - 0s 62us/step - loss: 0.0158 - val_loss: 0.0124\n",
      "Epoch 1196/1500\n",
      "514/514 [==============================] - 0s 62us/step - loss: 0.0146 - val_loss: 0.0100\n",
      "Epoch 1197/1500\n",
      "514/514 [==============================] - 0s 58us/step - loss: 0.0135 - val_loss: 0.0138\n",
      "Epoch 1198/1500\n",
      "514/514 [==============================] - 0s 52us/step - loss: 0.0169 - val_loss: 0.0131\n",
      "Epoch 1199/1500\n",
      "514/514 [==============================] - 0s 60us/step - loss: 0.0155 - val_loss: 0.0098\n",
      "Epoch 1200/1500\n",
      "514/514 [==============================] - 0s 59us/step - loss: 0.0118 - val_loss: 0.0103\n",
      "Epoch 1201/1500\n",
      "514/514 [==============================] - 0s 59us/step - loss: 0.0126 - val_loss: 0.0139\n",
      "Epoch 1202/1500\n",
      "514/514 [==============================] - 0s 59us/step - loss: 0.0137 - val_loss: 0.0104\n",
      "Epoch 1203/1500\n",
      "514/514 [==============================] - 0s 57us/step - loss: 0.0111 - val_loss: 0.0103\n",
      "Epoch 1204/1500\n",
      "514/514 [==============================] - 0s 59us/step - loss: 0.0114 - val_loss: 0.0108\n",
      "Epoch 1205/1500\n",
      "514/514 [==============================] - 0s 52us/step - loss: 0.0123 - val_loss: 0.0100\n",
      "Epoch 1206/1500\n",
      "514/514 [==============================] - 0s 66us/step - loss: 0.0112 - val_loss: 0.0102\n",
      "Epoch 1207/1500\n",
      "514/514 [==============================] - 0s 48us/step - loss: 0.0115 - val_loss: 0.0109\n",
      "Epoch 1208/1500\n",
      "514/514 [==============================] - 0s 48us/step - loss: 0.0119 - val_loss: 0.0095\n",
      "Epoch 1209/1500\n",
      "514/514 [==============================] - 0s 53us/step - loss: 0.0108 - val_loss: 0.0091\n",
      "Epoch 1210/1500\n",
      "514/514 [==============================] - 0s 47us/step - loss: 0.0108 - val_loss: 0.0098\n",
      "Epoch 1211/1500\n",
      "514/514 [==============================] - 0s 50us/step - loss: 0.0113 - val_loss: 0.0097\n",
      "Epoch 1212/1500\n",
      "514/514 [==============================] - 0s 59us/step - loss: 0.0109 - val_loss: 0.0095\n",
      "Epoch 1213/1500\n",
      "514/514 [==============================] - 0s 46us/step - loss: 0.0108 - val_loss: 0.0111\n",
      "Epoch 1214/1500\n",
      "514/514 [==============================] - 0s 49us/step - loss: 0.0127 - val_loss: 0.0112\n",
      "Epoch 1215/1500\n",
      "514/514 [==============================] - 0s 49us/step - loss: 0.0123 - val_loss: 0.0088\n",
      "Epoch 1216/1500\n",
      "514/514 [==============================] - 0s 48us/step - loss: 0.0105 - val_loss: 0.0091\n",
      "Epoch 1217/1500\n",
      "514/514 [==============================] - 0s 55us/step - loss: 0.0116 - val_loss: 0.0088\n",
      "Epoch 1218/1500\n",
      "514/514 [==============================] - 0s 52us/step - loss: 0.0119 - val_loss: 0.0091\n",
      "Epoch 1219/1500\n",
      "514/514 [==============================] - 0s 48us/step - loss: 0.0119 - val_loss: 0.0118\n",
      "Epoch 1220/1500\n",
      "514/514 [==============================] - 0s 53us/step - loss: 0.0143 - val_loss: 0.0135\n",
      "Epoch 1221/1500\n",
      "514/514 [==============================] - 0s 50us/step - loss: 0.0152 - val_loss: 0.0117\n",
      "Epoch 1222/1500\n",
      "514/514 [==============================] - 0s 56us/step - loss: 0.0133 - val_loss: 0.0104\n",
      "Epoch 1223/1500\n",
      "514/514 [==============================] - 0s 47us/step - loss: 0.0122 - val_loss: 0.0100\n",
      "Epoch 1224/1500\n",
      "514/514 [==============================] - 0s 51us/step - loss: 0.0121 - val_loss: 0.0097\n",
      "Epoch 1225/1500\n",
      "514/514 [==============================] - 0s 45us/step - loss: 0.0116 - val_loss: 0.0100\n",
      "Epoch 1226/1500\n",
      "514/514 [==============================] - 0s 53us/step - loss: 0.0117 - val_loss: 0.0114\n",
      "Epoch 1227/1500\n",
      "514/514 [==============================] - 0s 54us/step - loss: 0.0124 - val_loss: 0.0152\n",
      "Epoch 1228/1500\n",
      "514/514 [==============================] - 0s 51us/step - loss: 0.0145 - val_loss: 0.0143\n",
      "Epoch 1229/1500\n",
      "514/514 [==============================] - 0s 50us/step - loss: 0.0137 - val_loss: 0.0125\n",
      "Epoch 1230/1500\n",
      "514/514 [==============================] - 0s 54us/step - loss: 0.0129 - val_loss: 0.0130\n",
      "Epoch 1231/1500\n",
      "514/514 [==============================] - 0s 46us/step - loss: 0.0137 - val_loss: 0.0118\n",
      "Epoch 1232/1500\n",
      "514/514 [==============================] - 0s 49us/step - loss: 0.0136 - val_loss: 0.0109\n",
      "Epoch 1233/1500\n",
      "514/514 [==============================] - 0s 50us/step - loss: 0.0140 - val_loss: 0.0124\n",
      "Epoch 1234/1500\n",
      "514/514 [==============================] - 0s 59us/step - loss: 0.0153 - val_loss: 0.0097\n",
      "Epoch 1235/1500\n",
      "514/514 [==============================] - 0s 54us/step - loss: 0.0118 - val_loss: 0.0105\n",
      "Epoch 1236/1500\n",
      "514/514 [==============================] - 0s 55us/step - loss: 0.0130 - val_loss: 0.0106\n",
      "Epoch 1237/1500\n",
      "514/514 [==============================] - 0s 55us/step - loss: 0.0114 - val_loss: 0.0111\n",
      "Epoch 1238/1500\n",
      "514/514 [==============================] - 0s 53us/step - loss: 0.0146 - val_loss: 0.0163\n",
      "Epoch 1239/1500\n",
      "514/514 [==============================] - 0s 50us/step - loss: 0.0167 - val_loss: 0.0116\n",
      "Epoch 1240/1500\n",
      "514/514 [==============================] - 0s 49us/step - loss: 0.0133 - val_loss: 0.0122\n",
      "Epoch 1241/1500\n",
      "514/514 [==============================] - 0s 47us/step - loss: 0.0126 - val_loss: 0.0097\n",
      "Epoch 1242/1500\n",
      "514/514 [==============================] - 0s 49us/step - loss: 0.0119 - val_loss: 0.0107\n",
      "Epoch 1243/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "514/514 [==============================] - 0s 45us/step - loss: 0.0138 - val_loss: 0.0103\n",
      "Epoch 1244/1500\n",
      "514/514 [==============================] - 0s 51us/step - loss: 0.0124 - val_loss: 0.0114\n",
      "Epoch 1245/1500\n",
      "514/514 [==============================] - 0s 55us/step - loss: 0.0134 - val_loss: 0.0133\n",
      "Epoch 1246/1500\n",
      "514/514 [==============================] - 0s 50us/step - loss: 0.0143 - val_loss: 0.0114\n",
      "Epoch 1247/1500\n",
      "514/514 [==============================] - 0s 50us/step - loss: 0.0120 - val_loss: 0.0114\n",
      "Epoch 1248/1500\n",
      "514/514 [==============================] - 0s 49us/step - loss: 0.0141 - val_loss: 0.0136\n",
      "Epoch 1249/1500\n",
      "514/514 [==============================] - 0s 48us/step - loss: 0.0148 - val_loss: 0.0100\n",
      "Epoch 1250/1500\n",
      "514/514 [==============================] - 0s 50us/step - loss: 0.0123 - val_loss: 0.0113\n",
      "Epoch 1251/1500\n",
      "514/514 [==============================] - 0s 45us/step - loss: 0.0133 - val_loss: 0.0150\n",
      "Epoch 1252/1500\n",
      "514/514 [==============================] - 0s 51us/step - loss: 0.0195 - val_loss: 0.0169\n",
      "Epoch 1253/1500\n",
      "514/514 [==============================] - 0s 46us/step - loss: 0.0201 - val_loss: 0.0170\n",
      "Epoch 1254/1500\n",
      "514/514 [==============================] - 0s 50us/step - loss: 0.0204 - val_loss: 0.0118\n",
      "Epoch 1255/1500\n",
      "514/514 [==============================] - 0s 43us/step - loss: 0.0137 - val_loss: 0.0141\n",
      "Epoch 1256/1500\n",
      "514/514 [==============================] - 0s 53us/step - loss: 0.0174 - val_loss: 0.0128\n",
      "Epoch 1257/1500\n",
      "514/514 [==============================] - 0s 60us/step - loss: 0.0130 - val_loss: 0.0098\n",
      "Epoch 1258/1500\n",
      "514/514 [==============================] - 0s 54us/step - loss: 0.0115 - val_loss: 0.0101\n",
      "Epoch 1259/1500\n",
      "514/514 [==============================] - 0s 49us/step - loss: 0.0122 - val_loss: 0.0106\n",
      "Epoch 1260/1500\n",
      "514/514 [==============================] - 0s 55us/step - loss: 0.0123 - val_loss: 0.0097\n",
      "Epoch 1261/1500\n",
      "514/514 [==============================] - 0s 61us/step - loss: 0.0113 - val_loss: 0.0116\n",
      "Epoch 1262/1500\n",
      "514/514 [==============================] - 0s 46us/step - loss: 0.0131 - val_loss: 0.0092\n",
      "Epoch 1263/1500\n",
      "514/514 [==============================] - 0s 51us/step - loss: 0.0115 - val_loss: 0.0122\n",
      "Epoch 1264/1500\n",
      "514/514 [==============================] - 0s 52us/step - loss: 0.0137 - val_loss: 0.0096\n",
      "Epoch 1265/1500\n",
      "514/514 [==============================] - 0s 51us/step - loss: 0.0127 - val_loss: 0.0200\n",
      "Epoch 1266/1500\n",
      "514/514 [==============================] - 0s 48us/step - loss: 0.0201 - val_loss: 0.0228\n",
      "Epoch 1267/1500\n",
      "514/514 [==============================] - 0s 50us/step - loss: 0.0211 - val_loss: 0.0201\n",
      "Epoch 1268/1500\n",
      "514/514 [==============================] - 0s 45us/step - loss: 0.0186 - val_loss: 0.0173\n",
      "Epoch 1269/1500\n",
      "514/514 [==============================] - 0s 53us/step - loss: 0.0152 - val_loss: 0.0133\n",
      "Epoch 1270/1500\n",
      "514/514 [==============================] - 0s 49us/step - loss: 0.0122 - val_loss: 0.0237\n",
      "Epoch 1271/1500\n",
      "514/514 [==============================] - 0s 50us/step - loss: 0.0352 - val_loss: 0.1012\n",
      "Epoch 1272/1500\n",
      "514/514 [==============================] - 0s 52us/step - loss: 0.0858 - val_loss: 0.0203\n",
      "Epoch 1273/1500\n",
      "514/514 [==============================] - 0s 56us/step - loss: 0.0198 - val_loss: 0.0138\n",
      "Epoch 1274/1500\n",
      "514/514 [==============================] - 0s 46us/step - loss: 0.0178 - val_loss: 0.0221\n",
      "Epoch 1275/1500\n",
      "514/514 [==============================] - 0s 54us/step - loss: 0.0233 - val_loss: 0.0164\n",
      "Epoch 1276/1500\n",
      "514/514 [==============================] - 0s 47us/step - loss: 0.0170 - val_loss: 0.0150\n",
      "Epoch 1277/1500\n",
      "514/514 [==============================] - 0s 43us/step - loss: 0.0158 - val_loss: 0.0108\n",
      "Epoch 1278/1500\n",
      "514/514 [==============================] - 0s 51us/step - loss: 0.0135 - val_loss: 0.0146\n",
      "Epoch 1279/1500\n",
      "514/514 [==============================] - 0s 41us/step - loss: 0.0199 - val_loss: 0.0217\n",
      "Epoch 1280/1500\n",
      "514/514 [==============================] - 0s 48us/step - loss: 0.0224 - val_loss: 0.0181\n",
      "Epoch 1281/1500\n",
      "514/514 [==============================] - 0s 52us/step - loss: 0.0215 - val_loss: 0.0140\n",
      "Epoch 1282/1500\n",
      "514/514 [==============================] - 0s 48us/step - loss: 0.0175 - val_loss: 0.0115\n",
      "Epoch 1283/1500\n",
      "514/514 [==============================] - 0s 47us/step - loss: 0.0150 - val_loss: 0.0112\n",
      "Epoch 1284/1500\n",
      "514/514 [==============================] - 0s 53us/step - loss: 0.0132 - val_loss: 0.0113\n",
      "Epoch 1285/1500\n",
      "514/514 [==============================] - 0s 47us/step - loss: 0.0133 - val_loss: 0.0123\n",
      "Epoch 1286/1500\n",
      "514/514 [==============================] - 0s 44us/step - loss: 0.0134 - val_loss: 0.0122\n",
      "Epoch 1287/1500\n",
      "514/514 [==============================] - 0s 48us/step - loss: 0.0144 - val_loss: 0.0134\n",
      "Epoch 1288/1500\n",
      "514/514 [==============================] - 0s 47us/step - loss: 0.0146 - val_loss: 0.0115\n",
      "Epoch 1289/1500\n",
      "514/514 [==============================] - 0s 45us/step - loss: 0.0130 - val_loss: 0.0113\n",
      "Epoch 1290/1500\n",
      "514/514 [==============================] - 0s 44us/step - loss: 0.0130 - val_loss: 0.0088\n",
      "Epoch 1291/1500\n",
      "514/514 [==============================] - 0s 43us/step - loss: 0.0116 - val_loss: 0.0103\n",
      "Epoch 1292/1500\n",
      "514/514 [==============================] - 0s 47us/step - loss: 0.0133 - val_loss: 0.0088\n",
      "Epoch 1293/1500\n",
      "514/514 [==============================] - 0s 44us/step - loss: 0.0120 - val_loss: 0.0130\n",
      "Epoch 1294/1500\n",
      "514/514 [==============================] - 0s 44us/step - loss: 0.0152 - val_loss: 0.0120\n",
      "Epoch 1295/1500\n",
      "514/514 [==============================] - 0s 40us/step - loss: 0.0136 - val_loss: 0.0087\n",
      "Epoch 1296/1500\n",
      "514/514 [==============================] - 0s 44us/step - loss: 0.0114 - val_loss: 0.0106\n",
      "Epoch 1297/1500\n",
      "514/514 [==============================] - 0s 47us/step - loss: 0.0128 - val_loss: 0.0090\n",
      "Epoch 1298/1500\n",
      "514/514 [==============================] - 0s 41us/step - loss: 0.0108 - val_loss: 0.0094\n",
      "Epoch 1299/1500\n",
      "514/514 [==============================] - 0s 37us/step - loss: 0.0114 - val_loss: 0.0090\n",
      "Epoch 1300/1500\n",
      "514/514 [==============================] - 0s 38us/step - loss: 0.0116 - val_loss: 0.0096\n",
      "Epoch 1301/1500\n",
      "514/514 [==============================] - 0s 36us/step - loss: 0.0131 - val_loss: 0.0115\n",
      "Epoch 1302/1500\n",
      "514/514 [==============================] - 0s 37us/step - loss: 0.0140 - val_loss: 0.0087\n",
      "Epoch 1303/1500\n",
      "514/514 [==============================] - 0s 37us/step - loss: 0.0117 - val_loss: 0.0137\n",
      "Epoch 1304/1500\n",
      "514/514 [==============================] - 0s 37us/step - loss: 0.0145 - val_loss: 0.0105\n",
      "Epoch 1305/1500\n",
      "514/514 [==============================] - 0s 50us/step - loss: 0.0126 - val_loss: 0.0138\n",
      "Epoch 1306/1500\n",
      "514/514 [==============================] - 0s 55us/step - loss: 0.0149 - val_loss: 0.0117\n",
      "Epoch 1307/1500\n",
      "514/514 [==============================] - 0s 51us/step - loss: 0.0130 - val_loss: 0.0111\n",
      "Epoch 1308/1500\n",
      "514/514 [==============================] - 0s 50us/step - loss: 0.0117 - val_loss: 0.0107\n",
      "Epoch 1309/1500\n",
      "514/514 [==============================] - 0s 54us/step - loss: 0.0113 - val_loss: 0.0098\n",
      "Epoch 1310/1500\n",
      "514/514 [==============================] - 0s 50us/step - loss: 0.0109 - val_loss: 0.0093\n",
      "Epoch 1311/1500\n",
      "514/514 [==============================] - 0s 47us/step - loss: 0.0114 - val_loss: 0.0096\n",
      "Epoch 1312/1500\n",
      "514/514 [==============================] - 0s 41us/step - loss: 0.0122 - val_loss: 0.0097\n",
      "Epoch 1313/1500\n",
      "514/514 [==============================] - 0s 51us/step - loss: 0.0114 - val_loss: 0.0112\n",
      "Epoch 1314/1500\n",
      "514/514 [==============================] - 0s 42us/step - loss: 0.0121 - val_loss: 0.0114\n",
      "Epoch 1315/1500\n",
      "514/514 [==============================] - 0s 55us/step - loss: 0.0121 - val_loss: 0.0191\n",
      "Epoch 1316/1500\n",
      "514/514 [==============================] - 0s 71us/step - loss: 0.0194 - val_loss: 0.0228\n",
      "Epoch 1317/1500\n",
      "514/514 [==============================] - 0s 55us/step - loss: 0.0210 - val_loss: 0.0198\n",
      "Epoch 1318/1500\n",
      "514/514 [==============================] - 0s 54us/step - loss: 0.0204 - val_loss: 0.0217\n",
      "Epoch 1319/1500\n",
      "514/514 [==============================] - 0s 56us/step - loss: 0.0206 - val_loss: 0.0185\n",
      "Epoch 1320/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "514/514 [==============================] - 0s 51us/step - loss: 0.0171 - val_loss: 0.0256\n",
      "Epoch 1321/1500\n",
      "514/514 [==============================] - 0s 85us/step - loss: 0.0218 - val_loss: 0.0132\n",
      "Epoch 1322/1500\n",
      "514/514 [==============================] - 0s 57us/step - loss: 0.0128 - val_loss: 0.0152\n",
      "Epoch 1323/1500\n",
      "514/514 [==============================] - 0s 57us/step - loss: 0.0164 - val_loss: 0.0106\n",
      "Epoch 1324/1500\n",
      "514/514 [==============================] - 0s 53us/step - loss: 0.0122 - val_loss: 0.0225\n",
      "Epoch 1325/1500\n",
      "514/514 [==============================] - 0s 50us/step - loss: 0.0225 - val_loss: 0.0110\n",
      "Epoch 1326/1500\n",
      "514/514 [==============================] - 0s 57us/step - loss: 0.0119 - val_loss: 0.0144\n",
      "Epoch 1327/1500\n",
      "514/514 [==============================] - 0s 59us/step - loss: 0.0168 - val_loss: 0.0109\n",
      "Epoch 1328/1500\n",
      "514/514 [==============================] - 0s 56us/step - loss: 0.0129 - val_loss: 0.0112\n",
      "Epoch 1329/1500\n",
      "514/514 [==============================] - 0s 51us/step - loss: 0.0134 - val_loss: 0.0115\n",
      "Epoch 1330/1500\n",
      "514/514 [==============================] - ETA: 0s - loss: 0.011 - 0s 46us/step - loss: 0.0122 - val_loss: 0.0120\n",
      "Epoch 1331/1500\n",
      "514/514 [==============================] - 0s 59us/step - loss: 0.0137 - val_loss: 0.0142\n",
      "Epoch 1332/1500\n",
      "514/514 [==============================] - 0s 64us/step - loss: 0.0146 - val_loss: 0.0114\n",
      "Epoch 1333/1500\n",
      "514/514 [==============================] - 0s 47us/step - loss: 0.0126 - val_loss: 0.0094\n",
      "Epoch 1334/1500\n",
      "514/514 [==============================] - 0s 49us/step - loss: 0.0109 - val_loss: 0.0102\n",
      "Epoch 1335/1500\n",
      "514/514 [==============================] - 0s 52us/step - loss: 0.0122 - val_loss: 0.0099\n",
      "Epoch 1336/1500\n",
      "514/514 [==============================] - 0s 51us/step - loss: 0.0113 - val_loss: 0.0093\n",
      "Epoch 1337/1500\n",
      "514/514 [==============================] - 0s 52us/step - loss: 0.0120 - val_loss: 0.0121\n",
      "Epoch 1338/1500\n",
      "514/514 [==============================] - 0s 60us/step - loss: 0.0133 - val_loss: 0.0094\n",
      "Epoch 1339/1500\n",
      "514/514 [==============================] - 0s 58us/step - loss: 0.0111 - val_loss: 0.0105\n",
      "Epoch 1340/1500\n",
      "514/514 [==============================] - 0s 57us/step - loss: 0.0125 - val_loss: 0.0099\n",
      "Epoch 1341/1500\n",
      "514/514 [==============================] - 0s 60us/step - loss: 0.0119 - val_loss: 0.0099\n",
      "Epoch 1342/1500\n",
      "514/514 [==============================] - 0s 50us/step - loss: 0.0126 - val_loss: 0.0126\n",
      "Epoch 1343/1500\n",
      "514/514 [==============================] - 0s 46us/step - loss: 0.0151 - val_loss: 0.0109\n",
      "Epoch 1344/1500\n",
      "514/514 [==============================] - 0s 54us/step - loss: 0.0119 - val_loss: 0.0098\n",
      "Epoch 1345/1500\n",
      "514/514 [==============================] - 0s 53us/step - loss: 0.0110 - val_loss: 0.0109\n",
      "Epoch 1346/1500\n",
      "514/514 [==============================] - 0s 49us/step - loss: 0.0124 - val_loss: 0.0123\n",
      "Epoch 1347/1500\n",
      "514/514 [==============================] - 0s 56us/step - loss: 0.0140 - val_loss: 0.0128\n",
      "Epoch 1348/1500\n",
      "514/514 [==============================] - 0s 58us/step - loss: 0.0134 - val_loss: 0.0117\n",
      "Epoch 1349/1500\n",
      "514/514 [==============================] - 0s 49us/step - loss: 0.0119 - val_loss: 0.0103\n",
      "Epoch 1350/1500\n",
      "514/514 [==============================] - 0s 53us/step - loss: 0.0111 - val_loss: 0.0101\n",
      "Epoch 1351/1500\n",
      "514/514 [==============================] - 0s 52us/step - loss: 0.0113 - val_loss: 0.0111\n",
      "Epoch 1352/1500\n",
      "514/514 [==============================] - 0s 46us/step - loss: 0.0126 - val_loss: 0.0122\n",
      "Epoch 1353/1500\n",
      "514/514 [==============================] - 0s 48us/step - loss: 0.0138 - val_loss: 0.0106\n",
      "Epoch 1354/1500\n",
      "514/514 [==============================] - 0s 59us/step - loss: 0.0119 - val_loss: 0.0087\n",
      "Epoch 1355/1500\n",
      "514/514 [==============================] - 0s 55us/step - loss: 0.0105 - val_loss: 0.0099\n",
      "Epoch 1356/1500\n",
      "514/514 [==============================] - 0s 49us/step - loss: 0.0133 - val_loss: 0.0145\n",
      "Epoch 1357/1500\n",
      "514/514 [==============================] - 0s 48us/step - loss: 0.0185 - val_loss: 0.0131\n",
      "Epoch 1358/1500\n",
      "514/514 [==============================] - 0s 49us/step - loss: 0.0162 - val_loss: 0.0157\n",
      "Epoch 1359/1500\n",
      "514/514 [==============================] - 0s 51us/step - loss: 0.0209 - val_loss: 0.0191\n",
      "Epoch 1360/1500\n",
      "514/514 [==============================] - 0s 48us/step - loss: 0.0209 - val_loss: 0.0109\n",
      "Epoch 1361/1500\n",
      "514/514 [==============================] - 0s 46us/step - loss: 0.0138 - val_loss: 0.0099\n",
      "Epoch 1362/1500\n",
      "514/514 [==============================] - 0s 48us/step - loss: 0.0125 - val_loss: 0.0116\n",
      "Epoch 1363/1500\n",
      "514/514 [==============================] - 0s 52us/step - loss: 0.0140 - val_loss: 0.0126\n",
      "Epoch 1364/1500\n",
      "514/514 [==============================] - 0s 50us/step - loss: 0.0128 - val_loss: 0.0126\n",
      "Epoch 1365/1500\n",
      "514/514 [==============================] - 0s 48us/step - loss: 0.0131 - val_loss: 0.0112\n",
      "Epoch 1366/1500\n",
      "514/514 [==============================] - 0s 45us/step - loss: 0.0129 - val_loss: 0.0116\n",
      "Epoch 1367/1500\n",
      "514/514 [==============================] - 0s 51us/step - loss: 0.0151 - val_loss: 0.0308\n",
      "Epoch 1368/1500\n",
      "514/514 [==============================] - 0s 45us/step - loss: 0.0273 - val_loss: 0.0153\n",
      "Epoch 1369/1500\n",
      "514/514 [==============================] - 0s 49us/step - loss: 0.0155 - val_loss: 0.0124\n",
      "Epoch 1370/1500\n",
      "514/514 [==============================] - 0s 42us/step - loss: 0.0155 - val_loss: 0.0157\n",
      "Epoch 1371/1500\n",
      "514/514 [==============================] - 0s 47us/step - loss: 0.0186 - val_loss: 0.0120\n",
      "Epoch 1372/1500\n",
      "514/514 [==============================] - 0s 44us/step - loss: 0.0153 - val_loss: 0.0137\n",
      "Epoch 1373/1500\n",
      "514/514 [==============================] - 0s 52us/step - loss: 0.0167 - val_loss: 0.0102\n",
      "Epoch 1374/1500\n",
      "514/514 [==============================] - 0s 41us/step - loss: 0.0133 - val_loss: 0.0124\n",
      "Epoch 1375/1500\n",
      "514/514 [==============================] - 0s 42us/step - loss: 0.0158 - val_loss: 0.0117\n",
      "Epoch 1376/1500\n",
      "514/514 [==============================] - 0s 50us/step - loss: 0.0132 - val_loss: 0.0100\n",
      "Epoch 1377/1500\n",
      "514/514 [==============================] - ETA: 0s - loss: 0.011 - 0s 49us/step - loss: 0.0114 - val_loss: 0.0106\n",
      "Epoch 1378/1500\n",
      "514/514 [==============================] - 0s 50us/step - loss: 0.0115 - val_loss: 0.0103\n",
      "Epoch 1379/1500\n",
      "514/514 [==============================] - 0s 56us/step - loss: 0.0116 - val_loss: 0.0100\n",
      "Epoch 1380/1500\n",
      "514/514 [==============================] - 0s 45us/step - loss: 0.0116 - val_loss: 0.0097\n",
      "Epoch 1381/1500\n",
      "514/514 [==============================] - 0s 49us/step - loss: 0.0114 - val_loss: 0.0100\n",
      "Epoch 1382/1500\n",
      "514/514 [==============================] - 0s 46us/step - loss: 0.0119 - val_loss: 0.0128\n",
      "Epoch 1383/1500\n",
      "514/514 [==============================] - 0s 42us/step - loss: 0.0143 - val_loss: 0.0138\n",
      "Epoch 1384/1500\n",
      "514/514 [==============================] - 0s 49us/step - loss: 0.0143 - val_loss: 0.0117\n",
      "Epoch 1385/1500\n",
      "514/514 [==============================] - 0s 49us/step - loss: 0.0129 - val_loss: 0.0108\n",
      "Epoch 1386/1500\n",
      "514/514 [==============================] - 0s 47us/step - loss: 0.0123 - val_loss: 0.0100\n",
      "Epoch 1387/1500\n",
      "514/514 [==============================] - 0s 45us/step - loss: 0.0124 - val_loss: 0.0107\n",
      "Epoch 1388/1500\n",
      "514/514 [==============================] - 0s 44us/step - loss: 0.0123 - val_loss: 0.0085\n",
      "Epoch 1389/1500\n",
      "514/514 [==============================] - 0s 46us/step - loss: 0.0109 - val_loss: 0.0090\n",
      "Epoch 1390/1500\n",
      "514/514 [==============================] - 0s 52us/step - loss: 0.0113 - val_loss: 0.0086\n",
      "Epoch 1391/1500\n",
      "514/514 [==============================] - 0s 50us/step - loss: 0.0108 - val_loss: 0.0088\n",
      "Epoch 1392/1500\n",
      "514/514 [==============================] - 0s 51us/step - loss: 0.0109 - val_loss: 0.0092\n",
      "Epoch 1393/1500\n",
      "514/514 [==============================] - 0s 57us/step - loss: 0.0110 - val_loss: 0.0088\n",
      "Epoch 1394/1500\n",
      "514/514 [==============================] - 0s 53us/step - loss: 0.0108 - val_loss: 0.0114\n",
      "Epoch 1395/1500\n",
      "514/514 [==============================] - 0s 50us/step - loss: 0.0130 - val_loss: 0.0138\n",
      "Epoch 1396/1500\n",
      "514/514 [==============================] - 0s 53us/step - loss: 0.0142 - val_loss: 0.0149\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1397/1500\n",
      "514/514 [==============================] - 0s 48us/step - loss: 0.0149 - val_loss: 0.0146\n",
      "Epoch 1398/1500\n",
      "514/514 [==============================] - 0s 53us/step - loss: 0.0144 - val_loss: 0.0108\n",
      "Epoch 1399/1500\n",
      "514/514 [==============================] - 0s 47us/step - loss: 0.0119 - val_loss: 0.0100\n",
      "Epoch 1400/1500\n",
      "514/514 [==============================] - 0s 47us/step - loss: 0.0133 - val_loss: 0.0104\n",
      "Epoch 1401/1500\n",
      "514/514 [==============================] - 0s 48us/step - loss: 0.0132 - val_loss: 0.0108\n",
      "Epoch 1402/1500\n",
      "514/514 [==============================] - 0s 51us/step - loss: 0.0140 - val_loss: 0.0196\n",
      "Epoch 1403/1500\n",
      "514/514 [==============================] - 0s 50us/step - loss: 0.0231 - val_loss: 0.0170\n",
      "Epoch 1404/1500\n",
      "514/514 [==============================] - 0s 48us/step - loss: 0.0172 - val_loss: 0.0104\n",
      "Epoch 1405/1500\n",
      "514/514 [==============================] - 0s 50us/step - loss: 0.0132 - val_loss: 0.0111\n",
      "Epoch 1406/1500\n",
      "514/514 [==============================] - 0s 48us/step - loss: 0.0132 - val_loss: 0.0101\n",
      "Epoch 1407/1500\n",
      "514/514 [==============================] - 0s 53us/step - loss: 0.0148 - val_loss: 0.0108\n",
      "Epoch 1408/1500\n",
      "514/514 [==============================] - 0s 55us/step - loss: 0.0134 - val_loss: 0.0167\n",
      "Epoch 1409/1500\n",
      "514/514 [==============================] - 0s 51us/step - loss: 0.0196 - val_loss: 0.0136\n",
      "Epoch 1410/1500\n",
      "514/514 [==============================] - 0s 57us/step - loss: 0.0151 - val_loss: 0.0109\n",
      "Epoch 1411/1500\n",
      "514/514 [==============================] - 0s 60us/step - loss: 0.0133 - val_loss: 0.0114\n",
      "Epoch 1412/1500\n",
      "514/514 [==============================] - 0s 59us/step - loss: 0.0120 - val_loss: 0.0103\n",
      "Epoch 1413/1500\n",
      "514/514 [==============================] - 0s 62us/step - loss: 0.0124 - val_loss: 0.0109\n",
      "Epoch 1414/1500\n",
      "514/514 [==============================] - 0s 55us/step - loss: 0.0128 - val_loss: 0.0093\n",
      "Epoch 1415/1500\n",
      "514/514 [==============================] - 0s 43us/step - loss: 0.0113 - val_loss: 0.0106\n",
      "Epoch 1416/1500\n",
      "514/514 [==============================] - 0s 47us/step - loss: 0.0117 - val_loss: 0.0129\n",
      "Epoch 1417/1500\n",
      "514/514 [==============================] - 0s 68us/step - loss: 0.0142 - val_loss: 0.0105\n",
      "Epoch 1418/1500\n",
      "514/514 [==============================] - 0s 62us/step - loss: 0.0119 - val_loss: 0.0112\n",
      "Epoch 1419/1500\n",
      "514/514 [==============================] - 0s 49us/step - loss: 0.0122 - val_loss: 0.0113\n",
      "Epoch 1420/1500\n",
      "514/514 [==============================] - 0s 52us/step - loss: 0.0126 - val_loss: 0.0111\n",
      "Epoch 1421/1500\n",
      "514/514 [==============================] - 0s 54us/step - loss: 0.0123 - val_loss: 0.0102\n",
      "Epoch 1422/1500\n",
      "514/514 [==============================] - 0s 46us/step - loss: 0.0118 - val_loss: 0.0102\n",
      "Epoch 1423/1500\n",
      "514/514 [==============================] - 0s 52us/step - loss: 0.0115 - val_loss: 0.0095\n",
      "Epoch 1424/1500\n",
      "514/514 [==============================] - 0s 49us/step - loss: 0.0109 - val_loss: 0.0107\n",
      "Epoch 1425/1500\n",
      "514/514 [==============================] - 0s 52us/step - loss: 0.0118 - val_loss: 0.0104\n",
      "Epoch 1426/1500\n",
      "514/514 [==============================] - 0s 56us/step - loss: 0.0112 - val_loss: 0.0096\n",
      "Epoch 1427/1500\n",
      "514/514 [==============================] - 0s 46us/step - loss: 0.0109 - val_loss: 0.0092\n",
      "Epoch 1428/1500\n",
      "514/514 [==============================] - 0s 55us/step - loss: 0.0108 - val_loss: 0.0092\n",
      "Epoch 1429/1500\n",
      "514/514 [==============================] - 0s 59us/step - loss: 0.0108 - val_loss: 0.0091\n",
      "Epoch 1430/1500\n",
      "514/514 [==============================] - 0s 46us/step - loss: 0.0111 - val_loss: 0.0088\n",
      "Epoch 1431/1500\n",
      "514/514 [==============================] - 0s 50us/step - loss: 0.0113 - val_loss: 0.0086\n",
      "Epoch 1432/1500\n",
      "514/514 [==============================] - 0s 50us/step - loss: 0.0112 - val_loss: 0.0087\n",
      "Epoch 1433/1500\n",
      "514/514 [==============================] - 0s 44us/step - loss: 0.0112 - val_loss: 0.0088\n",
      "Epoch 1434/1500\n",
      "514/514 [==============================] - 0s 54us/step - loss: 0.0109 - val_loss: 0.0089\n",
      "Epoch 1435/1500\n",
      "514/514 [==============================] - 0s 47us/step - loss: 0.0106 - val_loss: 0.0097\n",
      "Epoch 1436/1500\n",
      "514/514 [==============================] - 0s 45us/step - loss: 0.0110 - val_loss: 0.0105\n",
      "Epoch 1437/1500\n",
      "514/514 [==============================] - 0s 47us/step - loss: 0.0117 - val_loss: 0.0088\n",
      "Epoch 1438/1500\n",
      "514/514 [==============================] - 0s 48us/step - loss: 0.0111 - val_loss: 0.0115\n",
      "Epoch 1439/1500\n",
      "514/514 [==============================] - 0s 48us/step - loss: 0.0159 - val_loss: 0.0142\n",
      "Epoch 1440/1500\n",
      "514/514 [==============================] - 0s 51us/step - loss: 0.0178 - val_loss: 0.0140\n",
      "Epoch 1441/1500\n",
      "514/514 [==============================] - 0s 55us/step - loss: 0.0179 - val_loss: 0.0124\n",
      "Epoch 1442/1500\n",
      "514/514 [==============================] - 0s 53us/step - loss: 0.0151 - val_loss: 0.0136\n",
      "Epoch 1443/1500\n",
      "514/514 [==============================] - 0s 45us/step - loss: 0.0139 - val_loss: 0.0219\n",
      "Epoch 1444/1500\n",
      "514/514 [==============================] - 0s 48us/step - loss: 0.0227 - val_loss: 0.0245\n",
      "Epoch 1445/1500\n",
      "514/514 [==============================] - 0s 51us/step - loss: 0.0233 - val_loss: 0.0211\n",
      "Epoch 1446/1500\n",
      "514/514 [==============================] - 0s 54us/step - loss: 0.0173 - val_loss: 0.0134\n",
      "Epoch 1447/1500\n",
      "514/514 [==============================] - 0s 55us/step - loss: 0.0122 - val_loss: 0.0110\n",
      "Epoch 1448/1500\n",
      "514/514 [==============================] - 0s 53us/step - loss: 0.0136 - val_loss: 0.0112\n",
      "Epoch 1449/1500\n",
      "514/514 [==============================] - 0s 50us/step - loss: 0.0145 - val_loss: 0.0119\n",
      "Epoch 1450/1500\n",
      "514/514 [==============================] - 0s 53us/step - loss: 0.0147 - val_loss: 0.0111\n",
      "Epoch 1451/1500\n",
      "514/514 [==============================] - 0s 49us/step - loss: 0.0139 - val_loss: 0.0104\n",
      "Epoch 1452/1500\n",
      "514/514 [==============================] - 0s 50us/step - loss: 0.0132 - val_loss: 0.0104\n",
      "Epoch 1453/1500\n",
      "514/514 [==============================] - 0s 48us/step - loss: 0.0122 - val_loss: 0.0137\n",
      "Epoch 1454/1500\n",
      "514/514 [==============================] - 0s 47us/step - loss: 0.0133 - val_loss: 0.0145\n",
      "Epoch 1455/1500\n",
      "514/514 [==============================] - 0s 50us/step - loss: 0.0131 - val_loss: 0.0129\n",
      "Epoch 1456/1500\n",
      "514/514 [==============================] - 0s 56us/step - loss: 0.0127 - val_loss: 0.0164\n",
      "Epoch 1457/1500\n",
      "514/514 [==============================] - 0s 52us/step - loss: 0.0152 - val_loss: 0.0161\n",
      "Epoch 1458/1500\n",
      "514/514 [==============================] - 0s 44us/step - loss: 0.0145 - val_loss: 0.0118\n",
      "Epoch 1459/1500\n",
      "514/514 [==============================] - 0s 47us/step - loss: 0.0120 - val_loss: 0.0116\n",
      "Epoch 1460/1500\n",
      "514/514 [==============================] - 0s 52us/step - loss: 0.0122 - val_loss: 0.0122\n",
      "Epoch 1461/1500\n",
      "514/514 [==============================] - 0s 50us/step - loss: 0.0147 - val_loss: 0.0140\n",
      "Epoch 1462/1500\n",
      "514/514 [==============================] - 0s 55us/step - loss: 0.0179 - val_loss: 0.0130\n",
      "Epoch 1463/1500\n",
      "514/514 [==============================] - 0s 57us/step - loss: 0.0172 - val_loss: 0.0140\n",
      "Epoch 1464/1500\n",
      "514/514 [==============================] - 0s 54us/step - loss: 0.0192 - val_loss: 0.0198\n",
      "Epoch 1465/1500\n",
      "514/514 [==============================] - 0s 50us/step - loss: 0.0228 - val_loss: 0.0139\n",
      "Epoch 1466/1500\n",
      "514/514 [==============================] - 0s 46us/step - loss: 0.0176 - val_loss: 0.0163\n",
      "Epoch 1467/1500\n",
      "514/514 [==============================] - 0s 48us/step - loss: 0.0200 - val_loss: 0.0149\n",
      "Epoch 1468/1500\n",
      "514/514 [==============================] - 0s 40us/step - loss: 0.0186 - val_loss: 0.0124\n",
      "Epoch 1469/1500\n",
      "514/514 [==============================] - 0s 43us/step - loss: 0.0166 - val_loss: 0.0146\n",
      "Epoch 1470/1500\n",
      "514/514 [==============================] - 0s 47us/step - loss: 0.0175 - val_loss: 0.0184\n",
      "Epoch 1471/1500\n",
      "514/514 [==============================] - 0s 51us/step - loss: 0.0213 - val_loss: 0.0143\n",
      "Epoch 1472/1500\n",
      "514/514 [==============================] - 0s 50us/step - loss: 0.0173 - val_loss: 0.0134\n",
      "Epoch 1473/1500\n",
      "514/514 [==============================] - 0s 55us/step - loss: 0.0154 - val_loss: 0.0138\n",
      "Epoch 1474/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "514/514 [==============================] - 0s 58us/step - loss: 0.0159 - val_loss: 0.0113\n",
      "Epoch 1475/1500\n",
      "514/514 [==============================] - 0s 51us/step - loss: 0.0133 - val_loss: 0.0110\n",
      "Epoch 1476/1500\n",
      "514/514 [==============================] - 0s 51us/step - loss: 0.0132 - val_loss: 0.0102\n",
      "Epoch 1477/1500\n",
      "514/514 [==============================] - 0s 53us/step - loss: 0.0137 - val_loss: 0.0182\n",
      "Epoch 1478/1500\n",
      "514/514 [==============================] - 0s 58us/step - loss: 0.0195 - val_loss: 0.0234\n",
      "Epoch 1479/1500\n",
      "514/514 [==============================] - 0s 61us/step - loss: 0.0243 - val_loss: 0.0244\n",
      "Epoch 1480/1500\n",
      "514/514 [==============================] - 0s 50us/step - loss: 0.0242 - val_loss: 0.0214\n",
      "Epoch 1481/1500\n",
      "514/514 [==============================] - 0s 50us/step - loss: 0.0205 - val_loss: 0.0183\n",
      "Epoch 1482/1500\n",
      "514/514 [==============================] - 0s 49us/step - loss: 0.0175 - val_loss: 0.0132\n",
      "Epoch 1483/1500\n",
      "514/514 [==============================] - 0s 52us/step - loss: 0.0151 - val_loss: 0.0156\n",
      "Epoch 1484/1500\n",
      "514/514 [==============================] - 0s 71us/step - loss: 0.0214 - val_loss: 0.0157\n",
      "Epoch 1485/1500\n",
      "514/514 [==============================] - 0s 87us/step - loss: 0.0200 - val_loss: 0.0151\n",
      "Epoch 1486/1500\n",
      "514/514 [==============================] - 0s 74us/step - loss: 0.0194 - val_loss: 0.0160\n",
      "Epoch 1487/1500\n",
      "514/514 [==============================] - 0s 72us/step - loss: 0.0235 - val_loss: 0.0282\n",
      "Epoch 1488/1500\n",
      "514/514 [==============================] - 0s 64us/step - loss: 0.0317 - val_loss: 0.0163\n",
      "Epoch 1489/1500\n",
      "514/514 [==============================] - 0s 57us/step - loss: 0.0195 - val_loss: 0.0125\n",
      "Epoch 1490/1500\n",
      "514/514 [==============================] - 0s 53us/step - loss: 0.0155 - val_loss: 0.0115\n",
      "Epoch 1491/1500\n",
      "514/514 [==============================] - 0s 102us/step - loss: 0.0153 - val_loss: 0.0106\n",
      "Epoch 1492/1500\n",
      "514/514 [==============================] - 0s 50us/step - loss: 0.0139 - val_loss: 0.0091\n",
      "Epoch 1493/1500\n",
      "514/514 [==============================] - 0s 56us/step - loss: 0.0125 - val_loss: 0.0087\n",
      "Epoch 1494/1500\n",
      "514/514 [==============================] - 0s 48us/step - loss: 0.0119 - val_loss: 0.0098\n",
      "Epoch 1495/1500\n",
      "514/514 [==============================] - 0s 42us/step - loss: 0.0145 - val_loss: 0.0115\n",
      "Epoch 1496/1500\n",
      "514/514 [==============================] - 0s 66us/step - loss: 0.0129 - val_loss: 0.0085\n",
      "Epoch 1497/1500\n",
      "514/514 [==============================] - 0s 48us/step - loss: 0.0115 - val_loss: 0.0093\n",
      "Epoch 1498/1500\n",
      "514/514 [==============================] - 0s 34us/step - loss: 0.0124 - val_loss: 0.0095\n",
      "Epoch 1499/1500\n",
      "514/514 [==============================] - 0s 37us/step - loss: 0.0121 - val_loss: 0.0090\n",
      "Epoch 1500/1500\n",
      "514/514 [==============================] - 0s 34us/step - loss: 0.0118 - val_loss: 0.0094\n"
     ]
    }
   ],
   "source": [
    "# fit the model - INTENSIVE\n",
    "model3_history = model3.fit(X_train, Y_train, batch_size=256, epochs=1500, verbose=1, \\\n",
    "                            shuffle = True, validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model3_history.model.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAApsAAAGXCAYAAAD8uJuCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDMMwDMMwTGz8fzPdxll/KcCbAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# use our model to predict in the range we want\n",
    "num_epochs = f'{len(model2_history.epoch)}'\n",
    "\n",
    "X_range = np.linspace(-10, 10, 500)\n",
    "y_pred = model3.predict(X_range)\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(10,6))\n",
    "ax.scatter(X_train, Y_train, label='Training data')\n",
    "ax.plot(X_range, y_pred, lw=4, color='r', label=f'NN ({num_epochs} epochs)')\n",
    "\n",
    "ax.set_xlabel(r'$X$', fontsize=20)\n",
    "ax.set_ylabel(r'$Y$', fontsize=20)\n",
    "ax.set_title(f'NN with {len(model3_history.model.layers)} layers, {H} nodes in each layer', fontsize=LABEL_SIZE)\n",
    "ax.tick_params(labelsize=20)\n",
    "\n",
    "ax.legend(loc=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAooAAAGJCAYAAAANCBK/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10,6))\n",
    "ax.plot(np.sqrt(model2_history.history['loss']), 'r')\n",
    "ax.plot(np.sqrt(model2_history.history['val_loss']), 'b' ,label='Val')\n",
    "ax.set_xlabel(r'Epoch', fontsize=20)\n",
    "ax.set_ylabel(r'Loss', fontsize=20)\n",
    "ax.legend()\n",
    "ax.tick_params(labelsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "735/735 [==============================] - 0s 35us/step\n",
      "Train loss: 0.011619159849804071\n",
      "Train R2: 0.8187427858361475\n"
     ]
    }
   ],
   "source": [
    "# evaluate the training and testing performance of your model \n",
    "# note: you should extract and check both the loss function and your evaluation metric\n",
    "score = model2.evaluate(X_train, Y_train, verbose=1)\n",
    "print('Train loss:', score)\n",
    "print('Train R2:', r2(Y_train, model2.predict(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score = model2.evaluate(X_test, Y_test, verbose=1)\n",
    "print('Test loss:', score)\n",
    "print('Test R2:', r2(Y_test, model2.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got a better score this time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"><b>Exercise 5</b> </div>\n",
    "\n",
    "Usually we want to avoid overfitting of the data to our model. But here we want to achive overfitting! So we can regularize! There are a few reasons why a model overfits. One is the lack of data. So we will try to overfit by reducing the data. Try that with model3 and see if it overfits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Having very few points in our data\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_samples = 50 # set the number of samples to take for each dataset\n",
    "test_size = 0.3 # set the proportion of data to hold out for testing\n",
    "\n",
    "# define the function and add noise\n",
    "\n",
    "def f_gauss(x):\n",
    "    return np.exp(-x * x) + np.random.normal(loc=0, scale=.1, size = x.shape[0])\n",
    "\n",
    "X = np.random.permutation(np.linspace(-10, 10, n_samples)) # choose some points from the function\n",
    "Y = f_gauss(X)\n",
    "\n",
    "# create training and testing data from this set of points\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# number of hidden nodes\n",
    "H =  100\n",
    "# input dimension\n",
    "input_dim = 1\n",
    "\n",
    "# create sequential multi-layer perceptron\n",
    "model4 = models.Sequential()\n",
    "# layer 0\n",
    "model4.add(layers.Dense(H, input_dim=input_dim,  \n",
    "                activation='tanh')) \n",
    "# layer 1\n",
    "model4.add(layers.Dense(H,\n",
    "                activation='tanh')) \n",
    "# layer 2\n",
    "model4.add(layers.Dense(H,\n",
    "                activation='tanh')) \n",
    "# layer 3\n",
    "model4.add(layers.Dense(H,  \n",
    "                activation='tanh')) \n",
    "# layer 4\n",
    "model4.add(layers.Dense(H,  \n",
    "                activation='tanh')) \n",
    "# layer 5\n",
    "model4.add(layers.Dense(H,  \n",
    "                activation='tanh')) \n",
    "# layer 6\n",
    "model4.add(layers.Dense(H,  \n",
    "                activation='tanh'))\n",
    "# layer 7\n",
    "model4.add(layers.Dense(H,  \n",
    "                activation='tanh'))\n",
    "# layer 8\n",
    "model4.add(layers.Dense(H,  \n",
    "                activation='tanh'))\n",
    "# layer 9\n",
    "model4.add(layers.Dense(H,  \n",
    "                activation='tanh'))\n",
    "# layer 10 - output\n",
    "model4.add(layers.Dense(1, \n",
    "                activation='linear')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# configure the model\n",
    "model4.compile(loss='mean_squared_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fit the model - INTENSIVE\n",
    "model4_history = model4.fit(X_train, Y_train, batch_size=256, epochs=1500, verbose=1, \\\n",
    "                            shuffle = True, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use our model to predict in the range we want\n",
    "num_epochs = f'{len(model4_history.epoch)}'\n",
    "\n",
    "X_range = np.linspace(-10, 10, 500)\n",
    "y_pred = model4.predict(X_range)\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(10,6))\n",
    "ax.scatter(X_train, Y_train, label='Training data')\n",
    "ax.plot(X_range, y_pred, lw=4, color='r', label=f'NN ({num_epochs} epochs)')\n",
    "\n",
    "ax.set_xlabel(r'$X$', fontsize=20)\n",
    "ax.set_ylabel(r'$Y$', fontsize=20)\n",
    "ax.set_title(f'NN with {len(model4_history.model.layers)} layers, {H} nodes in each layer', fontsize=LABEL_SIZE)\n",
    "ax.tick_params(labelsize=20)\n",
    "\n",
    "ax.legend(loc=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10,6))\n",
    "ax.plot(np.sqrt(model4_history.history['loss']), 'r')\n",
    "ax.plot(np.sqrt(model4_history.history['val_loss']), 'b' ,label='Val')\n",
    "ax.set_xlabel(r'Epoch', fontsize=20)\n",
    "ax.set_ylabel(r'Loss', fontsize=20)\n",
    "ax.legend()\n",
    "ax.tick_params(labelsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization\n",
    "\n",
    "Let's try adding a regularizer in our model: `kernel_regularizer=regularizers.l2(l2)`. Also let's create a function that takes the number of layers and the l2 value as the input and creates the model.\n",
    "\n",
    "Usage: `def create_dense([10, 20], l2=0.01)` will create a model with two hidden layers of 10 and 20 nodes each, l2=0.01 regularization and num_classes output nodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "H =  100  # number of hidden nodes\n",
    "input_dim = 1\n",
    "\n",
    "model5 = models.Sequential()\n",
    "\n",
    "# Input layer of the neural network with ReLU activation function and L2 regularization\n",
    "model5.add(layers.Dense(H, input_dim=input_dim,  \n",
    "                activation='tanh', \n",
    "                kernel_regularizer=regularizers.l2(0.01)))\n",
    "           \n",
    "# hidden layers\n",
    "model5.add(layers.Dense(H,   \n",
    "                activation='tanh', \n",
    "                kernel_regularizer=regularizers.l2(0.01)))\n",
    "model5.add(layers.Dense(H,   \n",
    "                activation='tanh', \n",
    "                kernel_regularizer=regularizers.l2(0.01)))\n",
    "model5.add(layers.Dense(H,   \n",
    "                activation='tanh', \n",
    "                kernel_regularizer=regularizers.l2(0.01)))\n",
    "model5.add(layers.Dense(H,   \n",
    "                activation='tanh', \n",
    "                kernel_regularizer=regularizers.l2(0.01)))\n",
    "model5.add(layers.Dense(H,   \n",
    "                activation='tanh', \n",
    "                kernel_regularizer=regularizers.l2(0.01)))\n",
    "model5.add(layers.Dense(H,   \n",
    "                activation='tanh', \n",
    "                kernel_regularizer=regularizers.l2(0.01)))\n",
    "# output layer\n",
    "model5.add(layers.Dense(1, \n",
    "                activation='linear')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# configure the model\n",
    "model5.compile(loss='mean_squared_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fit the model - INTENSIVE\n",
    "model5_history = model5.fit(X_train, Y_train, batch_size=256, epochs=1500, verbose=1, \\\n",
    "                            shuffle = True, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use our model to predict in the range we want\n",
    "num_epochs = f'{len(model5_history.epoch)}'\n",
    "\n",
    "X_range = np.linspace(-10, 10, 500)\n",
    "y_pred = model5.predict(X_range)\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(10,6))\n",
    "ax.scatter(X_train, Y_train, label='Training data')\n",
    "ax.plot(X_range, y_pred, lw=4, color='r', label=f'NN ({num_epochs} epochs)')\n",
    "\n",
    "ax.set_xlabel(r'$X$', fontsize=20)\n",
    "ax.set_ylabel(r'$Y$', fontsize=20)\n",
    "ax.set_title(f'NN with {len(model5_history.model.layers)} layers, {H} nodes in each layer', fontsize=LABEL_SIZE)\n",
    "ax.tick_params(labelsize=20)\n",
    "\n",
    "ax.legend(loc=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That seems very good. Let's see the $R^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score as r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score = model5.evaluate(X_test, Y_test, verbose=1)\n",
    "print('Test loss:', score)\n",
    "print('Test R2:', r2(Y_test, model5.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10,6))\n",
    "ax.plot(np.sqrt(model5_history.history['loss']), 'r')\n",
    "ax.plot(np.sqrt(model5_history.history['val_loss']), 'b' ,label='Val')\n",
    "ax.set_xlabel(r'Epoch', fontsize=20)\n",
    "ax.set_ylabel(r'Loss', fontsize=20)\n",
    "ax.legend()\n",
    "ax.tick_params(labelsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
